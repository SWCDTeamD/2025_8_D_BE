"""
SWCD Panel Data ETL Pipeline

panel_data.json íŒŒì¼ì„ DBì— ì ì¬í•˜ëŠ” íŒŒì´í”„ë¼ì¸:
ì…ë ¥ â†’ ë°ì´í„° ì „ì²˜ë¦¬ â†’ ë¹„ì •í˜• ë°ì´í„° ì„ë² ë”© â†’ DB ì €ì¥

ì‚¬ìš© ì˜ˆì‹œ:
    python backend/scripts/etl_pipeline.py --input backend/data/panel_data.json
"""

import argparse
import asyncio
import json
import os
import sys
import base64
from pathlib import Path
from typing import Any, Dict, List, Optional, cast

from dotenv import load_dotenv
from sqlalchemy import create_engine, text
from sqlalchemy.orm import sessionmaker

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = str(Path(__file__).resolve().parents[2])
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

load_dotenv(Path(PROJECT_ROOT) / ".env")

# LangChain + Bedrock
try:
    from langchain_aws import ChatBedrock  # type: ignore
    import boto3  # type: ignore
    from langchain_core.prompts import ChatPromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    from langchain_core.runnables import RunnableLambda, RunnablePassthrough
    _HAS_BEDROCK = True
except ImportError:
    ChatBedrock = None  # type: ignore
    boto3 = None  # type: ignore
    _HAS_BEDROCK = False

# KoSimCSE ì„ë² ë”© ëª¨ë¸
try:
    from sentence_transformers import SentenceTransformer  # type: ignore
    _HAS_KOSIMCSE = True
except ImportError:
    SentenceTransformer = None  # type: ignore
    _HAS_KOSIMCSE = False
    print("âš ï¸ Warning: sentence-transformers not installed. Embedding features will be disabled.")


# ===== DB ì—°ê²° ì„¤ì • =====
raw_url = os.getenv(
    "DATABASE_URL",
    "postgresql://swcd:swcdpw@127.0.0.1:5432/swcddb"
).replace("+asyncpg", "")  # ë™ê¸° ì—°ê²°

# RDS ì—°ê²°ì¸ ê²½ìš° SSL ì„¤ì • ì¶”ê°€ (rds.amazonaws.com í¬í•¨ ì‹œ)
# psycopg2ëŠ” sslmode íŒŒë¼ë¯¸í„°ë¥¼ ì§€ì›
if "rds.amazonaws.com" in raw_url and "sslmode" not in raw_url:
    separator = "&" if "?" in raw_url else "?"
    DATABASE_URL = f"{raw_url}{separator}sslmode=require"
else:
    DATABASE_URL = raw_url

engine = create_engine(DATABASE_URL, echo=False)
SessionLocal = sessionmaker(bind=engine)


# ===== ì „ì—­ ë³€ìˆ˜ =====
_KOSIMCSE_MODEL = None
_BEDROCK_SESSION = None
_BEDROCK_REGION = None


# ===== AWS Bedrock ì„¤ì • =====
def get_bedrock_config():
    """Bedrock API í‚¤ ë° ì„¤ì • ë¡œë“œ"""
    global _BEDROCK_SESSION, _BEDROCK_REGION
    
    if _BEDROCK_SESSION and _BEDROCK_REGION:
        return _BEDROCK_SESSION, _BEDROCK_REGION
    
    bedrock_key_encoded = os.getenv("AWS_BEARER_TOKEN_BEDROCK") or os.getenv("AWS_BEDROCK_API_KEY")
    if not bedrock_key_encoded:
        return None, None
    
    try:
        decoded_key = base64.b64decode(bedrock_key_encoded).decode("utf-8")
        if ":" in decoded_key:
            parts = decoded_key.split(":", 1)
            access_key = parts[0]
            secret_key = parts[1] if len(parts) > 1 else ""
        else:
            access_key = decoded_key
            secret_key = ""
    except (UnicodeDecodeError, Exception):
        try:
            decoded_bytes = base64.b64decode(bedrock_key_encoded)
            bed_key_marker = b'BedrockAPIKey'
            start_idx = decoded_bytes.find(bed_key_marker)
            if start_idx > 0:
                actual_key_bytes = decoded_bytes[start_idx:]
                decoded_key = actual_key_bytes.decode("utf-8", errors="ignore")
            elif len(decoded_bytes) > 2 and decoded_bytes[0:1] == b'\x00':
                decoded_key = decoded_bytes[2:].decode("utf-8", errors="replace")
            else:
                decoded_key = decoded_bytes.decode("latin-1", errors="ignore")
            decoded_key = decoded_key.strip("\x00").strip()
            if ":" in decoded_key:
                parts = decoded_key.split(":", 1)
                access_key = parts[0]
                secret_key = parts[1] if len(parts) > 1 else ""
            else:
                access_key = decoded_key
                secret_key = ""
        except Exception:
            if ":" in bedrock_key_encoded:
                parts = bedrock_key_encoded.split(":", 1)
                access_key = parts[0]
                secret_key = parts[1] if len(parts) > 1 else ""
            else:
                access_key = bedrock_key_encoded
                secret_key = ""
    
    region = os.getenv("AWS_REGION", "us-west-2")
    
    if access_key and secret_key:
        _BEDROCK_SESSION = boto3.Session(  # type: ignore
            aws_access_key_id=access_key,
            aws_secret_access_key=secret_key,
            region_name=region
        )
    else:
        _BEDROCK_SESSION = boto3.Session(region_name=region)  # type: ignore
    
    _BEDROCK_REGION = region
    return _BEDROCK_SESSION, _BEDROCK_REGION


def get_bedrock_llm(model_id: str = "anthropic.claude-3-haiku-20240307-v1:0"):
    """Bedrock Claude Haiku LLM ì´ˆê¸°í™”"""
    if not _HAS_BEDROCK:
        return None
    
    session, region = get_bedrock_config()
    if not session or not region:
        return None
    
    return ChatBedrock(  # type: ignore[call-arg]
        model_id=model_id,  # type: ignore[arg-type]
        credentials_profile_name=None,
        region_name=region,  # type: ignore[arg-type]
        model_kwargs={"temperature": 0.7, "max_tokens": 1000}
    )


# ===== KoSimCSE ì„ë² ë”© ëª¨ë¸ =====
def get_embedding_model():
    """KoSimCSE ëª¨ë¸ ë°˜í™˜ (768 ì°¨ì›)"""
    global _KOSIMCSE_MODEL
    
    if not _HAS_KOSIMCSE:
        return None
    
    if _KOSIMCSE_MODEL is None:
        print("ğŸ”„ KoSimCSE ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        _KOSIMCSE_MODEL = SentenceTransformer('BM-K/KoSimCSE-roberta-multitask')  # type: ignore
        print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")
    
    return _KOSIMCSE_MODEL


# ===== ë°ì´í„° ì „ì²˜ë¦¬ =====
def parse_array_field(value: Any) -> List[str]:
    """ë°°ì—´ í•„ë“œ íŒŒì‹± (ë¬¸ìì—´ ë˜ëŠ” ë¦¬ìŠ¤íŠ¸ í˜•íƒœ ëª¨ë‘ ì²˜ë¦¬)"""
    if value is None:
        return []
    if isinstance(value, list):
        return [str(v) for v in value if v]
    if isinstance(value, str):
        # ë¬¸ìì—´ í˜•íƒœì˜ ë°°ì—´ íŒŒì‹±: "['TV', 'ëƒ‰ì¥ê³ ']" ë˜ëŠ” "[TV, ëƒ‰ì¥ê³ ]"
        try:
            # ast.literal_eval ì‚¬ìš©
            import ast
            parsed = ast.literal_eval(value)
            if isinstance(parsed, list):
                return [str(v) for v in parsed if v]
        except:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
            pass
    return []


def parse_income(income: Any) -> Optional[int]:
    """ì†Œë“ ë¬¸ìì—´ì„ ìˆ«ìë¡œ íŒŒì‹± (ì˜ˆ: "ì›” 500~599ë§Œì›" -> 500)"""
    if income is None:
        return None
    if isinstance(income, (int, float)):
        return int(income)
    if isinstance(income, str):
        # "ì›” 500~599ë§Œì›" í˜•ì‹ì—ì„œ ì²« ë²ˆì§¸ ìˆ«ì ì¶”ì¶œ
        import re
        match = re.search(r'(\d+)', income)
        if match:
            return int(match.group(1))
    return None


def parse_car_ownership(value: Any) -> Optional[bool]:
    """ì°¨ëŸ‰ ë³´ìœ  ì—¬ë¶€ íŒŒì‹± ("ìˆë‹¤"/"ì—†ë‹¤" -> True/False)"""
    if value is None:
        return None
    if isinstance(value, bool):
        return value
    if isinstance(value, str):
        if value == "ìˆë‹¤" or "ìˆ" in value:
            return True
        elif value == "ì—†ë‹¤" or "ì—†" in value:
            return False
    return None


def preprocess_panel_data(raw_data: Dict[str, Any]) -> Dict[str, Any]:
    """panel_data.json í˜•ì‹ì˜ ë°ì´í„°ë¥¼ DB ìŠ¤í‚¤ë§ˆì— ë§ê²Œ ì „ì²˜ë¦¬"""
    panel_id_str = str(raw_data.get("panel_id", "")).strip()
    
    # ì„±ë³„ (ê·¸ëŒ€ë¡œ ì €ì¥, "ë‚¨ì„±"/"ì—¬ì„±")
    gender = raw_data.get("gender") or None
    
    # ë‚˜ì´ (floatë¥¼ intë¡œ ë³€í™˜)
    age = None
    if raw_data.get("age") is not None:
        try:
            age = int(float(raw_data.get("age", 0)))
        except:
            age = None
    
    # ì§€ì—­
    region_city = raw_data.get("region_city") or None
    region_gu = raw_data.get("region_district") or None  # region_district -> region_gu
    
    # ê²°í˜¼ ì—¬ë¶€
    marital_status = raw_data.get("marital_status") or None
    
    # ìë…€ìˆ˜
    children_count = None
    if raw_data.get("children_count") is not None:
        try:
            children_count = int(float(raw_data.get("children_count", 0)))
        except:
            children_count = None
    
    # ê°€ì¡±ìˆ˜ (family_members -> family_size)
    family_size = None
    if raw_data.get("family_members") is not None:
        try:
            family_size = int(float(raw_data.get("family_members", 0)))
        except:
            family_size = None
    
    # í•™ë ¥ (education -> education_level), ì‹ ê·œ í‚¤ë„ í—ˆìš©
    education_level = raw_data.get("education") or raw_data.get("education_level") or None
    
    # ì§ì—… (job -> occupation)
    occupation = raw_data.get("job") or None
    
    # ì†Œë“
    # êµ¬í˜•/ì‹ ê·œ í‚¤ ëª¨ë‘ í—ˆìš©
    monthly_personal_income = parse_income(
        raw_data.get("monthly_personal_income") or raw_data.get("income_personal_monthly")
    )
    monthly_household_income = parse_income(
        raw_data.get("monthly_household_monthly") or raw_data.get("income_household_monthly")
    )
    
    # íœ´ëŒ€í°
    phone_brand = raw_data.get("phone_brand") or None
    phone_model = raw_data.get("phone_model") or None
    
    # ì°¨ëŸ‰
    car_ownership = parse_car_ownership(raw_data.get("car_ownership"))
    car_manufacturer = raw_data.get("car_manufacturer") or None
    car_model = raw_data.get("car_model") or None
    
    # ë°°ì—´ í•„ë“œ íŒŒì‹±
    # êµ¬í˜•/ì‹ ê·œ í‚¤ ëª¨ë‘ í—ˆìš©
    owned_electronics = parse_array_field(
        raw_data.get("owned_electronics") or raw_data.get("electronics_owned_multi")
    )
    smoking_experience = parse_array_field(
        raw_data.get("smoking_experience") or raw_data.get("smoking_experience_multi_label")
    )
    smoking_brand = parse_array_field(
        raw_data.get("smoking_brands") or raw_data.get("smoking_brand_multi_label")
    )
    e_cig_heated_brand = parse_array_field(
        raw_data.get("heated_tobacco_brands") or raw_data.get("smoking_brand_cigarette_heat_multi_label")
    )
    e_cig_liquid_brand = parse_array_field(
        raw_data.get("liquid_ecig_brands") or raw_data.get("smoking_brand_liquid_vape_multi_label")
    )
    drinking_experience = parse_array_field(
        raw_data.get("drinking_experience") or raw_data.get("drinking_experience_multi_label")
    )
    
    return {
        "panel_id": panel_id_str,
        "gender": gender,
        "age": age,  # ì •ìˆ˜ë¡œ ë³€í™˜ë¨
        "region_city": region_city,
        "region_gu": region_gu,
        "marital_status": marital_status,
        "children_count": children_count,  # ì •ìˆ˜ë¡œ ë³€í™˜ë¨
        "family_size": family_size,  # ì •ìˆ˜ë¡œ ë³€í™˜ë¨
        "education_level": education_level,
        "occupation": occupation,
        "monthly_personal_income": monthly_personal_income,
        "monthly_household_income": monthly_household_income,
        "phone_brand": phone_brand,
        "phone_model": phone_model,
        "car_ownership": car_ownership,
        "car_manufacturer": car_manufacturer,
        "car_model": car_model,
        "owned_electronics": owned_electronics,
        "smoking_experience": smoking_experience,
        "smoking_brand": smoking_brand,
        "e_cig_heated_brand": e_cig_heated_brand,
        "e_cig_liquid_brand": e_cig_liquid_brand,
        "drinking_experience": drinking_experience,
    }


# í•„ë“œëª… â†’ í•œêµ­ì–´ ë§¤í•‘ (column_metadataì— ì—†ëŠ” í•„ë“œìš©)
FIELD_NAME_KO_MAP = {
    "fitness_management_method": "ìš´ë™ ê´€ë¦¬ ë°©ë²•",
    "chatbot_experience": "ì±—ë´‡ ì‚¬ìš© ê²½í—˜",
    "chatbot_main_purpose": "ì±—ë´‡ ì£¼ìš” ì‚¬ìš© ëª©ì ",
    "main_chatbot_used": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡",
    "preferred_chatbot": "ì„ í˜¸í•˜ëŠ” ì±—ë´‡",
    "ai_usage_field": "AI ì‚¬ìš© ë¶„ì•¼",
    "main_apps_used": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì•±",
    "ott_service_count": "ì´ìš©í•˜ëŠ” OTT ì„œë¹„ìŠ¤ ê°œìˆ˜",
    "skincare_spending_monthly": "ì›” ìŠ¤í‚¨ì¼€ì–´ ì§€ì¶œ",
    "skincare_considerations": "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ ì„ íƒ ì‹œ ê³ ë ¤ì‚¬í•­",
    "skin_satisfaction": "í”¼ë¶€ ë§Œì¡±ë„",
    "most_effective_diet_experience": "ê°€ì¥ íš¨ê³¼ì ì´ì—ˆë˜ ë‹¤ì´ì–´íŠ¸ ê²½í—˜",
    "most_saved_photos_topic": "ê°€ì¥ ë§ì´ ì €ì¥í•˜ëŠ” ì‚¬ì§„ ì£¼ì œ",
    "preferred_spending_category": "ì„ í˜¸í•˜ëŠ” ì†Œë¹„ ì¹´í…Œê³ ë¦¬",
    "high_spending_category": "ë†’ì€ ì§€ì¶œ ì¹´í…Œê³ ë¦¬",
    "preferred_new_year_gift": "ì„ í˜¸í•˜ëŠ” ì„¤ ì„ ë¬¼",
    "preferred_water_play_area": "ì„ í˜¸í•˜ëŠ” ë¬¼ë†€ì´ ì¥ì†Œ",
    "preferred_overseas_destination": "ì„ í˜¸í•˜ëŠ” í•´ì™¸ ì—¬í–‰ì§€",
    "preferred_summer_snack": "ì„ í˜¸í•˜ëŠ” ì—¬ë¦„ ê°„ì‹",
    "memorable_childhood_winter_activity": "ê¸°ì–µì— ë‚¨ëŠ” ì–´ë¦° ì‹œì ˆ ê²¨ìš¸ í™œë™",
    "travel_style": "ì—¬í–‰ ìŠ¤íƒ€ì¼",
    "traditional_market_visit_frequency": "ì „í†µì‹œì¥ ë°©ë¬¸ ë¹ˆë„",
    "main_quick_delivery_products": "ì£¼ë¡œ ì£¼ë¬¸í•˜ëŠ” í€µë°°ì†¡ ìƒí’ˆ",
    "reward_points_interest": "ë¦¬ì›Œë“œ í¬ì¸íŠ¸ ê´€ì‹¬ë„",
    "lifestyle_values": "ë¼ì´í”„ìŠ¤íƒ€ì¼ ê°€ì¹˜ê´€",
    "privacy_habits": "ê°œì¸ì •ë³´ ë³´í˜¸ ìŠµê´€",
    "reducing_plastic_bags": "ë¹„ë‹ë´‰ì§€ ì‚¬ìš© ì¤„ì´ê¸° ë°©ë²•",
    "rainy_day_coping_method": "ë¹„ ì˜¤ëŠ” ë‚  ëŒ€ì²˜ ë°©ë²•",
    "late_night_snack_method": "ì•¼ì‹ ì„­ì·¨ ë°©ë²•",
    "morning_wakeup_method": "ì•„ì¹¨ ê¸°ìƒ ë°©ë²•",
    "solo_dining_frequency": "í˜¼ë°¥ ë¹ˆë„",
    "preferred_chocolate_situation": "ì´ˆì½œë¦¿ì„ ì„ í˜¸í•˜ëŠ” ìƒí™©",
    "moving_stress_factors": "ì´ì‚¬ ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
    "conditions_for_happy_old_age": "í–‰ë³µí•œ ë…¸í›„ë¥¼ ìœ„í•œ ì¡°ê±´",
    "pets": "ë°˜ë ¤ë™ë¬¼ ê²½í—˜",
    "stress_factors": "ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
    "stress_relief_method": "ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²•",
    "summer_fashion_essential": "ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í’ˆ",
    "summer_sweat_discomfort": "ì—¬ë¦„ ë•€ ë¶ˆí¸í•¨",
    "summer_worries": "ì—¬ë¦„ ê±±ì •",
    "waste_disposal_method": "ì“°ë ˆê¸° ì²˜ë¦¬ ë°©ë²•",
}


def load_column_metadata() -> Dict[str, Dict[str, Any]]:
    """column_metadata.jsonì„ ë¡œë“œí•˜ì—¬ í•„ë“œëª… â†’ ë©”íƒ€ë°ì´í„° ë§¤í•‘ ë°˜í™˜"""
    metadata_path = Path(PROJECT_ROOT) / "backend" / "data" / "column_metadata.json"
    try:
        with open(metadata_path, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception as e:
        print(f"âš ï¸ column_metadata.json ë¡œë“œ ì‹¤íŒ¨: {e}")
        return {}


def field_name_to_korean(field_name: str) -> str:
    """í•„ë“œëª…ì„ í•œêµ­ì–´ë¡œ ë³€í™˜"""
    # ë§¤í•‘ í…Œì´ë¸” í™•ì¸
    if field_name.lower() in FIELD_NAME_KO_MAP:
        return FIELD_NAME_KO_MAP[field_name.lower()]
    
    # snake_caseë¥¼ í•œê¸€ë¡œ ë³€í™˜ ì‹œë„
    words = field_name.lower().split("_")
    # ê°„ë‹¨í•œ ì˜ì–´ ë‹¨ì–´ â†’ í•œê¸€ ë³€í™˜ (migrate_qa_format.pyì™€ ë™ì¼)
    word_map = {
        "fitness": "ìš´ë™", "management": "ê´€ë¦¬", "method": "ë°©ë²•",
        "chatbot": "ì±—ë´‡", "experience": "ê²½í—˜", "main": "ì£¼ìš”", "purpose": "ëª©ì ",
        "used": "ì‚¬ìš©", "preferred": "ì„ í˜¸", "ai": "AI", "usage": "ì‚¬ìš©", "field": "ë¶„ì•¼",
        "apps": "ì•±", "ott": "OTT", "service": "ì„œë¹„ìŠ¤", "count": "ê°œìˆ˜",
        "skincare": "ìŠ¤í‚¨ì¼€ì–´", "spending": "ì§€ì¶œ", "monthly": "ì›”",
        "considerations": "ê³ ë ¤ì‚¬í•­", "skin": "í”¼ë¶€", "satisfaction": "ë§Œì¡±ë„",
        "most": "ê°€ì¥", "effective": "íš¨ê³¼ì ì¸", "diet": "ë‹¤ì´ì–´íŠ¸",
        "saved": "ì €ì¥í•œ", "photos": "ì‚¬ì§„", "topic": "ì£¼ì œ",
        "spending": "ì§€ì¶œ", "category": "ì¹´í…Œê³ ë¦¬", "high": "ë†’ì€",
        "new": "ìƒˆí•´", "year": "ë…„", "gift": "ì„ ë¬¼",
        "water": "ë¬¼", "play": "ë†€ì´", "area": "ì¥ì†Œ",
        "overseas": "í•´ì™¸", "destination": "ì—¬í–‰ì§€",
        "summer": "ì—¬ë¦„", "snack": "ê°„ì‹",
        "memorable": "ê¸°ì–µì— ë‚¨ëŠ”", "childhood": "ì–´ë¦° ì‹œì ˆ", "winter": "ê²¨ìš¸", "activity": "í™œë™",
        "travel": "ì—¬í–‰", "style": "ìŠ¤íƒ€ì¼",
        "traditional": "ì „í†µ", "market": "ì‹œì¥", "visit": "ë°©ë¬¸", "frequency": "ë¹ˆë„",
        "quick": "í€µ", "delivery": "ë°°ì†¡", "products": "ìƒí’ˆ",
        "reward": "ë¦¬ì›Œë“œ", "points": "í¬ì¸íŠ¸", "interest": "ê´€ì‹¬ë„",
        "lifestyle": "ë¼ì´í”„ìŠ¤íƒ€ì¼", "values": "ê°€ì¹˜ê´€",
        "privacy": "ê°œì¸ì •ë³´", "habits": "ìŠµê´€",
        "reducing": "ì¤„ì´ê¸°", "plastic": "ë¹„ë‹", "bags": "ë´‰ì§€",
        "rainy": "ë¹„ ì˜¤ëŠ”", "day": "ë‚ ", "coping": "ëŒ€ì²˜",
        "late": "ëŠ¦ì€", "night": "ë°¤",
        "morning": "ì•„ì¹¨", "wakeup": "ê¸°ìƒ",
        "solo": "í˜¼ì", "dining": "ì‹ì‚¬",
        "chocolate": "ì´ˆì½œë¦¿", "situation": "ìƒí™©",
        "moving": "ì´ì‚¬", "stress": "ìŠ¤íŠ¸ë ˆìŠ¤", "factors": "ìš”ì¸",
        "conditions": "ì¡°ê±´", "for": "ì„ ìœ„í•œ", "happy": "í–‰ë³µí•œ", "old": "ë…¸í›„", "age": "ë‚˜ì´",
        "pets": "ë°˜ë ¤ë™ë¬¼",
    }
    
    translated = []
    for word in words:
        if word in word_map:
            translated.append(word_map[word])
        else:
            translated.append(word)
    
    return " ".join(translated)


def generate_question(field_name: str, metadata: Optional[Dict[str, Any]] = None) -> str:
    """í•„ë“œëª…ê³¼ ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì§ˆë¬¸ ìƒì„±
    
    Args:
        field_name: í•„ë“œëª… (ì˜ˆ: "fitness_management_method")
        metadata: column_metadata.jsonì˜ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
    
    Returns:
        ì§ˆë¬¸ í…ìŠ¤íŠ¸ (ì˜ˆ: "ìš´ë™ ê´€ë¦¬ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”?")
    """
    if metadata and "name_ko" in metadata:
        field_name_ko = metadata["name_ko"]
    else:
        field_name_ko = field_name_to_korean(field_name)
    
    # ì¡°ì‚¬ ì²˜ë¦¬
    last_char = field_name_ko[-1]
    if ord(last_char) >= 0xAC00 and ord(last_char) <= 0xD7A3:
        if (ord(last_char) - 0xAC00) % 28 == 0:
            return f"{field_name_ko}ëŠ” ë¬´ì—‡ì¸ê°€ìš”?"
        else:
            return f"{field_name_ko}ì€ ë¬´ì—‡ì¸ê°€ìš”?"
    else:
        return f"{field_name_ko}ì€ ë¬´ì—‡ì¸ê°€ìš”?"


def extract_summary_segments(panel_data: Dict[str, Any], include_question: bool = True) -> List[Dict[str, str]]:
    """ë¹„ì •í˜• ë°ì´í„°: drinking_experience_multi_label ì´í›„ì˜ ëª¨ë“  í•„ë“œë¥¼ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì¶”ì¶œ
    
    panel_data.jsonì—ì„œ drinking_experience_multi_label ì´í›„ì— ë‚˜ì˜¤ëŠ” ëª¨ë“  í•„ë“œë“¤ì„
    ë¹„ì •í˜• ë°ì´í„°ë¡œ ì²˜ë¦¬í•˜ì—¬ ê°ê°ì„ ê°œë³„ ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    
    Args:
        panel_data: íŒ¨ë„ ë°ì´í„° ë”•ì…”ë„ˆë¦¬
        include_question: ì§ˆë¬¸ì„ í¬í•¨í• ì§€ ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    
    Returns:
        ì„¸ê·¸ë¨¼íŠ¸ ë¦¬ìŠ¤íŠ¸ (segment_name, summary_text í¬í•¨)
    """
    segments = []
    
    # column_metadata ë¡œë“œ (ì§ˆë¬¸ ìƒì„±ì„ ìœ„í•´)
    column_metadata = load_column_metadata() if include_question else {}
    
    # ëª¨ë“  í‚¤ë¥¼ ìˆœì„œëŒ€ë¡œ ê°€ì ¸ì˜¤ê¸°
    all_keys = list(panel_data.keys())
    
    # drinking_experience_multi_label í•„ë“œì˜ ì¸ë±ìŠ¤ ì°¾ê¸°
    drinking_idx = next(
        (i for i, k in enumerate(all_keys) if k == "drinking_experience_multi_label"),
        -1
    )
    
    if drinking_idx < 0:
        # drinking_experience_multi_labelì´ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
        return segments
    
    # drinking_experience_multi_label ì´í›„ì˜ ëª¨ë“  í•„ë“œë¥¼ ë¹„ì •í˜• ë°ì´í„°ë¡œ ì²˜ë¦¬
    unstructured_keys = all_keys[drinking_idx + 1:]
    
    for field_name in unstructured_keys:
        value = panel_data.get(field_name)
        
        # ê°’ì´ Noneì´ ì•„ë‹ˆê³ , ë¬¸ìì—´ì´ê±°ë‚˜ ìˆ«ì/ë¶ˆë¦°ì¸ ê²½ìš° í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
        if value is not None:
            answer_text = None
            
            if isinstance(value, (str, int, float, bool)):
                text = str(value).strip()
                if text and text.lower() != "null":
                    answer_text = text
            elif isinstance(value, list):
                # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì‰¼í‘œë¡œ êµ¬ë¶„ëœ ë¬¸ìì—´ë¡œ ë³€í™˜
                text = ", ".join(str(v) for v in value if v is not None).strip()
                if text:
                    answer_text = text
            
            if answer_text:
                # ì§ˆë¬¸ ì œê±°: ë‹µë³€ í…ìŠ¤íŠ¸ë§Œ ì‚¬ìš© (ê²€ìƒ‰ ë…¸ì´ì¦ˆ ì œê±°)
                # ì§ˆë¬¸ íŒ¨í„´ ì œê±°: "~ì€ ë¬´ì—‡ì¸ê°€ìš”?", "~ëŠ” ë¬´ì—‡ì¸ê°€ìš”?" ë“±
                import re
                # ì§ˆë¬¸ íŒ¨í„´ ì œê±° (ì˜ˆ: "ìš´ë™ ê´€ë¦¬ ë°©ë²•ì€ ë¬´ì—‡ì¸ê°€ìš”? ë‹¬ë¦¬ê¸°" â†’ "ë‹¬ë¦¬ê¸°")
                question_patterns = [
                    r'^[^?]*?ì€\s*ë¬´ì—‡ì¸ê°€ìš”\s*\?',
                    r'^[^?]*?ëŠ”\s*ë¬´ì—‡ì¸ê°€ìš”\s*\?',
                    r'^[^?]*?ì„\s*ë¬´ì—‡ì¸ê°€ìš”\s*\?',
                    r'^[^?]*?ë¥¼\s*ë¬´ì—‡ì¸ê°€ìš”\s*\?',
                ]
                
                cleaned_text = answer_text
                for pattern in question_patterns:
                    cleaned_text = re.sub(pattern, '', cleaned_text, flags=re.IGNORECASE)
                
                # ì•ë’¤ ê³µë°± ì œê±°
                cleaned_text = cleaned_text.strip()
                
                # ì§ˆë¬¸ë§Œ ë‚¨ê³  ë‹µë³€ì´ ì—†ìœ¼ë©´ ì›ë³¸ ì‚¬ìš©
                if not cleaned_text:
                    cleaned_text = answer_text
                
                # ì§ˆë¬¸ ìƒì„± (ë””ë²„ê¹…/ë¡œê¹…ìš©, ì‹¤ì œ ì €ì¥ì—ëŠ” ì‚¬ìš© ì•ˆ í•¨)
                if include_question:
                    metadata = column_metadata.get(field_name)
                    question = generate_question(field_name, metadata)
                    # ì§ˆë¬¸ì€ ë¡œê¹…ìš©ìœ¼ë¡œë§Œ ì‚¬ìš©, ì‹¤ì œ ì €ì¥ì€ ë‹µë³€ë§Œ
                    summary_text = cleaned_text  # ì§ˆë¬¸ ì œê±°ëœ ë‹µë³€ë§Œ ì €ì¥
                else:
                    summary_text = cleaned_text
                
                    segments.append({
                    "segment_name": field_name.upper(),  # í•„ë“œëª…ì„ ëŒ€ë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ ì„¸ê·¸ë¨¼íŠ¸ ì´ë¦„ìœ¼ë¡œ ì‚¬ìš©
                    "summary_text": summary_text,
                    })
    
    return segments


# ===== LangChain Chain: ë¹„ì •í˜• ë°ì´í„° ì„ë² ë”© ìƒì„± =====
def create_embedding_chain(embedding_model):
    """ë¹„ì •í˜• ë°ì´í„°(ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½)ë¥¼ KoSimCSEë¡œ ì„ë² ë”©í•˜ëŠ” Chain"""
    
    def generate_embeddings(segments: List[Dict[str, str]]) -> List[Dict[str, Any]]:
        """ë¹„ì •í˜• ë°ì´í„°ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        if not embedding_model:
            return []
        
        results = []
        for segment in segments:
            try:
                embedding = embedding_model.encode(
                    segment["summary_text"],
                    convert_to_numpy=True,
                    show_progress_bar=False
                ).tolist()
                results.append({
                    **segment,
                    "embedding": embedding
                })
            except Exception as e:
                print(f"  âš ï¸ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨ ({segment['segment_name']}): {e}")
        
        return results
    
    return RunnableLambda(generate_embeddings)


# ===== LLM ìš”ì•½ ìƒì„± (íŒ¨ë„ 1-2ì¤„ í”„ë¡œí•„) =====
def create_profile_summary_chain(bedrock_llm):
    """íŒ¨ë„ JSONìœ¼ë¡œë¶€í„° 1-2ì¤„ í•µì‹¬ ìš”ì•½ í”„ë¡œí•„ ìƒì„± Chain
    
    - null ì •ë³´ëŠ” ì–¸ê¸‰ ê¸ˆì§€
    - ë¼ˆëŒ€: age, gender, job, region_city
    - ì‚´: income/ì°¨ëŸ‰/AI ì‚¬ìš©/OTT/ì—¬í–‰ì„±í–¥/ë³´ìœ ì „ìì œí’ˆ ë“± 1~2ê°œ í•µì‹¬
    """
    if not bedrock_llm:
        # LLMì´ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´ ë°˜í™˜
        return RunnableLambda(lambda panel: "")
    
    from json import dumps as json_dumps
    
    def pick_fields(panel: Dict[str, Any]) -> Dict[str, Any]:
        # ê¸°ë³¸ ë¼ˆëŒ€
        core_keys = [
            "age", "gender", "job", "region_city"
        ]
        # ì£¼ìš” ìƒì„¸
        detail_keys = [
            "income_household_monthly", "income_personal_monthly",
            "car_manufacturer", "car_model", "ai_usage_field",
            "ott_service_count", "travel_style", "lifestyle_values",
            "electronics_owned_multi"
        ]
        sel: Dict[str, Any] = {}
        for k in core_keys + detail_keys:
            if k in panel and panel.get(k) is not None:
                sel[k] = panel.get(k)
        return sel
    
    prompt = ChatPromptTemplate.from_messages([
        ("system",
         "ë„ˆëŠ” 64ê°œ ì»¬ëŸ¼ì˜ íŒ¨ë„ JSONì—ì„œ 1-2ì¤„ ìš”ì•½ í”„ë¡œí•„ì„ ì‘ì„±í•˜ëŠ” AIë‹¤. "
         "ê·œì¹™: 1) null/ê²°ì¸¡ì€ ì–¸ê¸‰ ê¸ˆì§€. 2) ê°€ëŠ¥í•˜ë©´ age, gender, job, region_cityë¥¼ ë¼ˆëŒ€ë¡œ ì‚¬ìš©. "
         "3) ê·¸ ì™¸ì—ëŠ” income/ì°¨ëŸ‰/AI ì‚¬ìš©/OTT/ì—¬í–‰ì„±í–¥/ë³´ìœ ì „ìì œí’ˆ ë“± ì„±í–¥Â·ê²½ì œë ¥Â·ë””ì§€í„¸ ìˆ˜ìš©ë„ë¥¼ ë“œëŸ¬ë‚´ëŠ” í•µì‹¬ 1~2ê°œë§Œ ì„ íƒ. "
         "4) ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´ ì„œìˆ í˜• 1~2ë¬¸ì¥, ë¶ˆë¦¿/ë¨¸ë¦¿ë§/ì‚¬ì¡± ê¸ˆì§€. 5) ê³¼ì¥/ì¶”ì¸¡ ê¸ˆì§€."),
        ("human",
         "íŒ¨ë„ JSON(ìš”ì•½ìš© í•„ë“œë§Œ ì¶”ì¶œë¨):\n{panel_json}\n\n"
         "ì¶œë ¥: í•œêµ­ì–´ 1~2ë¬¸ì¥ìœ¼ë¡œ ê°„ê²° ìš”ì•½.")
    ])
    
    chain = (
        RunnableLambda(lambda raw: {"panel_json": json_dumps(pick_fields(raw), ensure_ascii=False)})
        | prompt
        | bedrock_llm
        | StrOutputParser()
    )
    return chain




# ===== LangChain Chain: í†µí•© ETL íŒŒì´í”„ë¼ì¸ =====
def create_etl_pipeline_chain(embedding_model, enable_summary: bool = False, bedrock_llm=None):
    """ETL ì „ì²´ íŒŒì´í”„ë¼ì¸ Chain (ìš”ì•½ ìƒì„± ì œì™¸)
    
    í”„ë¡œì„¸ìŠ¤:
    1. ë°ì´í„° ì „ì²˜ë¦¬
    2. ë¹„ì •í˜• ë°ì´í„° ì¶”ì¶œ (G1~G7)
    3. ë¹„ì •í˜• ë°ì´í„° ì„ë² ë”© ìƒì„±
    4. (ì˜µì…˜) íŒ¨ë„ 1-2ì¤„ í”„ë¡œí•„ ìš”ì•½ ìƒì„±
    """
    
    # Step 1: ë°ì´í„° ì „ì²˜ë¦¬ ë° ë¹„ì •í˜• ë°ì´í„° ì¶”ì¶œ
    def preprocess_and_extract(raw_panel: Dict[str, Any]) -> Dict[str, Any]:
        """ì „ì²˜ë¦¬ ë° ë¹„ì •í˜• ë°ì´í„° ì¶”ì¶œ"""
        panel_data = preprocess_panel_data(raw_panel)
        segments = extract_summary_segments(raw_panel)
        
        return {
            "raw_panel": raw_panel,
            "panel_data": panel_data,
            "segments": segments,
        }
    
    # Step 2: ë¹„ì •í˜• ë°ì´í„° ì„ë² ë”© ìƒì„±
    embedding_chain = create_embedding_chain(embedding_model)
    # Step 3: (ì˜µì…˜) LLM ìš”ì•½ ìƒì„±
    summary_chain = create_profile_summary_chain(bedrock_llm) if enable_summary else RunnableLambda(lambda _: "")
    
    # Chain ì¡°í•©
    pipeline = (
        RunnableLambda(preprocess_and_extract)
        | RunnableLambda(lambda x: {
            **x,
            "segments_with_embeddings": embedding_chain.invoke(x["segments"])
        })
        | RunnableLambda(lambda x: {
            **x,
            "panel_summary_text": summary_chain.invoke(x["raw_panel"]) if enable_summary else ""
        })
    )
    
    return pipeline


# ===== DB ì €ì¥ =====
def save_to_db(
    panel_data: Dict[str, Any],
    segments_with_embeddings: List[Dict[str, Any]],
    db_session,
    summary_text: Optional[str] = None
):
    """DBì— íŒ¨ë„ ë°ì´í„° ì €ì¥ (ìš”ì•½ í…ìŠ¤íŠ¸ ì—†ìŒ)"""
    panel_id = panel_data["panel_id"]
    
    # 1. panels í…Œì´ë¸” ì €ì¥ ë˜ëŠ” ì—…ë°ì´íŠ¸ (ìš”ì•½ í…ìŠ¤íŠ¸ë§Œ ì—…ë°ì´íŠ¸)
    with db_session.begin():
        # ê¸°ì¡´ íŒ¨ë„ì´ ìˆìœ¼ë©´ ìš”ì•½ í…ìŠ¤íŠ¸ë§Œ ì—…ë°ì´íŠ¸, ì—†ìœ¼ë©´ ìƒˆë¡œ ì‚½ì…
        db_session.execute(
            text("""
                INSERT INTO panels (
                    panel_id, gender, age, region_city, region_gu, marital_status, children_count, family_size,
                    education_level, occupation, monthly_personal_income, monthly_household_income,
                    phone_brand, phone_model, car_ownership, car_manufacturer, car_model,
                    owned_electronics, smoking_experience, smoking_brand,
                    e_cig_heated_brand, e_cig_liquid_brand, drinking_experience,
                    panel_summary_text, search_labels
                ) VALUES (
                    :panel_id, :gender, :age, :region_city, :region_gu, :marital_status, :children_count, :family_size,
                    :education_level, :occupation, :monthly_personal_income, :monthly_household_income,
                    :phone_brand, :phone_model, :car_ownership, :car_manufacturer, :car_model,
                    :owned_electronics, :smoking_experience, :smoking_brand,
                    :e_cig_heated_brand, :e_cig_liquid_brand, :drinking_experience,
                    :panel_summary_text, :search_labels
                )
                ON CONFLICT (panel_id) 
                DO UPDATE SET 
                    -- ê¸°ë³¸ í•„ë“œê°€ NULLì¸ ê²½ìš°ì—ë§Œ ì—…ë°ì´íŠ¸ (ê¸°ì¡´ ê°’ ë³´ì¡´)
                    gender = COALESCE(panels.gender, EXCLUDED.gender),
                    age = COALESCE(panels.age, EXCLUDED.age),
                    region_city = COALESCE(panels.region_city, EXCLUDED.region_city),
                    region_gu = COALESCE(panels.region_gu, EXCLUDED.region_gu),
                    -- ìš”ì•½ í…ìŠ¤íŠ¸ëŠ” í•­ìƒ ì—…ë°ì´íŠ¸
                    panel_summary_text = EXCLUDED.panel_summary_text,
                    updated_at = NOW()
            """),
            {
                **panel_data,
                "panel_summary_text": (summary_text or None) if summary_text else None,
                "search_labels": [],
            }
        )
        # ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™” (ì†ë„ ê°œì„ )
        # print(f"  âœ“ panels í…Œì´ë¸”ì— íŒ¨ë„ '{panel_id}' {'ì—…ë°ì´íŠ¸' if summary_text else 'ì €ì¥'} ì™„ë£Œ")
    
    # 2. panel_summary_segments í…Œì´ë¸” ì €ì¥ (UPSERT)
    for segment in segments_with_embeddings:
        if "embedding" not in segment:
            continue
        
        segment_id = f"{panel_id}-{segment['segment_name']}"
        try:
            with db_session.begin():
                db_session.execute(
                    text("""
                        INSERT INTO panel_summary_segments (
                            panel_id, segment_name, summary_text, embedding, ts_vector_korean
                        ) VALUES (
                            :panel_id, :segment_name, :summary_text,
                            CAST(:embedding AS vector), to_tsvector('korean', :summary_text)
                        )
                        ON CONFLICT (panel_id, segment_name) 
                        DO UPDATE SET 
                            summary_text = EXCLUDED.summary_text,
                            embedding = EXCLUDED.embedding,
                            ts_vector_korean = EXCLUDED.ts_vector_korean
                    """),
                    {
                        "panel_id": panel_id,
                        "segment_name": segment["segment_name"],
                        "summary_text": segment["summary_text"],
                        "embedding": f"[{','.join(map(str, segment['embedding']))}]",
                    }
                )
            # ë¡œê·¸ ì¶œë ¥ ìµœì†Œí™” (ì†ë„ ê°œì„ )
            # print(f"  âœ“ ì„¸ê·¸ë¨¼íŠ¸ '{segment_id}' ì €ì¥ ì™„ë£Œ")
        except Exception as e:
            print(f"  âš ï¸ ì„¸ê·¸ë¨¼íŠ¸ '{segment_id}' ì €ì¥ ì‹¤íŒ¨: {e}")


# ===== ë©”ì¸ ETL íŒŒì´í”„ë¼ì¸ =====
def load_json_to_db(json_file_path: str):
    """panel_data.json íŒŒì¼ì„ ì½ì–´ DBì— ì ì¬ (ìš”ì•½ ìƒì„± ì—†ìŒ)"""
    print(f"ğŸ“‚ JSON íŒŒì¼ ì½ê¸°: {json_file_path}")
    with open(json_file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    if isinstance(data, dict):
        panels_data = [data]
    elif isinstance(data, list):
        panels_data = data
    else:
        raise ValueError("JSONì€ ê°ì²´ ë˜ëŠ” ê°ì²´ ë°°ì—´ì´ì–´ì•¼ í•©ë‹ˆë‹¤.")

    print(f"ğŸ“Š ì´ {len(panels_data)}ê°œ íŒ¨ë„ ë°ì´í„° ë°œê²¬")
    
    # ì´ˆê¸°í™”
    embedding_model = get_embedding_model()
    
    if embedding_model is None:
        raise RuntimeError("KoSimCSE Embedding Model ì´ˆê¸°í™” ì‹¤íŒ¨")
    
    # LLM ì´ˆê¸°í™” (ì˜µì…˜)
    enable_summary_env = os.getenv("ETL_ENABLE_SUMMARY", "false").lower() in ("1", "true", "yes")
    bedrock_llm = get_bedrock_llm() if enable_summary_env else None
    
    # LangChain Chain ìƒì„±
    etl_chain = create_etl_pipeline_chain(
        embedding_model=embedding_model,
        enable_summary=bool(bedrock_llm),
        bedrock_llm=bedrock_llm
    )
    
    db = SessionLocal()
    
    try:
        # ì´ë¯¸ ì²˜ë¦¬ëœ íŒ¨ë„ ID ì¡°íšŒ (ìŠ¤í‚µìš©)
        existing_panels_result = db.execute(text("SELECT panel_id FROM panels WHERE panel_summary_text IS NOT NULL"))
        existing_panel_ids = {row[0] for row in existing_panels_result}
        print(f"ğŸ“‹ ì´ë¯¸ ì²˜ë¦¬ëœ íŒ¨ë„: {len(existing_panel_ids)}ê°œ (ìŠ¤í‚µ)")
        
        # ì²˜ë¦¬ë˜ì§€ ì•Šì€ íŒ¨ë„ë§Œ í•„í„°ë§
        panels_to_process = [p for p in panels_data if p.get("panel_id") not in existing_panel_ids]
        print(f"ğŸ“Š ì²˜ë¦¬ ëŒ€ìƒ íŒ¨ë„: {len(panels_to_process)}ê°œ (ì „ì²´ {len(panels_data)}ê°œ ì¤‘)")
        
        if not panels_to_process:
            print("âœ… ëª¨ë“  íŒ¨ë„ì´ ì´ë¯¸ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return
        
        processed_count = 0
        # ì„±ëŠ¥ ìµœì í™”: ë°°ì¹˜ í¬ê¸° ë° ë™ì‹œì„± ì¦ê°€
        batch_size = 50  # ë°°ì¹˜ í¬ê¸° ì¦ê°€ (15 â†’ 50)
        max_concurrency = 10  # ë™ì‹œì„± ì¦ê°€ (3 â†’ 10)
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì²˜ë¦¬
        for batch_start in range(0, len(panels_to_process), batch_size):
            batch_end = min(batch_start + batch_size, len(panels_to_process))
            batch_panels = panels_to_process[batch_start:batch_end]
            
            if batch_start % 50 == 0 or batch_start == 0:
                print(f"\n[{batch_start}/{len(panels_to_process)}] ğŸ“ ë°°ì¹˜ ì²˜ë¦¬ ì¤‘... (ì§„í–‰ë¥ : {batch_start/len(panels_to_process)*100:.2f}%)")
            
            try:
                # ë°°ì¹˜ë¡œ Chain ì²˜ë¦¬ (LLM í˜¸ì¶œ ë³‘ë ¬í™”, AWS Bedrock ì œí•œ ê³ ë ¤)
                # ë™ì‹œì„± ì¦ê°€: ì„±ëŠ¥ í–¥ìƒì„ ìœ„í•´ ë™ì‹œì„± ì¦ê°€
                batch_results = etl_chain.batch(batch_panels, config={"max_concurrency": max_concurrency})
                
                # ê° ê²°ê³¼ë¥¼ DBì— ì €ì¥
                for idx, processed in enumerate(batch_results):
                    raw_panel = batch_panels[idx]
                    panel_id = raw_panel.get("panel_id", "")
                    
                    try:
                        # íƒ€ì… í™•ì¸
                        panel_data = cast(Dict[str, Any], processed.get("panel_data"))
                        segments_with_embeddings = cast(List[Dict[str, Any]], processed.get("segments_with_embeddings") or [])
                        summary_text = cast(Optional[str], processed.get("panel_summary_text")) or None
                        
                        if not isinstance(panel_data, dict):
                            print(f"  âš ï¸ íŒ¨ë„ '{panel_id}' ë°ì´í„° í˜•ì‹ ì˜¤ë¥˜: {type(panel_data)}")
                            continue
                        
                        # DB ì €ì¥ (ìš”ì•½ í…ìŠ¤íŠ¸ í¬í•¨)
                        save_to_db(
                            panel_data,
                            segments_with_embeddings,
                            db,
                            summary_text=summary_text
                        )
                        
                        processed_count += 1
                        
                    except Exception as e:
                        print(f"  âš ï¸ íŒ¨ë„ '{panel_id}' ì €ì¥ ì‹¤íŒ¨: {e}")
                        continue
                
                # ë°°ì¹˜ë§ˆë‹¤ ì»¤ë°‹í•˜ì—¬ ì§„í–‰ ìƒí™© í™•ì¸ ê°€ëŠ¥í•˜ë„ë¡
                db.commit()
                if batch_start % 50 == 0:
                    print(f"  âœ… {processed_count}ê°œ íŒ¨ë„ ì»¤ë°‹ ì™„ë£Œ (ì§„í–‰ë¥ : {processed_count/len(panels_to_process)*100:.2f}%)")
                
                # ë°°ì¹˜ ê°„ ìµœì†Œ ëŒ€ê¸°(Throttling ì™„í™”, ì„±ëŠ¥ ìµœì í™”)
                import time
                time.sleep(0.3)  # ëŒ€ê¸° ì‹œê°„ ê°ì†Œ (2ì´ˆ â†’ 0.3ì´ˆ) - ì„±ëŠ¥ í–¥ìƒ
                
            except Exception as e:
                print(f"  âš ï¸ ë°°ì¹˜ [{batch_start}-{batch_end}] ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                # ThrottlingException ë°±ì˜¤í”„ ì¬ì‹œë„
                err_str = str(e)
                if "ThrottlingException" in err_str or "Too many requests" in err_str:
                    import time, random
                    backoff = 10 + random.uniform(0, 5)  # ëŒ€ê¸° ì‹œê°„ ê°ì†Œ (15-25ì´ˆ â†’ 10-15ì´ˆ)
                    print(f"   â³ Throttling ê°ì§€. {backoff:.1f}s ëŒ€ê¸° í›„ 1íšŒ ì¬ì‹œë„í•©ë‹ˆë‹¤.")
                    time.sleep(backoff)
                    try:
                        batch_results = etl_chain.batch(batch_panels, config={"max_concurrency": 5})  # ì¬ì‹œë„ ì‹œ ë™ì‹œì„± ê°ì†Œ (10 â†’ 5)
                        for idx, processed in enumerate(batch_results):
                            raw_panel = batch_panels[idx]
                            panel_id = raw_panel.get("panel_id", "")
                            try:
                                panel_data = cast(Dict[str, Any], processed.get("panel_data"))
                                segments_with_embeddings = cast(List[Dict[str, Any]], processed.get("segments_with_embeddings") or [])
                                summary_text = cast(Optional[str], processed.get("panel_summary_text")) or None
                                
                                if isinstance(panel_data, dict):
                                    save_to_db(panel_data, segments_with_embeddings, db, summary_text=summary_text)
                                    processed_count += 1
                            except Exception as e2:
                                print(f"  âš ï¸ íŒ¨ë„ '{panel_id}' ì €ì¥ ì‹¤íŒ¨(ì¬ì‹œë„): {e2}")
                                continue
                        db.commit()
                        import time as _time
                        _time.sleep(0.5)  # ëŒ€ê¸° ì‹œê°„ ê°ì†Œ (2ì´ˆ â†’ 0.5ì´ˆ)
                        continue  # ë‹¤ìŒ ë°°ì¹˜ë¡œ
                    except Exception as e3:
                        print(f"   âš ï¸ ì¬ì‹œë„ ì‹¤íŒ¨: {e3}")
                # ê°œë³„ íŒ¨ë„ë¡œ í´ë°± ì²˜ë¦¬
                for raw_panel in batch_panels:
                    panel_id = raw_panel.get("panel_id", "")
                    try:
                        processed = cast(Dict[str, Any], etl_chain.invoke(raw_panel))
                        panel_data = cast(Dict[str, Any], processed.get("panel_data"))
                        segments_with_embeddings = cast(List[Dict[str, Any]], processed.get("segments_with_embeddings") or [])
                        summary_text = cast(Optional[str], processed.get("panel_summary_text")) or None
                        
                        if isinstance(panel_data, dict):
                            save_to_db(panel_data, segments_with_embeddings, db, summary_text=summary_text)
                            processed_count += 1
                    except Exception as e2:
                        print(f"  âš ï¸ íŒ¨ë„ '{panel_id}' í´ë°± ì²˜ë¦¬ ì‹¤íŒ¨: {e2}")
                        continue
                db.commit()
        
        # ë§ˆì§€ë§‰ ë‚¨ì€ ë°ì´í„° ì»¤ë°‹
        db.commit()
        print(f"\nâœ… ì´ {processed_count}/{len(panels_to_process)}ê°œ íŒ¨ë„ ETL ì™„ë£Œ")
        
    except Exception as e:
        db.rollback()
        print(f"âŒ ETL ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        db.close()


# ===== ë©”íƒ€ë°ì´í„° ì ì¬ í•¨ìˆ˜ë“¤ =====
# label.json í‚¤ â†’ DB ì»¬ëŸ¼ëª… ë§¤í•‘
LABEL_TO_DB_COLUMN = {
    "region_district": "region_gu",
    "family_members": "family_size",
    "job": "occupation",
    "income_personal_monthly": "monthly_personal_income",
    "income_household_monthly": "monthly_household_income",
    "electronics_owned_multi": "owned_electronics",
    "smoking_experience_multi_label": "smoking_experience",
    "smoking_brand_multi_label": "smoking_brand",
    "smoking_brand_cigarette_heat_multi_label": "e_cig_heated_brand",
    "smoking_brand_liquid_vape_multi_label": "e_cig_liquid_brand",
    "drinking_experience_multi_label": "drinking_experience",
}


def load_column_metadata(session, metadata_path: Path):
    """column_metadata.jsonì„ DBì— ì ì¬"""
    print("ğŸ“Š ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° ì ì¬ ì¤‘...")
    
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = json.load(f)
    
    for column_name, meta in metadata.items():
        # range ì²˜ë¦¬
        range_min = meta.get("range", [None, None])[0] if isinstance(meta.get("range"), list) else None
        range_max = meta.get("range", [None, None])[1] if isinstance(meta.get("range"), list) else None
        
        session.execute(
            text("""
                INSERT INTO column_metadata (
                    column_name, name_ko, name_en, type, description, unit,
                    range_min, range_max, analysis_priority, chart_types, statistics
                ) VALUES (
                    :column_name, :name_ko, :name_en, :type, :description, :unit,
                    :range_min, :range_max, :analysis_priority, :chart_types, :statistics
                )
                ON CONFLICT (column_name) DO UPDATE SET
                    name_ko = EXCLUDED.name_ko,
                    name_en = EXCLUDED.name_en,
                    type = EXCLUDED.type,
                    description = EXCLUDED.description,
                    unit = EXCLUDED.unit,
                    range_min = EXCLUDED.range_min,
                    range_max = EXCLUDED.range_max,
                    analysis_priority = EXCLUDED.analysis_priority,
                    chart_types = EXCLUDED.chart_types,
                    statistics = EXCLUDED.statistics,
                    updated_at = NOW()
            """),
            {
                "column_name": column_name,
                "name_ko": meta.get("name_ko"),
                "name_en": meta.get("name_en"),
                "type": meta.get("type"),
                "description": meta.get("description"),
                "unit": meta.get("unit"),
                "range_min": range_min,
                "range_max": range_max,
                "analysis_priority": meta.get("analysis_priority"),
                "chart_types": meta.get("chart_types", []),
                "statistics": meta.get("statistics", []),
            }
        )
    
    session.commit()
    print(f"  âœ… {len(metadata)}ê°œ ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° ì ì¬ ì™„ë£Œ")


def load_label_values(session, label_path: Path):
    """label.jsonì„ DBì— ì ì¬"""
    print("ğŸ·ï¸ ë¼ë²¨ ê°’ ì ì¬ ì¤‘...")
    
    with open(label_path, "r", encoding="utf-8") as f:
        labels = json.load(f)
    
    total_values = 0
    
    for label_key, values in labels.items():
        # label.json í‚¤ë¥¼ DB ì»¬ëŸ¼ëª…ìœ¼ë¡œ ë³€í™˜
        column_name = LABEL_TO_DB_COLUMN.get(label_key, label_key)
        
        # ë¨¼ì € ì»¬ëŸ¼ì´ column_metadataì— ìˆëŠ”ì§€ í™•ì¸
        result = session.execute(
            text("SELECT column_name FROM column_metadata WHERE column_name = :col"),
            {"col": column_name}
        ).fetchone()
        
        if not result:
            print(f"  âš ï¸ ì»¬ëŸ¼ '{column_name}' (label.json í‚¤: '{label_key}')ì´ column_metadataì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
        
        # ê¸°ì¡´ ê°’ ë¹„í™œì„±í™”
        session.execute(
            text("UPDATE label_values SET is_active = FALSE WHERE column_name = :col"),
            {"col": column_name}
        )
        
        # ìƒˆ ê°’ ì‚½ì…
        for idx, value in enumerate(values):
            # ê°’ íƒ€ì… íŒë‹¨
            if isinstance(value, (int, float)):
                value_type = "number"
                value_str = str(value)
            elif isinstance(value, bool):
                value_type = "boolean"
                value_str = str(value)
            else:
                value_type = "string"
                value_str = str(value)
            
            session.execute(
                text("""
                    INSERT INTO label_values (
                        column_name, value, value_type, display_order, is_active
                    ) VALUES (
                        :column_name, :value, :value_type, :display_order, TRUE
                    )
                    ON CONFLICT (column_name, value) DO UPDATE SET
                        value_type = EXCLUDED.value_type,
                        display_order = EXCLUDED.display_order,
                        is_active = TRUE,
                        updated_at = NOW()
                """),
                {
                    "column_name": column_name,
                    "value": value_str,
                    "value_type": value_type,
                    "display_order": idx,
                }
            )
            total_values += 1
    
    session.commit()
    print(f"  âœ… {total_values}ê°œ ë¼ë²¨ ê°’ ì ì¬ ì™„ë£Œ")


def load_category_groups(session, groups_path: Path):
    """category_groups.jsonì„ DBì— ì ì¬"""
    print("ğŸ“ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì ì¬ ì¤‘...")
    
    with open(groups_path, "r", encoding="utf-8") as f:
        groups = json.load(f)
    
    for group_key, group_data in groups.items():
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì‚½ì…
        session.execute(
            text("""
                INSERT INTO category_groups (
                    group_key, name_ko, name_en, description, analysis_focus
                ) VALUES (
                    :group_key, :name_ko, :name_en, :description, :analysis_focus
                )
                ON CONFLICT (group_key) DO UPDATE SET
                    name_ko = EXCLUDED.name_ko,
                    name_en = EXCLUDED.name_en,
                    description = EXCLUDED.description,
                    analysis_focus = EXCLUDED.analysis_focus,
                    updated_at = NOW()
            """),
            {
                "group_key": group_key,
                "name_ko": group_data.get("name_ko"),
                "name_en": group_data.get("name_en"),
                "description": group_data.get("description"),
                "analysis_focus": group_data.get("analysis_focus", []),
            }
        )
        
        # ê·¸ë£¹-ì»¬ëŸ¼ ë§¤í•‘ ì‚½ì…
        fields = group_data.get("fields", [])
        for idx, column_name in enumerate(fields):
            # ë¹„ì •í˜• í•„ë“œ ëª©ë¡ (ì„¸ê·¸ë¨¼íŠ¸ë¡œ ì €ì¥ë˜ëŠ” í•„ë“œë“¤ - column_metadataì— ì—†ì–´ë„ ë§¤í•‘ ê°€ëŠ¥)
            unstructured_fields = {
                "fitness_management_method", "skin_satisfaction", "skincare_spending_monthly",
                "skincare_considerations", "most_effective_diet_experience", "summer_worries",
                "summer_sweat_discomfort", "conditions_for_happy_old_age", "ai_usage_field",
                "most_saved_photos_topic", "ott_service_count", "main_apps_used",
                "chatbot_experience", "main_chatbot_used", "chatbot_main_purpose",
                "preferred_chatbot", "preferred_new_year_gift", "main_quick_delivery_products",
                "reward_points_interest", "preferred_spending_category", "high_spending_category",
                "preferred_water_play_area", "travel_style", "traditional_market_visit_frequency",
                "preferred_overseas_destination", "memorable_childhood_winter_activity",
                "preferred_summer_snack", "stress_factors", "stress_relief_method",
                "moving_stress_factors", "rainy_day_coping_method", "privacy_habits",
                "preferred_chocolate_situation", "waste_disposal_method", "morning_wakeup_method",
                "late_night_snack_method", "reducing_plastic_bags", "solo_dining_frequency",
                "summer_fashion_essential", "pets", "lifestyle_values"
            }
            
            # ë¹„ì •í˜• í•„ë“œëŠ” column_metadata ì²´í¬ ì—†ì´ ë§¤í•‘ ê°€ëŠ¥
            if column_name not in unstructured_fields:
                # ì •í˜• í•„ë“œëŠ” column_metadataì— ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                result = session.execute(
                    text("SELECT column_name FROM column_metadata WHERE column_name = :col"),
                    {"col": column_name}
                ).fetchone()
                
                if not result:
                    print(f"  âš ï¸ ì»¬ëŸ¼ '{column_name}'ì´ column_metadataì— ì—†ìŠµë‹ˆë‹¤. ê±´ë„ˆëœë‹ˆë‹¤.")
                    continue
            
            # ë§¤í•‘ ì‚½ì… (ë¹„ì •í˜• í•„ë“œë„ í¬í•¨)
            session.execute(
                text("""
                    INSERT INTO category_group_columns (
                        group_key, column_name, display_order
                    ) VALUES (
                        :group_key, :column_name, :display_order
                    )
                    ON CONFLICT (group_key, column_name) DO UPDATE SET
                        display_order = EXCLUDED.display_order
                """),
                {
                    "group_key": group_key,
                    "column_name": column_name,
                    "display_order": idx,
                }
            )
    
    session.commit()
    print(f"  âœ… {len(groups)}ê°œ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì ì¬ ì™„ë£Œ")


# ===== ë¹„ì •í˜• í•„ë“œ ë©”íƒ€ë°ì´í„° =====
UNSTRUCTURED_FIELDS_METADATA = {
    "fitness_management_method": {
        "name_ko": "ì²´ë ¥ ê´€ë¦¬ ë°©ë²•",
        "name_en": "Fitness Management Method",
        "type": "text",
        "description": "ì²´ë ¥ ê´€ë¦¬ ë° ìš´ë™ ë°©ë²•",
        "analysis_priority": "medium"
    },
    "skin_satisfaction": {
        "name_ko": "í”¼ë¶€ ë§Œì¡±ë„",
        "name_en": "Skin Satisfaction",
        "type": "text",
        "description": "í”¼ë¶€ ìƒíƒœì— ëŒ€í•œ ë§Œì¡±ë„",
        "analysis_priority": "low"
    },
    "skincare_spending_monthly": {
        "name_ko": "ì›” ìŠ¤í‚¨ì¼€ì–´ ì§€ì¶œ",
        "name_en": "Monthly Skincare Spending",
        "type": "text",
        "description": "ì›”ê°„ ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ ì§€ì¶œ",
        "analysis_priority": "medium"
    },
    "skincare_considerations": {
        "name_ko": "ìŠ¤í‚¨ì¼€ì–´ ê³ ë ¤ì‚¬í•­",
        "name_en": "Skincare Considerations",
        "type": "text",
        "description": "ìŠ¤í‚¨ì¼€ì–´ ì œí’ˆ ì„ íƒ ì‹œ ê³ ë ¤ì‚¬í•­",
        "analysis_priority": "medium"
    },
    "most_effective_diet_experience": {
        "name_ko": "ê°€ì¥ íš¨ê³¼ì ì¸ ë‹¤ì´ì–´íŠ¸ ê²½í—˜",
        "name_en": "Most Effective Diet Experience",
        "type": "text",
        "description": "ê°€ì¥ íš¨ê³¼ì ì´ì—ˆë˜ ë‹¤ì´ì–´íŠ¸ ë°©ë²•",
        "analysis_priority": "medium"
    },
    "summer_worries": {
        "name_ko": "ì—¬ë¦„ ê±±ì •ì‚¬í•­",
        "name_en": "Summer Worries",
        "type": "text",
        "description": "ì—¬ë¦„ì²  ê±±ì •ë˜ëŠ” ì‚¬í•­",
        "analysis_priority": "low"
    },
    "summer_sweat_discomfort": {
        "name_ko": "ì—¬ë¦„ ë•€ ë¶ˆí¸ê°",
        "name_en": "Summer Sweat Discomfort",
        "type": "text",
        "description": "ì—¬ë¦„ì²  ë•€ìœ¼ë¡œ ì¸í•œ ë¶ˆí¸ê°",
        "analysis_priority": "low"
    },
    "conditions_for_happy_old_age": {
        "name_ko": "í–‰ë³µí•œ ë…¸í›„ ì¡°ê±´",
        "name_en": "Conditions for Happy Old Age",
        "type": "text",
        "description": "í–‰ë³µí•œ ë…¸í›„ë¥¼ ìœ„í•œ ì¡°ê±´",
        "analysis_priority": "medium"
    },
    "ai_usage_field": {
        "name_ko": "AI ì‚¬ìš© ë¶„ì•¼",
        "name_en": "AI Usage Field",
        "type": "text",
        "description": "AIë¥¼ í™œìš©í•˜ëŠ” ë¶„ì•¼",
        "analysis_priority": "high"
    },
    "most_saved_photos_topic": {
        "name_ko": "ê°€ì¥ ë§ì´ ì €ì¥í•œ ì‚¬ì§„ ì£¼ì œ",
        "name_en": "Most Saved Photos Topic",
        "type": "text",
        "description": "ê°€ì¥ ë§ì´ ì €ì¥í•˜ëŠ” ì‚¬ì§„ì˜ ì£¼ì œ",
        "analysis_priority": "low"
    },
    "ott_service_count": {
        "name_ko": "OTT ì„œë¹„ìŠ¤ ì´ìš© ê°œìˆ˜",
        "name_en": "OTT Service Count",
        "type": "text",
        "description": "ì´ìš© ì¤‘ì¸ OTT ì„œë¹„ìŠ¤ ê°œìˆ˜",
        "analysis_priority": "medium"
    },
    "main_apps_used": {
        "name_ko": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì•±",
        "name_en": "Main Apps Used",
        "type": "text",
        "description": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ëª¨ë°”ì¼ ì•±",
        "analysis_priority": "medium"
    },
    "chatbot_experience": {
        "name_ko": "ì±—ë´‡ ê²½í—˜",
        "name_en": "Chatbot Experience",
        "type": "text",
        "description": "ì±—ë´‡ ì‚¬ìš© ê²½í—˜",
        "analysis_priority": "medium"
    },
    "main_chatbot_used": {
        "name_ko": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡",
        "name_en": "Main Chatbot Used",
        "type": "text",
        "description": "ì£¼ë¡œ ì‚¬ìš©í•˜ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤",
        "analysis_priority": "medium"
    },
    "chatbot_main_purpose": {
        "name_ko": "ì±—ë´‡ ì£¼ìš” ëª©ì ",
        "name_en": "Chatbot Main Purpose",
        "type": "text",
        "description": "ì±—ë´‡ ì‚¬ìš©ì˜ ì£¼ìš” ëª©ì ",
        "analysis_priority": "medium"
    },
    "preferred_chatbot": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ì±—ë´‡",
        "name_en": "Preferred Chatbot",
        "type": "text",
        "description": "ì„ í˜¸í•˜ëŠ” ì±—ë´‡ ì„œë¹„ìŠ¤",
        "analysis_priority": "medium"
    },
    "preferred_new_year_gift": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ì„¤ ì„ ë¬¼",
        "name_en": "Preferred New Year Gift",
        "type": "text",
        "description": "ì„¤ë‚ ì— ì„ í˜¸í•˜ëŠ” ì„ ë¬¼",
        "analysis_priority": "low"
    },
    "main_quick_delivery_products": {
        "name_ko": "ì£¼ë¡œ ì£¼ë¬¸í•˜ëŠ” ë¹ ë¥¸ ë°°ì†¡ ìƒí’ˆ",
        "name_en": "Main Quick Delivery Products",
        "type": "text",
        "description": "ë¹ ë¥¸ ë°°ì†¡ ì„œë¹„ìŠ¤ë¥¼ í†µí•´ ì£¼ë¡œ ì£¼ë¬¸í•˜ëŠ” ìƒí’ˆ",
        "analysis_priority": "medium"
    },
    "reward_points_interest": {
        "name_ko": "ë¦¬ì›Œë“œ í¬ì¸íŠ¸ ê´€ì‹¬ë„",
        "name_en": "Reward Points Interest",
        "type": "text",
        "description": "ë¦¬ì›Œë“œ í¬ì¸íŠ¸ì— ëŒ€í•œ ê´€ì‹¬ë„",
        "analysis_priority": "medium"
    },
    "preferred_spending_category": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ì§€ì¶œ ì¹´í…Œê³ ë¦¬",
        "name_en": "Preferred Spending Category",
        "type": "text",
        "description": "ì„ í˜¸í•˜ëŠ” ì†Œë¹„ ì¹´í…Œê³ ë¦¬",
        "analysis_priority": "high"
    },
    "high_spending_category": {
        "name_ko": "ë†’ì€ ì§€ì¶œ ì¹´í…Œê³ ë¦¬",
        "name_en": "High Spending Category",
        "type": "text",
        "description": "ì§€ì¶œì´ ë†’ì€ ì¹´í…Œê³ ë¦¬",
        "analysis_priority": "high"
    },
    "preferred_water_play_area": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ë¬¼ë†€ì´ ì¥ì†Œ",
        "name_en": "Preferred Water Play Area",
        "type": "text",
        "description": "ë¬¼ë†€ì´ë¥¼ ì„ í˜¸í•˜ëŠ” ì¥ì†Œ",
        "analysis_priority": "low"
    },
    "travel_style": {
        "name_ko": "ì—¬í–‰ ìŠ¤íƒ€ì¼",
        "name_en": "Travel Style",
        "type": "text",
        "description": "ì„ í˜¸í•˜ëŠ” ì—¬í–‰ ìŠ¤íƒ€ì¼",
        "analysis_priority": "medium"
    },
    "traditional_market_visit_frequency": {
        "name_ko": "ì „í†µì‹œì¥ ë°©ë¬¸ ë¹ˆë„",
        "name_en": "Traditional Market Visit Frequency",
        "type": "text",
        "description": "ì „í†µì‹œì¥ ë°©ë¬¸ ë¹ˆë„",
        "analysis_priority": "low"
    },
    "preferred_overseas_destination": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” í•´ì™¸ ì—¬í–‰ì§€",
        "name_en": "Preferred Overseas Destination",
        "type": "text",
        "description": "ì„ í˜¸í•˜ëŠ” í•´ì™¸ ì—¬í–‰ì§€",
        "analysis_priority": "medium"
    },
    "memorable_childhood_winter_activity": {
        "name_ko": "ê¸°ì–µì— ë‚¨ëŠ” ì–´ë¦° ì‹œì ˆ ê²¨ìš¸ í™œë™",
        "name_en": "Memorable Childhood Winter Activity",
        "type": "text",
        "description": "ì–´ë¦° ì‹œì ˆ ê²¨ìš¸ì— ê¸°ì–µì— ë‚¨ëŠ” í™œë™",
        "analysis_priority": "low"
    },
    "preferred_summer_snack": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ì—¬ë¦„ ê°„ì‹",
        "name_en": "Preferred Summer Snack",
        "type": "text",
        "description": "ì—¬ë¦„ì— ì„ í˜¸í•˜ëŠ” ê°„ì‹",
        "analysis_priority": "low"
    },
    "stress_factors": {
        "name_ko": "ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
        "name_en": "Stress Factors",
        "type": "text",
        "description": "ì£¼ìš” ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
        "analysis_priority": "high"
    },
    "stress_relief_method": {
        "name_ko": "ìŠ¤íŠ¸ë ˆìŠ¤ í•´ì†Œ ë°©ë²•",
        "name_en": "Stress Relief Method",
        "type": "text",
        "description": "ìŠ¤íŠ¸ë ˆìŠ¤ë¥¼ í•´ì†Œí•˜ëŠ” ë°©ë²•",
        "analysis_priority": "high"
    },
    "moving_stress_factors": {
        "name_ko": "ì´ì‚¬ ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
        "name_en": "Moving Stress Factors",
        "type": "text",
        "description": "ì´ì‚¬ ì‹œ ëŠë¼ëŠ” ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸",
        "analysis_priority": "low"
    },
    "rainy_day_coping_method": {
        "name_ko": "ìš°ìš¸í•œ ë‚  ëŒ€ì²˜ ë°©ë²•",
        "name_en": "Rainy Day Coping Method",
        "type": "text",
        "description": "ìš°ìš¸í•˜ê±°ë‚˜ ë¹„ ì˜¤ëŠ” ë‚  ëŒ€ì²˜ ë°©ë²•",
        "analysis_priority": "low"
    },
    "privacy_habits": {
        "name_ko": "í”„ë¼ì´ë²„ì‹œ ìŠµê´€",
        "name_en": "Privacy Habits",
        "type": "text",
        "description": "í”„ë¼ì´ë²„ì‹œ ë³´í˜¸ ìŠµê´€",
        "analysis_priority": "medium"
    },
    "preferred_chocolate_situation": {
        "name_ko": "ì„ í˜¸í•˜ëŠ” ì´ˆì½œë¦¿ ìƒí™©",
        "name_en": "Preferred Chocolate Situation",
        "type": "text",
        "description": "ì´ˆì½œë¦¿ì„ ì„ í˜¸í•˜ëŠ” ìƒí™©",
        "analysis_priority": "low"
    },
    "waste_disposal_method": {
        "name_ko": "ì“°ë ˆê¸° ì²˜ë¦¬ ë°©ë²•",
        "name_en": "Waste Disposal Method",
        "type": "text",
        "description": "ì“°ë ˆê¸° ì²˜ë¦¬ ë°©ë²•",
        "analysis_priority": "low"
    },
    "morning_wakeup_method": {
        "name_ko": "ì•„ì¹¨ ê¸°ìƒ ë°©ë²•",
        "name_en": "Morning Wakeup Method",
        "type": "text",
        "description": "ì•„ì¹¨ì— ì¼ì–´ë‚˜ëŠ” ë°©ë²•",
        "analysis_priority": "low"
    },
    "late_night_snack_method": {
        "name_ko": "ì•¼ì‹ ìŠµê´€",
        "name_en": "Late Night Snack Method",
        "type": "text",
        "description": "ì•¼ì‹ ì„­ì·¨ ìŠµê´€",
        "analysis_priority": "low"
    },
    "reducing_plastic_bags": {
        "name_ko": "ë¹„ë‹ë´‰ì§€ ì¤„ì´ê¸°",
        "name_en": "Reducing Plastic Bags",
        "type": "text",
        "description": "ë¹„ë‹ë´‰ì§€ ì‚¬ìš©ì„ ì¤„ì´ëŠ” ë°©ë²•",
        "analysis_priority": "medium"
    },
    "solo_dining_frequency": {
        "name_ko": "í˜¼ë°¥ ë¹ˆë„",
        "name_en": "Solo Dining Frequency",
        "type": "text",
        "description": "í˜¼ì ì‹ì‚¬í•˜ëŠ” ë¹ˆë„",
        "analysis_priority": "low"
    },
    "summer_fashion_essential": {
        "name_ko": "ì—¬ë¦„ íŒ¨ì…˜ í•„ìˆ˜í’ˆ",
        "name_en": "Summer Fashion Essential",
        "type": "text",
        "description": "ì—¬ë¦„ì— í•„ìˆ˜ì ì¸ íŒ¨ì…˜ ì•„ì´í…œ",
        "analysis_priority": "low"
    },
    "pets": {
        "name_ko": "ë°˜ë ¤ë™ë¬¼",
        "name_en": "Pets",
        "type": "text",
        "description": "ë°˜ë ¤ë™ë¬¼ ë³´ìœ  ì—¬ë¶€ ë° ì¢…ë¥˜",
        "analysis_priority": "low"
    },
    "lifestyle_values": {
        "name_ko": "ë¼ì´í”„ìŠ¤íƒ€ì¼ ê°€ì¹˜ê´€",
        "name_en": "Lifestyle Values",
        "type": "text",
        "description": "ë¼ì´í”„ìŠ¤íƒ€ì¼ ê°€ì¹˜ê´€ ë° ì² í•™",
        "analysis_priority": "high"
    }
}


def add_unstructured_metadata(session):
    """ë¹„ì •í˜• í•„ë“œë“¤ì„ column_metadataì— ì¶”ê°€"""
    print("ğŸ“ ë¹„ì •í˜• í•„ë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì¤‘...")
    
    for column_name, metadata in UNSTRUCTURED_FIELDS_METADATA.items():
        session.execute(
            text("""
                INSERT INTO column_metadata (
                    column_name, name_ko, name_en, type, description, 
                    analysis_priority, chart_types, statistics
                ) VALUES (
                    :column_name, :name_ko, :name_en, :type, :description,
                    :analysis_priority, :chart_types, :statistics
                )
                ON CONFLICT (column_name) DO UPDATE SET
                    name_ko = EXCLUDED.name_ko,
                    name_en = EXCLUDED.name_en,
                    type = EXCLUDED.type,
                    description = EXCLUDED.description,
                    analysis_priority = EXCLUDED.analysis_priority,
                    updated_at = NOW()
            """),
            {
                "column_name": column_name,
                "name_ko": metadata.get("name_ko"),
                "name_en": metadata.get("name_en"),
                "type": metadata.get("type", "text"),
                "description": metadata.get("description"),
                "analysis_priority": metadata.get("analysis_priority", "low"),
                "chart_types": [],
                "statistics": [],
            }
        )
    
    session.commit()
    print(f"  âœ… ì´ {len(UNSTRUCTURED_FIELDS_METADATA)}ê°œ ë¹„ì •í˜• í•„ë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ ì™„ë£Œ")


def load_all_metadata(data_dir: Path):
    """ëª¨ë“  ë©”íƒ€ë°ì´í„°ë¥¼ DBì— ì ì¬"""
    session = SessionLocal()
    
    try:
        metadata_path = data_dir / "column_metadata.json"
        label_path = data_dir / "label.json"
        groups_path = data_dir / "category_groups.json"
        
        # íŒŒì¼ ì¡´ì¬ í™•ì¸
        if not metadata_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {metadata_path}")
            return
        if not label_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {label_path}")
            return
        if not groups_path.exists():
            print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {groups_path}")
            return
        
        # 1. ë¹„ì •í˜• í•„ë“œ ë©”íƒ€ë°ì´í„° ì¶”ê°€ (ë¨¼ì € ì‹¤í–‰)
        add_unstructured_metadata(session)
        
        # 2. ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„° ì ì¬
        load_column_metadata(session, metadata_path)
        
        # 3. ë¼ë²¨ ê°’ ì ì¬
        load_label_values(session, label_path)
        
        # 4. ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì ì¬
        load_category_groups(session, groups_path)
        
        print("\nâœ… ëª¨ë“  ë©”íƒ€ë°ì´í„° ì ì¬ ì™„ë£Œ!")
        
    except Exception as e:
        session.rollback()
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        raise
    finally:
        session.close()


# ===== CLI ì§„ì…ì  =====
def main():
    parser = argparse.ArgumentParser(
        description="í†µí•© ETL íŒŒì´í”„ë¼ì¸: ë©”íƒ€ë°ì´í„° ì ì¬ ë° íŒ¨ë„ ë°ì´í„° ì ì¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì‹¤í–‰ ë‹¨ê³„:
  1. metadata  - ë©”íƒ€ë°ì´í„° ì ì¬ (ë¹„ì •í˜• í•„ë“œ, ì»¬ëŸ¼ ë©”íƒ€ë°ì´í„°, ë¼ë²¨, ì¹´í…Œê³ ë¦¬ ê·¸ë£¹)
  2. panels    - íŒ¨ë„ ë°ì´í„° ì ì¬ (ì „ì²˜ë¦¬, ì„ë² ë”©, ìš”ì•½ ìƒì„±)
  3. migration - ë§ˆì´ê·¸ë ˆì´ì…˜ (QA í˜•ì‹ ë³€í™˜, í˜•íƒœì†Œ ë¶„ì„, ì„ë² ë”© ì¬ìƒì„±)
  4. all       - ëª¨ë“  ë‹¨ê³„ ìë™ ìˆœì°¨ ì‹¤í–‰ (ê¸°ë³¸ê°’, 1â†’2 ìë™ ì§„í–‰, --auto-migration ì˜µì…˜ ì‹œ 1â†’2â†’3 ìë™ ì§„í–‰)

ì‚¬ìš© ì˜ˆì‹œ:
  # ëª¨ë“  ë‹¨ê³„ ì‹¤í–‰
  python backend/scripts/etl_pipeline.py --step all --input backend/data/panel_data.json
  
  # ë©”íƒ€ë°ì´í„°ë§Œ ì ì¬
  python backend/scripts/etl_pipeline.py --step metadata
  
  # íŒ¨ë„ ë°ì´í„°ë§Œ ì ì¬
  python backend/scripts/etl_pipeline.py --step panels --input backend/data/panel_data.json
        """
    )
    parser.add_argument(
        "--step", "-s",
        choices=["metadata", "panels", "migration", "all"],
        default="all",
        help="ì‹¤í–‰í•  ë‹¨ê³„ ì„ íƒ (ê¸°ë³¸: all, all ì„ íƒ ì‹œ ìë™ìœ¼ë¡œ ìˆœì°¨ ì§„í–‰)"
    )
    parser.add_argument(
        "--input", "-i",
        help="íŒ¨ë„ ë°ì´í„° JSON íŒŒì¼ ê²½ë¡œ (panels ë‹¨ê³„ì—ì„œ í•„ìˆ˜)"
    )
    parser.add_argument(
        "--generate-summaries",
        action="store_true",
        help="íŒ¨ë„ë³„ 1-2ì¤„ ìš”ì•½(LLM, Bedrock Haiku)ì„ ìƒì„±í•˜ì—¬ panels.panel_summary_textì— ì €ì¥ (ê¸°ë³¸: ë¹„í™œì„±í™”)"
    )
    parser.add_argument(
        "--auto-migration",
        action="store_true",
        help="all ë‹¨ê³„ ì‹¤í–‰ ì‹œ ë§ˆì´ê·¸ë ˆì´ì…˜ë„ ìë™ ì‹¤í–‰ (ê¸°ë³¸: ë¹„í™œì„±í™”, QA í˜•ì‹ ë³€í™˜, í˜•íƒœì†Œ ë¶„ì„, ì„ë² ë”© ì¬ìƒì„±)"
    )

    args = parser.parse_args()

    # íŒ¨ë„ ë°ì´í„° ì ì¬ ë‹¨ê³„ì—ì„œëŠ” ì…ë ¥ íŒŒì¼ í•„ìˆ˜
    if args.step in ["panels", "all"] and not args.input:
        parser.error("--input ì˜µì…˜ì´ í•„ìš”í•©ë‹ˆë‹¤ (panels ë˜ëŠ” all ë‹¨ê³„ ì‹¤í–‰ ì‹œ)")

    data_dir = Path(PROJECT_ROOT) / "backend" / "data"

    try:
        # ë©”íƒ€ë°ì´í„° ì ì¬ ë‹¨ê³„
        if args.step in ["metadata", "all"]:
            print("=" * 60)
            print("ğŸ“‹ ë‹¨ê³„ 1: ë©”íƒ€ë°ì´í„° ì ì¬")
            print("=" * 60)
            load_all_metadata(data_dir)
            print("âœ… ë‹¨ê³„ 1 ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤...\n")
            
            # all ëª¨ë“œì¸ ê²½ìš° ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
            if args.step == "all":
                args.step = "panels"  # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì„¤ì •

        # íŒ¨ë„ ë°ì´í„° ì ì¬ ë‹¨ê³„
        if args.step in ["panels", "all"]:
            print("=" * 60)
            print("ğŸ“Š ë‹¨ê³„ 2: íŒ¨ë„ ë°ì´í„° ì ì¬")
            print("=" * 60)
            
            # ì‹¤í–‰ í”Œë˜ê·¸ë¥¼ í™˜ê²½ ë³€ìˆ˜ë¡œë„ ì „ë‹¬(íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ì—ì„œ ì°¸ì¡°)
            if args.generate_summaries:
                os.environ["ETL_ENABLE_SUMMARY"] = "true"
            
            json_path = Path(args.input)
            if not json_path.exists():
                print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {json_path}")
                sys.exit(1)

            load_json_to_db(str(json_path))
            print("âœ… ë‹¨ê³„ 2 ì™„ë£Œ! ë‹¤ìŒ ë‹¨ê³„ë¡œ ì§„í–‰í•©ë‹ˆë‹¤...\n")
            
            # all ëª¨ë“œì´ê³  auto-migration ì˜µì…˜ì´ ìˆëŠ” ê²½ìš° ìë™ìœ¼ë¡œ ë‹¤ìŒ ë‹¨ê³„ ì§„í–‰
            if args.step == "all" and args.auto_migration:
                args.step = "migration"  # ë‹¤ìŒ ë‹¨ê³„ë¡œ ì„¤ì •
            elif args.step == "all":
                args.step = None  # ë§ˆì´ê·¸ë ˆì´ì…˜ì€ ìˆ˜ë™ ì‹¤í–‰

        # ë§ˆì´ê·¸ë ˆì´ì…˜ ë‹¨ê³„ (QA í˜•ì‹, í˜•íƒœì†Œ ë¶„ì„, ì„ë² ë”© ì¬ìƒì„±)
        if args.step in ["migration", "all"] and args.auto_migration:
            print("=" * 60)
            print("ğŸ”„ ë‹¨ê³„ 3: ë§ˆì´ê·¸ë ˆì´ì…˜ (QA í˜•ì‹ ë³€í™˜, í˜•íƒœì†Œ ë¶„ì„, ì„ë² ë”© ì¬ìƒì„±)")
            print("=" * 60)
            
            # ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸ ìë™ ì‹¤í–‰ (ìµœì í™”ëœ ë°°ì¹˜ í¬ê¸° ì‚¬ìš©)
            migration_scripts = [
                ("QA í˜•ì‹ ë³€í™˜", "backend/scripts/migrate_qa_format.py", "--batch-size", "5000"),
                ("í˜•íƒœì†Œ ë¶„ì„", "backend/scripts/migrate_tsvector_morphology.py", "--batch-size", "3000"),
                ("ì„ë² ë”© ì¬ìƒì„±", "backend/scripts/regenerate_embeddings.py", "--batch-size", "2000"),
            ]
            
            for step_name, script_path, *args in migration_scripts:
                script_full_path = Path(PROJECT_ROOT) / script_path
                if not script_full_path.exists():
                    print(f"âš ï¸  ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {script_path}")
                    continue
                
                print(f"\nğŸ“ {step_name} ì‹¤í–‰ ì¤‘... (ë°°ì¹˜ í¬ê¸°: {args[1] if len(args) > 1 else 'ê¸°ë³¸ê°’'})")
                print(f"   ìŠ¤í¬ë¦½íŠ¸: {script_path}")
                
                # ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰ (ë¹„ë™ê¸° í•¨ìˆ˜ í˜¸ì¶œ)
                try:
                    import subprocess
                    result = subprocess.run(
                        [sys.executable, str(script_full_path)] + list(args),
                        cwd=PROJECT_ROOT,
                        capture_output=False,
                        text=True,
                        env={**os.environ, "PYTHONPATH": PROJECT_ROOT}
                    )
                    if result.returncode == 0:
                        print(f"âœ… {step_name} ì™„ë£Œ!")
                    else:
                        print(f"âš ï¸  {step_name} ì‹¤íŒ¨ (ë°˜í™˜ ì½”ë“œ: {result.returncode})")
                except Exception as e:
                    print(f"âš ï¸  {step_name} ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            
            print("âœ… ë‹¨ê³„ 3 ì™„ë£Œ!\n")

        print("=" * 60)
        print("âœ… ëª¨ë“  ETL ì‘ì—… ì™„ë£Œ!")
        print("=" * 60)

    except Exception as e:
        print(f"\nâŒ ETL ì˜¤ë¥˜: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
