"""
TSVECTOR í˜•íƒœì†Œ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ìŠ¤í¬ë¦½íŠ¸

ê¸°ì¡´ panel_summary_segments í…Œì´ë¸”ì˜ summary_textë¥¼ í˜•íƒœì†Œ ë¶„ì„í•˜ì—¬
ts_vector_korean ì»¬ëŸ¼ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python backend/scripts/migrate_tsvector_morphology.py --batch-size 1000
"""
import argparse
import asyncio
import os
import sys
import time
import subprocess
import re
from pathlib import Path
from typing import Optional
from dotenv import load_dotenv
from sqlalchemy import text
from backend.repositories.database import AsyncSessionLocal

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = str(Path(__file__).resolve().parents[2])
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

load_dotenv(Path(PROJECT_ROOT) / ".env")

# í˜•íƒœì†Œ ë¶„ì„ê¸° (kiwipiepy - Java ë¶ˆí•„ìš”!)
_HAS_KIWI = False
Kiwi = None
try:
    from kiwipiepy import Kiwi
    _HAS_KIWI = True
except ImportError:
    print("âš ï¸ kiwipiepyê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì„¤ì¹˜: pip install kiwipiepy")
    
# í•˜ìœ„ í˜¸í™˜ì„±: konlpyë„ ì‹œë„
_HAS_KONLPY = False
Okt = None
try:
    from konlpy.tag import Okt
    import subprocess
    result = subprocess.run(['java', '-version'], capture_output=True, text=True, timeout=5)
    if result.returncode == 0:
        _HAS_KONLPY = True
except (ImportError, FileNotFoundError, subprocess.TimeoutExpired):
    pass


def normalize_text_morphology(text: str, kiwi_tagger=None, okt_tagger=None) -> str:
    """í˜•íƒœì†Œ ë¶„ì„ì„ í†µí•œ í…ìŠ¤íŠ¸ ì •ê·œí™”
    
    Args:
        text: ì›ë³¸ í…ìŠ¤íŠ¸
        kiwi_tagger: Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° (ìš°ì„  ì‚¬ìš©)
        okt_tagger: Okt í˜•íƒœì†Œ ë¶„ì„ê¸° (í•˜ìœ„ í˜¸í™˜ì„±)
    
    Returns:
        ì •ê·œí™”ëœ í…ìŠ¤íŠ¸ (í˜•íƒœì†Œë¡œ ë¶„ë¦¬ëœ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ)
    """
    if not text or not text.strip():
        return ""
    
    # 1ìˆœìœ„: Kiwi (Java ë¶ˆí•„ìš”)
    if kiwi_tagger:
        try:
            # Kiwi í˜•íƒœì†Œ ë¶„ì„
            result = kiwi_tagger.analyze(text)
            keywords = []
            
            for token in result[0][0]:  # ì²« ë²ˆì§¸ ë¬¸ì¥ì˜ í† í°ë“¤
                word = token.form  # í˜•íƒœì†Œ
                pos = token.tag   # í’ˆì‚¬ íƒœê·¸
                
                # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´, ìˆ«ìë§Œ í¬í•¨
                # Kiwi í’ˆì‚¬ íƒœê·¸: NNG(ì¼ë°˜ëª…ì‚¬), NNP(ê³ ìœ ëª…ì‚¬), VV(ë™ì‚¬), VA(í˜•ìš©ì‚¬), SL(ì™¸êµ­ì–´), SN(ìˆ«ì)
                if pos.startswith('NN') or pos.startswith('VV') or pos.startswith('VA') or \
                   pos == 'SL' or pos == 'SN':
                    keywords.append(word)
            
            if keywords:
                normalized = ' '.join(set(keywords))
                return normalized.strip()
        except Exception as e:
            print(f"  âš ï¸ Kiwi í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}, ë‹¤ìŒ ë°©ë²• ì‹œë„")
    
    # 2ìˆœìœ„: Okt (Java í•„ìš”)
    if okt_tagger:
        try:
            # í˜•íƒœì†Œ ë¶„ì„ (ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬ë§Œ ì¶”ì¶œ)
            morphs = okt_tagger.morphs(text, stem=True)  # ì–´ê°„ ì¶”ì¶œ
            
            # ë¶ˆìš©ì–´ ì œê±° (ì¡°ì‚¬, ì–´ë¯¸ ë“±)
            pos_tags = okt_tagger.pos(text, stem=True)
            keywords = []
            for word, pos in pos_tags:
                # ëª…ì‚¬, ë™ì‚¬, í˜•ìš©ì‚¬, ì˜ì–´, ìˆ«ìë§Œ í¬í•¨
                if pos.startswith('N') or pos.startswith('V') or pos.startswith('A') or \
                   pos == 'SL' or pos == 'SN':  # SL: ì™¸êµ­ì–´, SN: ìˆ«ì
                    keywords.append(word)
            
            # í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ í˜•íƒœì†Œë§Œ ì‚¬ìš©
            if not keywords:
                keywords = morphs
            
            # ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
            normalized = ' '.join(set(keywords))
            return normalized.strip()
        except Exception as e:
            print(f"  âš ï¸ Okt í˜•íƒœì†Œ ë¶„ì„ ì‹¤íŒ¨: {e}, ê°„ë‹¨í•œ ì •ê·œí™” ì‚¬ìš©")
    
    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ìœ¼ë©´ ê°„ë‹¨í•œ ì •ê·œí™”
    # 1. íŠ¹ìˆ˜ë¬¸ì ì œê±°
    normalized = re.sub(r'[^\w\sê°€-í£]', ' ', text)
    
    # 2. ì¡°ì‚¬/ì–´ë¯¸ íŒ¨í„´ ì œê±° (ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±)
    # "ì„/ë¥¼", "ì´/ê°€", "ì€/ëŠ”", "ì˜", "ì—", "ì—ì„œ", "ì™€/ê³¼", "ë¡œ/ìœ¼ë¡œ" ë“±
    common_particles = ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'ì˜', 'ì—', 'ì—ì„œ', 'ì™€', 'ê³¼', 'ë¡œ', 'ìœ¼ë¡œ', 
                       'ë„', 'ë§Œ', 'ê¹Œì§€', 'ë¶€í„°', 'ì—ê²Œ', 'í•œí…Œ', 'ê»˜', 'ë”ëŸ¬', 'ì—ê²Œì„œ', 'í•œí…Œì„œ']
    
    words = normalized.split()
    filtered_words = []
    for word in words:
        # ì¡°ì‚¬ ì œê±° (ë‹¨ì–´ ëì— ë¶™ì€ ì¡°ì‚¬)
        for particle in common_particles:
            if word.endswith(particle) and len(word) > len(particle):
                word = word[:-len(particle)]
                break
        
        # 2ê¸€ì ì´ìƒë§Œ í¬í•¨ (1ê¸€ìëŠ” ëŒ€ë¶€ë¶„ ì¡°ì‚¬/ì–´ë¯¸)
        if len(word) >= 2:
            filtered_words.append(word)
    
    # 3. ì¤‘ë³µ ì œê±° ë° ì •ë¦¬
    normalized = ' '.join(set(filtered_words))
    return normalized.strip()


async def create_backup_column(session, dry_run: bool = False):
    """ê¸°ì¡´ ts_vector_koreanì„ ë°±ì—…í•˜ëŠ” ì»¬ëŸ¼ ìƒì„±"""
    backup_column_name = "ts_vector_korean_backup"
    
    # ë°±ì—… ì»¬ëŸ¼ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
    check_query = text("""
        SELECT column_name 
        FROM information_schema.columns 
        WHERE table_name = 'panel_summary_segments' 
          AND column_name = :backup_name
    """)
    result = await session.execute(check_query, {"backup_name": backup_column_name})
    exists = result.scalar() is not None
    
    if exists:
        print(f"âœ… ë°±ì—… ì»¬ëŸ¼ '{backup_column_name}' ì´ë¯¸ ì¡´ì¬")
        return True
    
    if dry_run:
        print(f"âš ï¸ ë“œë¼ì´ëŸ°: ë°±ì—… ì»¬ëŸ¼ '{backup_column_name}' ìƒì„± ì˜ˆì •")
        return True
    
    try:
        # 1. ë°±ì—… ì»¬ëŸ¼ ìƒì„±
        create_backup_query = text(f"""
            ALTER TABLE panel_summary_segments 
            ADD COLUMN {backup_column_name} tsvector
        """)
        await session.execute(create_backup_query)
        await session.commit()
        
        # 2. ë°ì´í„° ë³µì‚¬
        copy_backup_query = text(f"""
            UPDATE panel_summary_segments 
            SET {backup_column_name} = ts_vector_korean 
            WHERE ts_vector_korean IS NOT NULL
        """)
        await session.execute(copy_backup_query)
        await session.commit()
        
        print(f"âœ… ë°±ì—… ì»¬ëŸ¼ '{backup_column_name}' ìƒì„± ë° ë°ì´í„° ë³µì‚¬ ì™„ë£Œ")
        return True
    except Exception as e:
        print(f"âŒ ë°±ì—… ì»¬ëŸ¼ ìƒì„± ì‹¤íŒ¨: {e}")
        await session.rollback()
        return False


async def migrate_tsvector_morphology(
    batch_size: int = 3000,  # 1000 â†’ 3000 (3ë°° ì¦ê°€, í˜•íƒœì†Œ ë¶„ì„ ë¶€í•˜ ê³ ë ¤)
    start_from: int = 0,
    dry_run: bool = False,
    create_backup: bool = True
):
    """TSVECTOR í˜•íƒœì†Œ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜
    
    Args:
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜
        start_from: ì‹œì‘ ì˜¤í”„ì…‹ (ì¬ì‹œì‘ìš©)
        dry_run: ì‹¤ì œ ì—…ë°ì´íŠ¸ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ
        create_backup: ë°±ì—… ì»¬ëŸ¼ ìƒì„± ì—¬ë¶€ (ê¸°ë³¸ê°’: True)
    """
    if not _HAS_KIWI and not _HAS_KONLPY:
        print("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        print("   ê¶Œì¥: pip install kiwipiepy (Java ë¶ˆí•„ìš”)")
        print("   ë˜ëŠ”: pip install konlpy + Java ì„¤ì¹˜")
        print("   ê°„ë‹¨í•œ ì •ê·œí™” ë°©ë²•ìœ¼ë¡œ ì§„í–‰í•©ë‹ˆë‹¤...")
    
    print("="*80)
    print("ğŸ” TSVECTOR í˜•íƒœì†Œ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì‹œì‘")
    print("="*80)
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
    print(f"ì‹œì‘ ì˜¤í”„ì…‹: {start_from}")
    print(f"ë“œë¼ì´ëŸ° ëª¨ë“œ: {dry_run}")
    print(f"ë°±ì—… ìƒì„±: {create_backup}")
    print()
    
    if not dry_run:
        print("âš ï¸ ì£¼ì˜ì‚¬í•­:")
        print("   - ê¸°ì¡´ 'ts_vector_korean' ì»¬ëŸ¼ì˜ ë°ì´í„°ê°€ ì—…ë°ì´íŠ¸ë©ë‹ˆë‹¤")
        if create_backup:
            print("   - ë°±ì—… ì»¬ëŸ¼ 'ts_vector_korean_backup'ì´ ìë™ ìƒì„±ë©ë‹ˆë‹¤")
            print("   - ë¬¸ì œ ë°œìƒ ì‹œ ë°±ì—…ì—ì„œ ë³µêµ¬ ê°€ëŠ¥í•©ë‹ˆë‹¤")
        else:
            print("   - âš ï¸ ë°±ì—… ì—†ì´ ì§„í–‰ë©ë‹ˆë‹¤ (ìœ„í—˜!)")
        print()
    
    # í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” (ìš°ì„ ìˆœìœ„: Kiwi > Okt > ê°„ë‹¨í•œ ì •ê·œí™”)
    kiwi_tagger = None
    okt_tagger = None
    
    # ì‚¬ìš©ì ì‚¬ì „ì— ì¶”ê°€í•  ì‹ ì¡°ì–´/ì™¸ë˜ì–´ ëª©ë¡
    user_dictionary = [
        # ì‹ ì¡°ì–´/ë¸Œëœë“œëª…
        ("ë§¥ì‹œë©€ë¦¬ìŠ¤íŠ¸", "NNG"),  # ì¼ë°˜ëª…ì‚¬
        ("ChatGPT", "SL"),  # ì™¸êµ­ì–´
        ("OTT", "SL"),  # ì™¸êµ­ì–´
        ("AI", "SL"),  # ì™¸êµ­ì–´
        ("ìŠ¤í‚¨ì¼€ì–´", "NNG"),  # ë³µí•©ì–´
        ("ë¼ì´í”„ìŠ¤íƒ€ì¼", "NNG"),  # ë³µí•©ì–´
        ("í€µë°°ì†¡", "NNG"),  # ë³µí•©ì–´
        ("ì „ê¸°ìš”ê¸ˆ", "NNG"),  # ë³µí•©ì–´
        ("ì„ ê¸€ë¼ìŠ¤", "NNG"),  # ë³µí•©ì–´
        ("ë°˜ë°”ì§€", "NNG"),  # ë³µí•©ì–´
        ("í˜¼ë°¥", "NNG"),  # ì‹ ì¡°ì–´
        ("í˜¼ì", "NNG"),  # ëª…ì‚¬í™”
        ("ë…¸í›„", "NNG"),  # ë³µí•©ì–´
        ("ê²½ì œë ¥", "NNG"),  # ë³µí•©ì–´
    ]
    
    if _HAS_KIWI:
        print("ğŸ“š Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘... (Java ë¶ˆí•„ìš”)")
        try:
            kiwi_tagger = Kiwi()
            
            # ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€
            print("ğŸ“– ì‚¬ìš©ì ì‚¬ì „ ì¶”ê°€ ì¤‘...")
            for word, pos in user_dictionary:
                try:
                    kiwi_tagger.add_user_word(word, pos)
                    print(f"   âœ“ {word} ({pos})")
                except Exception as e:
                    print(f"   âš ï¸ {word} ì¶”ê°€ ì‹¤íŒ¨: {e}")
            
            print("âœ… Kiwi í˜•íƒœì†Œ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ (ì‚¬ìš©ì ì‚¬ì „ í¬í•¨)\n")
        except Exception as e:
            print(f"âš ï¸ Kiwi ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            kiwi_tagger = None
    
    if not kiwi_tagger and _HAS_KONLPY:
        print("ğŸ“š Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì´ˆê¸°í™” ì¤‘... (Java í•„ìš”)")
        try:
            okt_tagger = Okt()
            print("âœ… Okt í˜•íƒœì†Œ ë¶„ì„ê¸° ì¤€ë¹„ ì™„ë£Œ\n")
        except Exception as e:
            print(f"âš ï¸ Okt ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            okt_tagger = None
    
    if not kiwi_tagger and not okt_tagger:
        print("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸° ì—†ìŒ - ê°„ë‹¨í•œ ì •ê·œí™” ë°©ë²• ì‚¬ìš©\n")
    
    async with AsyncSessionLocal() as session:
        # ë°±ì—… ì»¬ëŸ¼ ìƒì„± (ì•ˆì „ì„±)
        if create_backup and not dry_run:
            print("ğŸ’¾ ê¸°ì¡´ ë°ì´í„° ë°±ì—… ì¤‘...")
            backup_success = await create_backup_column(session, dry_run=False)
            if not backup_success:
                print("âš ï¸ ë°±ì—… ì‹¤íŒ¨í–ˆì§€ë§Œ ê³„ì† ì§„í–‰í•©ë‹ˆë‹¤...")
            print()
        
        # ì „ì²´ ê°œìˆ˜ í™•ì¸
        count_query = text("""
            SELECT COUNT(*) 
            FROM panel_summary_segments 
            WHERE summary_text IS NOT NULL
        """)
        result = await session.execute(count_query)
        total_count = result.scalar()
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ëŒ€ìƒ: {total_count:,}ê°œ\n")
        
        if start_from >= total_count:
            print(f"âš ï¸ ì‹œì‘ ì˜¤í”„ì…‹({start_from})ì´ ì´ ê°œìˆ˜({total_count})ë³´ë‹¤ í½ë‹ˆë‹¤.")
            return
        
        processed = 0
        updated = 0
        errors = 0
        start_time = time.time()
        
        # ë°°ì¹˜ ì²˜ë¦¬
        offset = start_from
        while offset < total_count:
            batch_start_time = time.time()
            
            # ë°°ì¹˜ ë°ì´í„° ì¡°íšŒ
            select_query = text("""
                SELECT 
                    panel_id,
                    segment_name,
                    summary_text,
                    ts_vector_korean
                FROM panel_summary_segments
                WHERE summary_text IS NOT NULL
                ORDER BY panel_id, segment_name
                LIMIT :limit OFFSET :offset
            """)
            result = await session.execute(select_query, {
                "limit": batch_size,
                "offset": offset
            })
            rows = result.fetchall()
            
            if not rows:
                break
            
            batch_num = offset // batch_size + 1
            print(f"\n--- ë°°ì¹˜ {batch_num} (ì˜¤í”„ì…‹: {offset:,} ~ {offset + len(rows):,}) ---")
            sys.stdout.flush()  # ì¶œë ¥ ì¦‰ì‹œ ë°˜ì˜
            
            # ë°°ì¹˜ ë‚´ ê° ë ˆì½”ë“œ ì²˜ë¦¬
            batch_updates = []
            update_params = []
            processed_in_batch = 0
            for i, (panel_id, segment_name, summary_text, old_tsvector) in enumerate(rows):
                try:
                    # í˜•íƒœì†Œ ë¶„ì„ (Kiwi ìš°ì„ , Okt í•˜ìœ„ í˜¸í™˜)
                    normalized_text = normalize_text_morphology(
                        summary_text, 
                        kiwi_tagger=kiwi_tagger,
                        okt_tagger=okt_tagger
                    )
                    
                    if not normalized_text:
                        continue
                    
                    # bulk updateë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ìˆ˜ì§‘
                    update_params.append({
                        "normalized_text": normalized_text,
                        "panel_id": panel_id,
                        "segment_name": segment_name
                    })
                    
                    # í˜•íƒœì†Œ ë¶„ì„ ìƒì„¸ ì •ë³´ (Kiwi ì‚¬ìš© ì‹œ)
                    morphs_info = ""
                    if kiwi_tagger and offset == start_from and i < 5:
                        try:
                            result_kiwi = kiwi_tagger.analyze(summary_text)
                            morphs_list = [f"{t.form}({t.tag})" for t in result_kiwi[0][0][:8]]
                            morphs_info = " | ".join(morphs_list)
                        except:
                            pass
                    
                    batch_updates.append({
                        "panel_id": panel_id,
                        "segment_name": segment_name,
                        "original": summary_text,
                        "normalized": normalized_text,
                        "morphs_info": morphs_info
                    })
                    updated += 1
                    processed_in_batch += 1
                    
                    # 100ê°œë§ˆë‹¤ ì§„í–‰ ìƒí™© ì¶œë ¥ (í° ë°°ì¹˜ì˜ ê²½ìš°)
                    if batch_size >= 1000 and processed_in_batch % 100 == 0:
                        print(f"    ì§„í–‰: {processed_in_batch}/{len(rows)}ê°œ ì²˜ë¦¬ ì¤‘...", flush=True)
                    
                except Exception as e:
                    errors += 1
                    print(f"  âŒ ì˜¤ë¥˜ (Panel {panel_id}, Segment {segment_name}): {e}", flush=True)
            
            # Bulk update ì‹¤í–‰ (executemany ì‚¬ìš©)
            if not dry_run and update_params:
                try:
                    update_query = text("""
                        UPDATE panel_summary_segments
                        SET ts_vector_korean = to_tsvector('korean', :normalized_text)
                        WHERE panel_id = :panel_id 
                          AND segment_name = :segment_name
                    """)
                    # executemanyë¡œ í•œ ë²ˆì— ì‹¤í–‰
                    await session.execute(update_query, update_params)
                    await session.commit()
                except Exception as e:
                    print(f"  âŒ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", flush=True)
                    await session.rollback()
                    errors += len(update_params)
            
            processed += len(rows)
            batch_time = time.time() - batch_start_time
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            progress = (processed / total_count) * 100
            elapsed = time.time() - start_time
            avg_time_per_record = elapsed / processed if processed > 0 else 0
            remaining = (total_count - processed) * avg_time_per_record
            
            print(f"  âœ… ì²˜ë¦¬: {len(rows)}ê°œ (ì—…ë°ì´íŠ¸: {len(batch_updates)}ê°œ, ì˜¤ë¥˜: {errors}ê°œ)", flush=True)
            print(f"  â±ï¸ ë°°ì¹˜ ì‹œê°„: {batch_time:.2f}ì´ˆ", flush=True)
            print(f"  ğŸ“ˆ ì „ì²´ ì§„í–‰: {processed:,}/{total_count:,} ({progress:.1f}%)", flush=True)
            print(f"  â³ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„", flush=True)
            
            # ìƒ˜í”Œ ì¶œë ¥ (ì²« ë°°ì¹˜ë§Œ, ë” ìì„¸í•˜ê²Œ)
            if offset == start_from and batch_updates:
                print(f"\n  ğŸ“ ìƒ˜í”Œ ë³€í™˜ (ìƒì„¸):")
                for sample in batch_updates[:5]:
                    print(f"\n    Segment: {sample['segment_name']}")
                    print(f"    ì›ë³¸: {sample['original']}")
                    if sample.get('morphs_info'):
                        print(f"    í˜•íƒœì†Œ ë¶„ì„: {sample['morphs_info']}...")
                    print(f"    ì •ê·œí™”: {sample['normalized']}")
                    
                    # ë³€ê²½ ì—¬ë¶€ í™•ì¸
                    orig = sample['original'].strip()
                    norm = sample['normalized'].strip()
                    if orig != norm:
                        # ì¡°ì‚¬/ì–´ë¯¸ê°€ ì œê±°ë˜ì—ˆëŠ”ì§€ í™•ì¸
                        has_particles = any(p in orig for p in ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”', 'í•©ë‹ˆë‹¤', 'ë‹ˆë‹¤', 'í•˜ëŠ”'])
                        if has_particles and not any(p in norm for p in ['ì„', 'ë¥¼', 'ì´', 'ê°€', 'ì€', 'ëŠ”']):
                            print(f"    âœ… ì¡°ì‚¬/ì–´ë¯¸ ì œê±°ë¨ (í˜•íƒœì†Œ ë¶„ì„ ì„±ê³µ)")
                        else:
                            print(f"    âœ… í˜•íƒœì†Œ ë¶„ì„ ì ìš©ë¨ (í‚¤ì›Œë“œ ì¶”ì¶œ)")
                    else:
                        print(f"    â„¹ï¸ ë³€ê²½ ì—†ìŒ (ì´ë¯¸ ì •ê·œí™”ë¨)")
            
            offset += batch_size
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (DB ë¶€í•˜ ë°©ì§€)
            if not dry_run:
                await asyncio.sleep(0.1)
        
        # ìµœì¢… í†µê³„
        total_time = time.time() - start_time
        print("\n" + "="*80)
        print("ğŸ‰ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ!")
        print("="*80)
        print(f"âœ… ì´ ì²˜ë¦¬: {processed:,}ê°œ")
        print(f"âœ… ì—…ë°ì´íŠ¸: {updated:,}ê°œ")
        print(f"âš ï¸ ì˜¤ë¥˜: {errors}ê°œ")
        print(f"â±ï¸ ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„ ({total_time:.1f}ì´ˆ)")
        print(f"ğŸ“Š í‰ê·  ì²˜ë¦¬ ì†ë„: {processed/total_time:.1f}ê°œ/ì´ˆ")
        
        if dry_run:
            print("\nâš ï¸ ë“œë¼ì´ëŸ° ëª¨ë“œì˜€ìŠµë‹ˆë‹¤. ì‹¤ì œ ì—…ë°ì´íŠ¸ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        else:
            print("\n" + "="*80)
            print("âœ…âœ…âœ… TSVECTOR í˜•íƒœì†Œ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜ ì™„ë£Œ! âœ…âœ…âœ…")
            print("="*80)
            print(f"ğŸ“ {updated:,}ê°œì˜ ì„¸ê·¸ë¨¼íŠ¸ì— í˜•íƒœì†Œ ë¶„ì„ì´ ì ìš©ë˜ì—ˆìŠµë‹ˆë‹¤.")
            print("ğŸ” ì´ì œ FTS ê²€ìƒ‰ì´ ë” ì •í™•í•˜ê²Œ ì‘ë™í•©ë‹ˆë‹¤!")
            print("="*80)
        
        if create_backup and not dry_run:
            print("\nğŸ’¾ ë°±ì—… ì •ë³´:")
            print(f"   ğŸ“¦ ë°±ì—… ì»¬ëŸ¼: ts_vector_korean_backup")
            print(f"   ğŸ”„ ë³µêµ¬ ë°©ë²•: UPDATE panel_summary_segments SET ts_vector_korean = ts_vector_korean_backup;")
            print(f"   ğŸ—‘ï¸ ë°±ì—… ì‚­ì œ: ALTER TABLE panel_summary_segments DROP COLUMN ts_vector_korean_backup;")
            print()


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="TSVECTOR í˜•íƒœì†Œ ë¶„ì„ ë§ˆì´ê·¸ë ˆì´ì…˜")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)"
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=0,
        help="ì‹œì‘ ì˜¤í”„ì…‹ (ì¬ì‹œì‘ìš©, ê¸°ë³¸ê°’: 0)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì—…ë°ì´íŠ¸ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"
    )
    parser.add_argument(
        "--no-backup",
        action="store_true",
        help="ë°±ì—… ì»¬ëŸ¼ ìƒì„± ì•ˆ í•¨ (ê¸°ë³¸ê°’: ë°±ì—… ìƒì„±)"
    )
    
    args = parser.parse_args()
    
    try:
        await migrate_tsvector_morphology(
            batch_size=args.batch_size,
            start_from=args.start_from,
            dry_run=args.dry_run,
            create_backup=not args.no_backup  # ê¸°ë³¸ê°’: True (ë°±ì—… ìƒì„±)
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë‹¤ìŒ ì‹¤í–‰ ì‹œ --start-from {args.start_from} ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

