"""
ì„ë² ë”© ì¬ìƒì„± ìŠ¤í¬ë¦½íŠ¸

ì§ˆë¬¸-ë‹µë³€ í˜•ì‹ìœ¼ë¡œ ë³€ê²½ëœ summary_textì— ëŒ€í•´ ì„ë² ë”©ì„ ì¬ìƒì„±í•©ë‹ˆë‹¤.

ì‚¬ìš© ì˜ˆì‹œ:
    python backend/scripts/regenerate_embeddings.py --batch-size 1000
"""

import argparse
import asyncio
import sys
import time
from pathlib import Path
from typing import Any, List

from dotenv import load_dotenv
from sqlalchemy import text

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
PROJECT_ROOT = str(Path(__file__).resolve().parents[2])
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)

from backend.repositories.database import AsyncSessionLocal

load_dotenv(Path(PROJECT_ROOT) / ".env")

# KoSimCSE ì„ë² ë”© ëª¨ë¸
try:
    from sentence_transformers import SentenceTransformer
    _HAS_KOSIMCSE = True
except ImportError:
    SentenceTransformer = None
    _HAS_KOSIMCSE = False


async def regenerate_embeddings(
    batch_size: int = 2000,  # 1000 â†’ 2000 (2ë°° ì¦ê°€, ì„ë² ë”© ëª¨ë¸ ë¶€í•˜ ê³ ë ¤)
    start_from: int = 0,
    dry_run: bool = False
):
    """ì„ë² ë”© ì¬ìƒì„±
    
    Args:
        batch_size: í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜
        start_from: ì‹œì‘ ì˜¤í”„ì…‹ (ì¬ì‹œì‘ìš©)
        dry_run: ì‹¤ì œ ì—…ë°ì´íŠ¸ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ
    """
    if not _HAS_KOSIMCSE:
        print("âŒ sentence_transformersê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        print("   ì„¤ì¹˜: pip install sentence-transformers")
        return
    
    print("="*80)
    print("ğŸ” ì„ë² ë”© ì¬ìƒì„± ì‹œì‘")
    print("="*80)
    print(f"ë°°ì¹˜ í¬ê¸°: {batch_size}ê°œ")
    print(f"ì‹œì‘ ì˜¤í”„ì…‹: {start_from}")
    print(f"ë“œë¼ì´ëŸ° ëª¨ë“œ: {dry_run}")
    print()
    
    # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("ğŸ“š ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    try:
        embedding_model = SentenceTransformer('BM-K/KoSimCSE-roberta-multitask')
        print("âœ… KoSimCSE ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    async with AsyncSessionLocal() as session:
        # ì „ì²´ ê°œìˆ˜ í™•ì¸
        count_query = text("""
            SELECT COUNT(*) 
            FROM panel_summary_segments 
            WHERE summary_text IS NOT NULL
        """)
        result = await session.execute(count_query)
        total_count = result.scalar()
        print(f"ğŸ“Š ì´ ì²˜ë¦¬ ëŒ€ìƒ: {total_count:,}ê°œ\n")
        
        start_time = time.time()
        processed = 0
        updated = 0
        errors = 0
        offset = start_from
        
        while offset < total_count:
            batch_start_time = time.time()
            
            # ë°°ì¹˜ ì¡°íšŒ
            query = text("""
                SELECT panel_id, segment_name, summary_text
                FROM panel_summary_segments
                WHERE summary_text IS NOT NULL
                ORDER BY panel_id, segment_name
                LIMIT :limit_val OFFSET :offset_val
            """)
            result = await session.execute(query, {
                "limit_val": batch_size,
                "offset_val": offset
            })
            rows = result.fetchall()
            
            if not rows:
                break
            
            # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì„ë² ë”© ìƒì„±
            batch_texts = [row[2] for row in rows]
            try:
                embeddings = embedding_model.encode(
                    batch_texts,
                    convert_to_numpy=True,
                    show_progress_bar=False,
                    batch_size=32
                )
            except Exception as e:
                print(f"  âŒ ì„ë² ë”© ìƒì„± ì‹¤íŒ¨: {e}")
                errors += len(rows)
                offset += batch_size
                continue
            
            # ë°°ì¹˜ ì—…ë°ì´íŠ¸ - bulk updateë¡œ ìµœì í™”
            update_params = []
            for i, (panel_id, segment_name, summary_text) in enumerate(rows):
                try:
                    embedding = embeddings[i]
                    embedding_str = f"[{','.join(map(str, embedding))}]"
                    
                    update_params.append({
                        "embedding": embedding_str,
                        "panel_id": panel_id,
                        "segment_name": segment_name
                    })
                    
                    updated += 1
                    
                except Exception as e:
                    errors += 1
                    print(f"  âŒ ì˜¤ë¥˜ (Panel {panel_id}, Segment {segment_name}): {e}", flush=True)
            
            # Bulk update ì‹¤í–‰ (executemany ì‚¬ìš©)
            if not dry_run and update_params:
                try:
                    update_query = text("""
                        UPDATE panel_summary_segments
                        SET embedding = CAST(:embedding AS vector),
                            updated_at = NOW()
                        WHERE panel_id = :panel_id 
                          AND segment_name = :segment_name
                    """)
                    # executemanyë¡œ í•œ ë²ˆì— ì‹¤í–‰
                    await session.execute(update_query, update_params)
                    await session.commit()
                except Exception as e:
                    print(f"  âŒ ë°°ì¹˜ ì—…ë°ì´íŠ¸ ì‹¤íŒ¨: {e}", flush=True)
                    await session.rollback()
                    errors += len(update_params)
            
            processed += len(rows)
            batch_time = time.time() - batch_start_time
            
            # ì§„í–‰ ìƒí™© ì¶œë ¥
            progress = (processed / total_count) * 100
            elapsed = time.time() - start_time
            avg_time_per_record = elapsed / processed if processed > 0 else 0
            remaining = (total_count - processed) * avg_time_per_record
            
            print(f"--- ë°°ì¹˜ {offset // batch_size + 1} (ì˜¤í”„ì…‹: {offset:,} ~ {offset + len(rows):,}) ---", flush=True)
            print(f"  âœ… ì²˜ë¦¬: {len(rows)}ê°œ (ì—…ë°ì´íŠ¸: {updated}ê°œ, ì˜¤ë¥˜: {errors}ê°œ)", flush=True)
            print(f"  â±ï¸ ë°°ì¹˜ ì‹œê°„: {batch_time:.2f}ì´ˆ", flush=True)
            print(f"  ğŸ“ˆ ì „ì²´ ì§„í–‰: {processed:,}/{total_count:,} ({progress:.1f}%)", flush=True)
            print(f"  â³ ì˜ˆìƒ ë‚¨ì€ ì‹œê°„: {remaining/60:.1f}ë¶„", flush=True)
            
            offset += batch_size
            
            # ë°°ì¹˜ ê°„ ì§§ì€ ëŒ€ê¸° (GPU/CPU ë¶€í•˜ ë°©ì§€)
            if not dry_run:
                await asyncio.sleep(0.1)
        
        # ìµœì¢… í†µê³„
        total_time = time.time() - start_time
        print("\n" + "="*80)
        print("âœ… ì„ë² ë”© ì¬ìƒì„± ì™„ë£Œ!")
        print("="*80)
        print(f"ì´ ì²˜ë¦¬: {processed:,}ê°œ")
        print(f"ì—…ë°ì´íŠ¸: {updated:,}ê°œ")
        print(f"ì˜¤ë¥˜: {errors}ê°œ")
        print(f"ì´ ì†Œìš” ì‹œê°„: {total_time/60:.1f}ë¶„")
        print(f"í‰ê·  ì²˜ë¦¬ ì†ë„: {processed/total_time:.1f}ê°œ/ì´ˆ")
        
        if dry_run:
            print("\nâš ï¸ ë“œë¼ì´ëŸ° ëª¨ë“œì˜€ìŠµë‹ˆë‹¤. ì‹¤ì œ ì—…ë°ì´íŠ¸ëŠ” ìˆ˜í–‰ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


async def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    parser = argparse.ArgumentParser(description="ì„ë² ë”© ì¬ìƒì„±")
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="í•œ ë²ˆì— ì²˜ë¦¬í•  ë ˆì½”ë“œ ìˆ˜ (ê¸°ë³¸ê°’: 1000)"
    )
    parser.add_argument(
        "--start-from",
        type=int,
        default=0,
        help="ì‹œì‘ ì˜¤í”„ì…‹ (ì¬ì‹œì‘ìš©, ê¸°ë³¸ê°’: 0)"
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì—…ë°ì´íŠ¸ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"
    )
    
    args = parser.parse_args()
    
    try:
        await regenerate_embeddings(
            batch_size=args.batch_size,
            start_from=args.start_from,
            dry_run=args.dry_run
        )
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ë‹¤ìŒ ì‹¤í–‰ ì‹œ --start-from {args.start_from} ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œì‘í•˜ì„¸ìš”.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    asyncio.run(main())

