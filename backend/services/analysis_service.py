"""
RAG ê¸°ë°˜ íŒ¨ë„ ë°ì´í„° ë¶„ì„ ì„œë¹„ìŠ¤

ê²€ìƒ‰ëœ íŒ¨ë„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì¸ì‚¬ì´íŠ¸, ì°¨íŠ¸ ì¶”ì²œ, ë¹„êµêµ° ì¶”ì²œì„ ì œê³µ
"""

from typing import Any, Dict, List, Optional
import json
import asyncio
import random
from sqlalchemy.ext.asyncio import AsyncSession

from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import JsonOutputParser, StrOutputParser
from botocore.exceptions import ClientError  # type: ignore

from backend.repositories.panel_repository import PanelRepository
from backend.repositories.database import AsyncSessionLocal
from backend.services.statistics_calculator import StatisticsCalculator
from backend.services.comparison_group_finder import ComparisonGroupFinder
from backend.services.metadata_loader import MetadataLoader
from backend.services.search_service import get_bedrock_llm


class AnalysisService:
    """RAG ê¸°ë°˜ íŒ¨ë„ ë¶„ì„ ì„œë¹„ìŠ¤"""
    
    def __init__(self):
        self.panel_repo = PanelRepository()
        self.stats_calculator = StatisticsCalculator()
        self.comparison_finder = ComparisonGroupFinder()
        self.metadata_loader = MetadataLoader()
        self.llm = get_bedrock_llm(model_id="anthropic.claude-3-5-sonnet-20241022-v2:0")
    
    async def analyze_panels(
        self,
        panel_ids: List[str],
        analysis_type: str = "comprehensive",
        focus_areas: Optional[List[str]] = None,
        query: Optional[str] = None,
        requested_count: Optional[int] = None,
        session: Optional[AsyncSession] = None,
    ) -> Dict[str, Any]:
        """RAG ê¸°ë°˜ íŒ¨ë„ ë¶„ì„
        
        Args:
            panel_ids: ë¶„ì„í•  íŒ¨ë„ ID ë¦¬ìŠ¤íŠ¸
            analysis_type: ë¶„ì„ íƒ€ì… ("basic" | "comprehensive" | "custom")
            focus_areas: ë¶„ì„ ëŒ€ìƒ ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ (ì˜ˆ: ["demographics", "economic"])
            session: DB ì„¸ì…˜ (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        close_session = False
        if session is None:
            session = AsyncSessionLocal()
            close_session = True
        
        try:
            total_panel_count = len(panel_ids)
            
            # [ìµœì í™” 1] ìƒ˜í”Œë§ ê°œìˆ˜ë¥¼ 300ëª…ìœ¼ë¡œ ì¶•ì†Œ (ì‹œì—° ì†ë„ ìµœì í™”)
            # 500ëª… -> 300ëª…ìœ¼ë¡œ ì¤„ì—¬ì„œ ë¶„ì„ ì†ë„ ëŒ€í­ í–¥ìƒ (ì‹œì—°ìš©)
            SAMPLE_LIMIT = 300
            if total_panel_count > SAMPLE_LIMIT:
                print(f"âš¡ ë¶„ì„ ìµœì í™”: {total_panel_count}ëª… -> {SAMPLE_LIMIT}ëª… ìƒ˜í”Œë§ (ì‹œì—° ì†ë„ ìµœì í™”)")
                random.seed(42)
                target_panel_ids = random.sample(panel_ids, SAMPLE_LIMIT)
            else:
                target_panel_ids = panel_ids
            
            # 1. Retrieval: ìƒ˜í”Œë§ëœ ë°ì´í„°ë§Œ DBì—ì„œ ì¡°íšŒ
            raw_panels = await self._retrieve_panel_data(target_panel_ids, session)
            
            # [ì¤‘ìš”] ORM ê°ì²´ -> Dict ë³€í™˜ (Lazy Loading ë°©ì§€)
            # Repositoryì—ì„œ ê°€ì ¸ì˜¨ ë°ì´í„°ê°€ ORM ê°ì²´ì¸ ê²½ìš° dictë¡œ ë³€í™˜í•˜ì—¬
            # ì´í›„ ë£¨í”„ì—ì„œ DB ì¿¼ë¦¬ê°€ ì¶”ê°€ë¡œ ë°œìƒí•˜ëŠ” ê²ƒì„ ë°©ì§€í•©ë‹ˆë‹¤.
            panels_data = []
            for p in raw_panels:
                if hasattr(p, "__dict__") and not isinstance(p, dict):
                    # ORM ê°ì²´ì¸ ê²½ìš°: SQLAlchemy state ì†ì„± ì œì™¸í•˜ê³  dictë¡œ ë³€í™˜
                    p_dict = getattr(p, "__dict__", {})
                    d = {k: v for k, v in p_dict.items() if not k.startswith('_')}
                    panels_data.append(d)
                else:
                    # ì´ë¯¸ dictì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                    panels_data.append(p)
            
            if not panels_data:
                return {
                    "summary": {"total_panels": 0, "key_insights": [], "notable_findings": []},
                    "statistics": {},
                    "insights": [],
                    "chart_recommendations": [],
                    "comparison_groups": [],
                }
            
            # 2. Augmentation: ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
            # contextì—ëŠ” 'total_count'ë¥¼ ì „ì²´ ê°œìˆ˜ë¡œ ë„˜ê²¨ì£¼ì–´ LLMì´ ì „ì²´ ê·œëª¨ë¥¼ ì¸ì§€í•˜ê²Œ í•¨
            context = await self._build_context(panels_data, focus_areas, session)
            context["total_count"] = total_panel_count  # ì‹¤ì œ ì „ì²´ ê°œìˆ˜ë¡œ ë®ì–´ì“°ê¸°
            
            # [ìµœì í™” 2] LLM ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ê°•ì œ ì œí•œ (ì‹œì—° ì†ë„ ìµœì í™”)
            # 6,000ì -> 3,000ìë¡œ ì¶•ì†Œí•˜ì—¬ LLM ì²˜ë¦¬ ì‹œê°„ ëŒ€í­ ë‹¨ì¶• (ì‹œì—°ìš©)
            if len(context.get("panels_text_summary", "")) > 3000:
                context["panels_text_summary"] = context["panels_text_summary"][:3000] + "\n...(ìš”ì•½ë¨)"
            
            # 3. Generation: LLM ë¶„ì„
            analysis_result = await self._generate_analysis(panels_data, context, query=query, requested_count=requested_count)
            
            # ìƒ˜í”Œë§ ì •ë³´ ì¶”ê°€
            if total_panel_count > SAMPLE_LIMIT:
                if "summary" in analysis_result:
                    analysis_result["summary"]["note"] = f"ì „ì²´ {total_panel_count}ëª… ì¤‘ {SAMPLE_LIMIT}ëª…ì„ í‘œë³¸ ë¶„ì„í•¨"
            
            return analysis_result
        except Exception as e:
            print(f"âŒ ë¶„ì„ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # ìµœì†Œí•œì˜ í†µê³„ë¼ë„ ë°˜í™˜
            total_count = len(panel_ids) if 'panel_ids' in locals() else 0
            return {
                "error": str(e),
                "summary": {"total_panels": total_count},
                "statistics": {},
                "insights": [],
                "chart_recommendations": [],
                "comparison_groups": [],
            }
        finally:
            if close_session:
                await session.close()
    
    async def _retrieve_panel_data(
        self,
        panel_ids: List[str],
        session: AsyncSession
    ) -> List[Dict[str, Any]]:
        """íŒ¨ë„ ë°ì´í„° ìˆ˜ì§‘"""
        return await self.panel_repo.get_panels_by_ids(panel_ids, session)
    
    async def _build_context(
        self,
        panels_data: List[Dict[str, Any]],
        focus_areas: Optional[List[str]] = None,
        session: Optional[AsyncSession] = None,
    ) -> Dict[str, Any]:
        """RAG ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±"""
        # focus_areasê°€ Noneì´ë©´ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì‚¬ìš©
        if focus_areas is None:
            # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì¡°íšŒ
            category_groups = await self.metadata_loader.load_category_groups(None, session)
            focus_areas = list(category_groups.keys())
        
        # 1. ì •í˜• ë°ì´í„° ë¶„í¬ ìš”ì•½
        panels_data_summary = self._format_panels_data_summary(panels_data)
        
        # 2. ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ (panel_summary_text)
        panels_text_summary = self._format_panels_text_summary(panels_data)
        
        # 3. í†µê³„ ê³„ì‚° (ë¹„ë™ê¸° ë˜í•‘ - CPU ì—°ì‚°ì´ ë¬´ê±°ìš´ ì‘ì—…ì€ ë³„ë„ ìŠ¤ë ˆë“œë¡œ ì‹¤í–‰)
        # [ìµœì í™”] íŒ¨ë„ ìˆ˜ê°€ ë§ìœ¼ë©´ í†µê³„ ê³„ì‚°ë„ ìƒ˜í”Œë§í•˜ì—¬ ì†ë„ í–¥ìƒ
        stats_panels = panels_data
        if len(panels_data) > 200:
            # 200ê°œ ì´ìƒì´ë©´ 200ê°œë§Œ ìƒ˜í”Œë§í•˜ì—¬ í†µê³„ ê³„ì‚° (ì‹œì—° ì†ë„ ìµœì í™”)
            random.seed(42)
            stats_panels = random.sample(panels_data, 200)
        
        loop = asyncio.get_running_loop()
        statistics = await loop.run_in_executor(
            None, 
            lambda: self.stats_calculator.calculate(stats_panels, focus_areas)
        )
        statistics_context = self._format_statistics(statistics)
        
        # 4. ë©”íƒ€ë°ì´í„° ì¡°íšŒ
        metadata = await self.metadata_loader.load_metadata(focus_areas, session)
        metadata_context = self._format_metadata(metadata)
        
        # 5. ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´ ì¶”ê°€
        category_groups = await self.metadata_loader.load_category_groups(focus_areas, session)
        category_groups_context = self._format_category_groups(category_groups)
        
        # 6. ë¹„êµêµ° ê²€ìƒ‰
        comparison_groups = await self.comparison_finder.find_comparison_groups(panels_data, session)
        comparison_context = self._format_comparison(comparison_groups)
        
        return {
            "total_count": len(panels_data),
            "panels_data_summary": panels_data_summary,
            "panels_text_summary": panels_text_summary,
            "statistics_context": statistics_context,
            "metadata_context": metadata_context,
            "category_groups_context": category_groups_context,
            "category_groups": category_groups,  # ì¹´í…Œê³ ë¦¬ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ì— ì „ë‹¬
            "comparison_context": comparison_context,
        }
    
    def _format_panels_data_summary(self, panels_data: List[Dict[str, Any]]) -> str:
        """ì •í˜• ë°ì´í„° ë¶„í¬ ìš”ì•½ (age, gender, income ë“±)"""
        if not panels_data:
            return "íŒ¨ë„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append(f"ì´ íŒ¨ë„ ìˆ˜: {len(panels_data)}ëª…")
        lines.append("")
        
        # ì¸êµ¬í†µê³„
        lines.append("[ì¸êµ¬í†µê³„]")
        gender_dist = self._count_distribution(panels_data, "gender")
        lines.append(f"  ì„±ë³„: {gender_dist}")
        
        # ë‚˜ì´ í†µê³„ (íƒ€ì… ì•ˆì „ì„± í™•ë³´)
        ages: List[int] = []
        for p in panels_data:
            age = p.get("age")
            if age is not None and isinstance(age, (int, float)):
                ages.append(int(age))
        if ages:
            avg_age = sum(ages) / len(ages)
            lines.append(f"  í‰ê·  ë‚˜ì´: {avg_age:.1f}ì„¸")
        
        region_dist = self._get_top_values(panels_data, "region_city", 5)
        lines.append(f"  ì£¼ìš” ì§€ì—­: {region_dist}")
        
        marital_dist = self._count_distribution(panels_data, "marital_status")
        lines.append(f"  ê²°í˜¼ ì—¬ë¶€: {marital_dist}")
        lines.append("")
        
        # ê²½ì œë ¥
        lines.append("[ê²½ì œë ¥]")
        incomes: List[int] = []
        for p in panels_data:
            income = p.get("monthly_household_income")
            if income is not None and isinstance(income, (int, float)):
                incomes.append(int(income))
        if incomes:
            avg_income = sum(incomes) / len(incomes)
            median_income = sorted(incomes)[len(incomes) // 2] if incomes else 0
            lines.append(f"  í‰ê·  ê°€êµ¬ì†Œë“: {avg_income:.0f}ë§Œì›")
            lines.append(f"  ì¤‘ì•™ê°’ ê°€êµ¬ì†Œë“: {median_income:.0f}ë§Œì›")
        
        car_owners = sum(1 for p in panels_data if p.get("car_ownership") is True)
        lines.append(f"  ì°¨ëŸ‰ ì†Œìœ : {car_owners}ëª… ({car_owners/len(panels_data)*100:.1f}%)")
        lines.append("")
        
        # ë””ì§€í„¸/ë¼ì´í”„ìŠ¤íƒ€ì¼
        lines.append("[ë””ì§€í„¸/ë¼ì´í”„ìŠ¤íƒ€ì¼]")
        phone_brand_dist = self._get_top_values(panels_data, "phone_brand", 3)
        lines.append(f"  íœ´ëŒ€í° ë¸Œëœë“œ: {phone_brand_dist}")
        
        # ë°°ì—´ í•„ë“œ ì²˜ë¦¬ (owned_electronics)
        electronics = {}
        for p in panels_data:
            if p.get("owned_electronics"):
                items = p["owned_electronics"] if isinstance(p["owned_electronics"], list) else []
                for item in items:
                    electronics[item] = electronics.get(item, 0) + 1
        if electronics:
            top_electronics = sorted(electronics.items(), key=lambda x: -x[1])[:5]
            lines.append(f"  ë³´ìœ  ì „ìì œí’ˆ: {', '.join(f'{k}({v})' for k, v in top_electronics)}")
        
        return "\n".join(lines)
    
    def _format_panels_text_summary(self, panels_data: List[Dict[str, Any]]) -> str:
        """ë¹„ì •í˜• ë°ì´í„° (panel_summary_text) ìš”ì•½
        
        íŒ¨ë„ ìˆ˜ì— ë”°ë¼ ë™ì ìœ¼ë¡œ ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ì¡°ì •í•˜ì—¬ ë” ì •í™•í•œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
        """
        summaries = [p.get("panel_summary_text") for p in panels_data if p.get("panel_summary_text")]
        if not summaries:
            return "ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # None ê°’ í•„í„°ë§
        valid_summaries = [s for s in summaries if s]
        if not valid_summaries:
            return "ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ì´ ì—†ìŠµë‹ˆë‹¤."
        
        total_count = len(valid_summaries)
        
        # [ìµœì í™”] ìƒ˜í”Œ ê°œìˆ˜ ëŒ€í­ ì¶•ì†Œ (ì‹œì—° ì†ë„ ìµœì í™”)
        # 20ê°œ -> 10ê°œë¡œ ì¤„ì—¬ì„œ í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë‹¨ì¶• ë° LLM ì‘ë‹µ ì†ë„ í–¥ìƒ
        if total_count <= 5:
            # 5ê°œ ì´í•˜: ëª¨ë‘ í¬í•¨
            sample_count = total_count
        else:
            # ìµœëŒ€ 10ê°œê¹Œì§€ë§Œ ë³´ì—¬ì¤˜ë„ ì¶©ë¶„í•©ë‹ˆë‹¤. (ì‹œì—° ì†ë„ ìµœì í™”)
            sample_count = min(10, int(total_count * 0.05) + 3)
        
        # ëœë¤ ìƒ˜í”Œë§ìœ¼ë¡œ ëŒ€í‘œì„± í–¥ìƒ (ë‹¨ìˆœíˆ ì²˜ìŒ Nê°œê°€ ì•„ë‹Œ)
        # ì‹œë“œ ê³ ì •ìœ¼ë¡œ ì¬í˜„ ê°€ëŠ¥ì„± ë³´ì¥ (íŒ¨ë„ ID ê¸°ë°˜ ì‹œë“œë¡œ ë” ì¼ê´€ëœ ìƒ˜í”Œë§)
        # íŒ¨ë„ IDì˜ í•´ì‹œê°’ì„ ì‹œë“œë¡œ ì‚¬ìš©í•˜ì—¬ ê°™ì€ íŒ¨ë„ ê·¸ë£¹ì—ì„œëŠ” í•­ìƒ ê°™ì€ ìƒ˜í”Œ ì„ íƒ
        if panels_data and len(panels_data) > 0:
            # ì²« ë²ˆì§¸ íŒ¨ë„ IDë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹œë“œ ìƒì„± (ê°™ì€ íŒ¨ë„ ê·¸ë£¹ = ê°™ì€ ìƒ˜í”Œ)
            first_panel_id = str(panels_data[0].get("panel_id", ""))
            seed_value = hash(first_panel_id) % 10000  # 0-9999 ë²”ìœ„ë¡œ ì •ê·œí™”
            random.seed(seed_value)
        else:
            random.seed(42)  # ê¸°ë³¸ ì‹œë“œ
        
        sampled_summaries = random.sample(valid_summaries, min(sample_count, len(valid_summaries)))
        
        lines = [f"ì´ {total_count}ê°œ íŒ¨ë„ì— ìš”ì•½ í…ìŠ¤íŠ¸ê°€ ìˆìŠµë‹ˆë‹¤."]
        lines.append(f"ëŒ€í‘œ ìƒ˜í”Œ {len(sampled_summaries)}ê°œ (ëœë¤ ìƒ˜í”Œë§):")
        for i, summary in enumerate(sampled_summaries, 1):
            if summary:  # None ì²´í¬
                # ê° ìƒ˜í”Œì€ 100ìë¡œ ì œí•œ (ì‹œì—° ì†ë„ ìµœì í™” - í”„ë¡¬í”„íŠ¸ ê¸¸ì´ ë‹¨ì¶•)
                # ë„ˆë¬´ ì§§ìœ¼ë©´ ì •ë³´ ë¶€ì¡±, ë„ˆë¬´ ê¸¸ë©´ ì»¨í…ìŠ¤íŠ¸ ê³¼ë‹¤
                truncated = summary[:100] if len(summary) > 100 else summary
                lines.append(f"  {i}. {truncated}{'...' if len(summary) > 100 else ''}")
        
        return "\n".join(lines)
    
    def _format_statistics(self, statistics: Dict[str, Dict[str, Any]]) -> str:
        """í†µê³„ ë°ì´í„° í¬ë§·íŒ… (í† í° ì ˆì•½ì„ ìœ„í•´ ìš”ì•½)"""
        if not statistics:
            return "í†µê³„ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ í¬í•¨ (í† í° ì ˆì•½)
        priority_categories = ["demographics", "economic", "digital", "lifestyle"]
        
        for category in priority_categories:
            if category in statistics:
                stats = statistics[category]
                lines.append(f"[{category}]")
                # ì£¼ìš” í†µê³„ë§Œ í¬í•¨ (ìµœëŒ€ 5ê°œ)
                for idx, (key, value) in enumerate(stats.items()):
                    if idx >= 5:  # ì¹´í…Œê³ ë¦¬ë‹¹ ìµœëŒ€ 5ê°œ í†µê³„ë§Œ
                        break
                    if isinstance(value, dict):
                        # dictëŠ” ê°„ë‹¨íˆ ìš”ì•½
                        value_str = json.dumps(value, ensure_ascii=False)
                        if len(value_str) > 200:  # ë„ˆë¬´ ê¸¸ë©´ ìš”ì•½
                            value_str = value_str[:200] + "..."
                        lines.append(f"  {key}: {value_str}")
                    else:
                        lines.append(f"  {key}: {value}")
                lines.append("")
        
        # ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ëŠ” ìš”ì•½ë§Œ
        other_categories = [cat for cat in statistics.keys() if cat not in priority_categories]
        if other_categories:
            lines.append(f"[ê¸°íƒ€ ì¹´í…Œê³ ë¦¬: {', '.join(other_categories)}]")
            lines.append("  (ìƒì„¸ í†µê³„ëŠ” ìƒëµ)")
            lines.append("")
        
        return "\n".join(lines)
    
    def _format_metadata(self, metadata: Dict[str, Any]) -> str:
        """ë©”íƒ€ë°ì´í„° í¬ë§·íŒ…"""
        if not metadata:
            return "ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        column_metadata = metadata.get("column_metadata", {})
        
        # ì£¼ìš” ì»¬ëŸ¼ë§Œ í¬í•¨ (í† í° ì ˆì•½)
        high_priority_cols = [
            col for col, meta in column_metadata.items()
            if meta.get("analysis_priority") == "high"
        ][:10]
        
        for col_name in high_priority_cols:
            meta = column_metadata.get(col_name, {})
            lines.append(f"  {col_name} ({meta.get('name_ko', '')}): {meta.get('type', '')} - {meta.get('description', '')}")
        
        return "\n".join(lines) if lines else "ë©”íƒ€ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def _format_category_groups(self, category_groups: Dict[str, Dict[str, Any]]) -> str:
        """ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´ í¬ë§·íŒ… (í† í° ì ˆì•½ì„ ìœ„í•´ ê°„ì†Œí™”)"""
        if not category_groups:
            return "ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        lines.append("ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ ê°€ì´ë“œ:")
        lines.append("**ë°˜ë“œì‹œ ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”:**")
        lines.append("")
        
        # ì£¼ìš” ì¹´í…Œê³ ë¦¬ì™€ ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ êµ¬ë¶„
        main_categories = ["demographics", "economic", "digital", "lifestyle", "health_wellness", "tech_digital_life", "consumption_finance"]
        other_categories = [k for k in category_groups.keys() if k not in main_categories and k != "summary"]
        
        lines.append("**ì£¼ìš” ì¹´í…Œê³ ë¦¬ (ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœì†Œ 2-3ê°œ ì¸ì‚¬ì´íŠ¸ í•„ìˆ˜):**")
        for group_key in main_categories:
            if group_key in category_groups:
                group_data = category_groups[group_key]
                name_ko = group_data.get("name_ko", group_key)
                # ê°„ì†Œí™”: ì´ë¦„ê³¼ ë¶„ì„ í¬ì»¤ìŠ¤ë§Œ í¬í•¨
                analysis_focus = group_data.get("analysis_focus", [])
                focus_str = ', '.join(analysis_focus[:3]) if analysis_focus else "ì¼ë°˜ ë¶„ì„"
                lines.append(f"  â€¢ {name_ko} ({group_key}): {focus_str}")
        
        lines.append("")
        lines.append("**ê¸°íƒ€ ì¹´í…Œê³ ë¦¬ (ê° ì¹´í…Œê³ ë¦¬ë‹¹ ìµœì†Œ 1-2ê°œ ì¸ì‚¬ì´íŠ¸ ê¶Œì¥):**")
        for group_key in other_categories:
            if group_key in category_groups:
                group_data = category_groups[group_key]
                name_ko = group_data.get("name_ko", group_key)
                lines.append(f"  â€¢ {name_ko} ({group_key})")
        
        return "\n".join(lines) if lines else "ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
    
    def _format_comparison(self, comparison_groups: List[Dict[str, Any]]) -> str:
        """ë¹„êµêµ° ì •ë³´ í¬ë§·íŒ…"""
        if not comparison_groups:
            return "ë¹„êµêµ°ì´ ì—†ìŠµë‹ˆë‹¤."
        
        lines = []
        for group in comparison_groups[:3]:  # ìƒìœ„ 3ê°œë§Œ
            lines.append(f"  {group.get('type', 'unknown')}: {len(group.get('panel_ids', []))}ê°œ íŒ¨ë„ - {group.get('reason', '')}")
        
        return "\n".join(lines) if lines else "ë¹„êµêµ°ì´ ì—†ìŠµë‹ˆë‹¤."
    
    def _count_distribution(self, panels_data: List[Dict], field: str) -> str:
        """í•„ë“œë³„ ë¶„í¬ ì¹´ìš´íŠ¸"""
        counts = {}
        for p in panels_data:
            val = p.get(field)
            if val:
                counts[val] = counts.get(val, 0) + 1
        return ", ".join(f"{k}({v}ëª…)" for k, v in sorted(counts.items(), key=lambda x: -x[1]))
    
    def _get_top_values(self, panels_data: List[Dict], field: str, top_n: int) -> str:
        """ìƒìœ„ Nê°œ ê°’"""
        counts = {}
        for p in panels_data:
            val = p.get(field)
            if val:
                counts[val] = counts.get(val, 0) + 1
        top = sorted(counts.items(), key=lambda x: -x[1])[:top_n]
        return ", ".join(f"{k}({v})" for k, v in top)
    
    async def _generate_analysis(
        self,
        panels_data: List[Dict[str, Any]],
        context: Dict[str, Any],
        query: Optional[str] = None,
        requested_count: Optional[int] = None
    ) -> Dict[str, Any]:
        """LLMì„ ì´ìš©í•œ ë¶„ì„ ìƒì„±"""
        if not self.llm:
            # LLMì´ ì—†ìœ¼ë©´ ê¸°ë³¸ í†µê³„ë§Œ ë°˜í™˜ (ë¹„ë™ê¸° ì²˜ë¦¬)
            loop = asyncio.get_running_loop()
            statistics = await loop.run_in_executor(
                None,
                lambda: self.stats_calculator.calculate(panels_data)
            )
            return {
                "summary": {
                    "total_panels": context.get("total_count", 0),
                    "key_insights": [],
                    "notable_findings": [],
                },
                "statistics": statistics,
                "insights": [],
                "chart_recommendations": [],
                "comparison_groups": [],
            }
        
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„± (ê°œì„ : ìˆ¨ê²¨ì§„ ì¸ì‚¬ì´íŠ¸ ë°œê²¬ ê°•ì¡°)
            system_prompt = """ë‹¹ì‹ ì€ íŒ¨ë„ ë°ì´í„° ë¶„ì„ ì „ë¬¸ê°€ì´ì ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ë‹¨ìˆœí•œ í†µê³„ ìš”ì•½ì´ ì•„ë‹Œ, ë°ì´í„° ì†ì— ìˆ¨ê²¨ì§„ íŒ¨í„´ê³¼ ì˜ë¯¸ë¥¼ ë°œê²¬í•˜ì—¬ ì‹¤ìš©ì ì¸ ë¹„ì¦ˆë‹ˆìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”.

**ì¤‘ìš”: ì¶œë ¥ í˜•ì‹**
- ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”.
- ì„¤ëª…ì´ë‚˜ ì„œë¬¸ ì—†ì´ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.
- JSONì´ ì™„ì „í•´ì•¼ í•©ë‹ˆë‹¤ (ëª¨ë“  ì¤‘ê´„í˜¸ì™€ ë°°ì—´ì´ ë‹«í˜€ìˆì–´ì•¼ í•¨).

**ì¼ê´€ì„± ì›ì¹™ (ë§¤ìš° ì¤‘ìš”):**
- ëª¨ë“  ì¸ì‚¬ì´íŠ¸ì—ì„œ ë™ì¼í•œ í’ˆì§ˆê³¼ ê¸¸ì´ ê¸°ì¤€ì„ ìœ ì§€í•˜ì„¸ìš”.
- ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜ì™€ ì¶”ì²œ ì‚¬í•­ì€ ëª¨ë“  ì¸ì‚¬ì´íŠ¸ì—ì„œ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ê¸¸ì´ì™€ ìƒì„¸ë„ê°€ ì¼ê´€ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
- ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ì—ì„œë„ ê°€ëŠ¥í•œ í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ë˜, í’ˆì§ˆì„ ìœ ì§€í•˜ì„¸ìš”.

**ë¶„ì„ ì›ì¹™:**
1. **ìˆ¨ê²¨ì§„ íŒ¨í„´ ë°œê²¬**: í‘œë©´ì ì¸ í†µê³„ê°€ ì•„ë‹Œ, ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ì™€ ì˜ˆìƒì¹˜ ëª»í•œ ì—°ê´€ì„±ì„ ì°¾ì•„ë‚´ì„¸ìš”.
   - ì˜ˆ: "ê³ ì†Œë“ì¸µì´ íŠ¹ì • ë¸Œëœë“œë¥¼ ì„ í˜¸í•œë‹¤"ëŠ” ë‹¨ìˆœ í†µê³„ê°€ ì•„ë‹ˆë¼, "ê³ ì†Œë“ì¸µ ì¤‘ì—ì„œë„ 30ëŒ€ ê¸°í˜¼ ë‚¨ì„±ì´ íŠ¹ì • ë¸Œëœë“œ ì„ í˜¸ë„ê°€ ë†’ë‹¤"ëŠ” êµ¬ì²´ì  íŒ¨í„´
   - ì˜ˆ: "OTT ì´ìš©ë¥ ì´ ë†’ë‹¤"ëŠ” ë‹¨ìˆœ í†µê³„ê°€ ì•„ë‹ˆë¼, "OTT ì´ìš©ë¥ ì´ ë†’ì€ ê·¸ë£¹ì€ íŠ¹ì • ë¼ì´í”„ìŠ¤íƒ€ì¼ íŠ¹ì„±ì„ ê³µìœ í•œë‹¤"ëŠ” ì—°ê´€ì„±

2. **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ vs ìƒì„¸ ì¸ì‚¬ì´íŠ¸ êµ¬ë¶„**:
   - **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (key_insights)**: ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ í° ê·¸ë¦¼, ì „ëµì  ê´€ì ì˜ ë°œê²¬ (3-5ê°œ)
   - **ìƒì„¸ ì¸ì‚¬ì´íŠ¸ (insights)**: êµ¬ì²´ì ì¸ ë°œê²¬ì‚¬í•­, ì‹¤í–‰ ê°€ëŠ¥í•œ ì•¡ì…˜ ì•„ì´í…œ (10-15ê°œ)
   - í•µì‹¬ ì¸ì‚¬ì´íŠ¸ì™€ ìƒì„¸ ì¸ì‚¬ì´íŠ¸ëŠ” ì¤‘ë³µë˜ì§€ ì•Šì•„ì•¼ í•©ë‹ˆë‹¤.

3. **ìƒì„¸í•œ ì½”ë©˜íŠ¸ ì œê³µ**: ê° ì¸ì‚¬ì´íŠ¸ë§ˆë‹¤ ë‹¤ìŒì„ í¬í•¨í•˜ì„¸ìš”:
   - ë°œê²¬ ì‚¬í•­ (finding): êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì‚¬ì‹¤
   - ì¤‘ìš”ë„ (significance): high/medium/low (ë¹„ì¦ˆë‹ˆìŠ¤ ì˜í–¥ë„ ê¸°ì¤€)
   - ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜ (business_implication): ì´ ë°œê²¬ì´ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì˜ë¯¸ (2-3ë¬¸ì¥)
   - ì¶”ì²œ ì‚¬í•­ (recommendation): êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì´ë‚˜ ì¶”ê°€ ë¶„ì„ ì œì•ˆ (2-3ë¬¸ì¥)

4. **ë¹„êµ ë¶„ì„**: ì „ì²´ íŒ¨ë„ ëŒ€ë¹„ ì´ ê·¸ë£¹ì˜ íŠ¹ì´ì ì„ ê°•ì¡°í•˜ì„¸ìš”.

ë©”íƒ€ë°ì´í„° ì •ë³´:
{metadata_context}

í†µê³„ ì •ë³´:
{statistics_context}

ë¹„êµêµ° ì •ë³´:
{comparison_context}"""

            # ëª…ìˆ˜ ì •ë³´ ì¶”ê°€
            count_info = ""
            if requested_count is not None:
                count_info = f"\n**ì›ë³¸ ì§ˆì˜ì—ì„œ ìš”ì²­í•œ ëª…ìˆ˜: {requested_count}ëª…**\n"
            
            query_info = ""
            if query:
                query_info = f"\n**ì›ë³¸ ì§ˆì˜: {query}**\n"
            
            user_prompt_template = """ë‹¤ìŒ íŒ¨ë„ ê·¸ë£¹ ë°ì´í„°ë¥¼ ì‹¬ì¸µ ë¶„ì„í•´ì£¼ì„¸ìš”:""" + query_info + count_info + """
ì´ íŒ¨ë„ ìˆ˜: {total_count}ëª…

[ì •í˜• ë°ì´í„° ë¶„í¬]
{panels_data_summary}

[ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ (LLM ìƒì„± ìš”ì•½ í…ìŠ¤íŠ¸)]
{panels_text_summary}

[ê³„ì‚°ëœ í†µê³„]
{statistics_context}

[ë©”íƒ€ë°ì´í„° ì •ë³´]
{metadata_context}

[ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´]
{category_groups_context}

[ë¹„êµêµ° ì •ë³´]
{comparison_context}

**ë¶„ì„ ìš”ì²­ (ì¤‘ìš”):**

1. **ìˆ¨ê²¨ì§„ íŒ¨í„´ ë°œê²¬**:
   - ì •í˜• ë°ì´í„°ì™€ ë¹„ì •í˜• ë°ì´í„°ë¥¼ êµì°¨ ë¶„ì„í•˜ì—¬ ì˜ˆìƒì¹˜ ëª»í•œ ì—°ê´€ì„± ì°¾ê¸°
   - ë³€ìˆ˜ ê°„ ìƒê´€ê´€ê³„ ë¶„ì„ (ì˜ˆ: ì†Œë“ê³¼ ë¼ì´í”„ìŠ¤íƒ€ì¼, ì—°ë ¹ê³¼ ë””ì§€í„¸ ìˆ˜ìš©ë„)
   - íŠ¹ì´í•œ ì¡°í•©ì´ë‚˜ ì˜ˆì™¸ì  íŒ¨í„´ ë°œê²¬

2. **í•µì‹¬ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ** (5-7ê°œ):
   - ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ì „ëµì  ê´€ì ì˜ ë°œê²¬
   - ì´ íŒ¨ë„ ê·¸ë£¹ì˜ í•µì‹¬ íŠ¹ì„±ê³¼ ì°¨ë³„ì 
   - ë¹„ì¦ˆë‹ˆìŠ¤ ì „ëµ ìˆ˜ë¦½ì— ì¤‘ìš”í•œ í° ê·¸ë¦¼
   - **ì¤‘ìš”**: ê° í•µì‹¬ ì¸ì‚¬ì´íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì ì´ë‚˜ ì¸¡ë©´ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: ì¸êµ¬í†µê³„, ì†Œë¹„íŒ¨í„´, ë¼ì´í”„ìŠ¤íƒ€ì¼, ë””ì§€í„¸ ìˆ˜ìš©ë„ ë“±)
   - **ê¸¸ì´**: ê° ì¸ì‚¬ì´íŠ¸ëŠ” ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì„ ë‹´ì•„ì•¼ í•©ë‹ˆë‹¤ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šê²Œ ì ì ˆí•œ ê¸¸ì´ë¡œ ì‘ì„±)

3. **ì¹´í…Œê³ ë¦¬ë³„ ìƒì„¸ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ** (**ë°˜ë“œì‹œ ìµœì†Œ 15ê°œ ì´ìƒ, ëª©í‘œ 20-30ê°œ**):
   - **ì¤‘ìš”**: insights ë°°ì—´ì—ëŠ” ë°˜ë“œì‹œ ìµœì†Œ 15ê°œ ì´ìƒì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤. 15ê°œ ë¯¸ë§Œì´ë©´ ë¶„ì„ì´ ë¶ˆì™„ì „í•œ ê²ƒìœ¼ë¡œ ê°„ì£¼ë©ë‹ˆë‹¤.
   - **ë°˜ë“œì‹œ ê° ì¹´í…Œê³ ë¦¬ë³„ë¡œ ë¶„ì„**: ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ëª¨ë“  ì¹´í…Œê³ ë¦¬ì—ì„œ ì¸ì‚¬ì´íŠ¸ ë„ì¶œ
   - **ì‹¤ë¬´ í™œìš© ì¤‘ì‹¬**: ë¦¬ì„œì¹˜ ê¸°ì—…ì—ì„œ ì‹¤ë¬´ì— ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ë°œê²¬ì‚¬í•­ ì œì‹œ
   - **ì¤‘ë³µ ë°©ì§€**: 
     * ê°™ì€ ì£¼ì œë‚˜ ì¸¡ë©´ì„ ë‹¤ë£¨ëŠ” ì¸ì‚¬ì´íŠ¸ëŠ” í•œ ë²ˆë§Œ ì‘ì„±
     * ê° ì¸ì‚¬ì´íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ ê´€ì , ë³€ìˆ˜, ë˜ëŠ” ì¸¡ë©´ì„ ë‹¤ë£¨ì–´ì•¼ í•¨
     * ì˜ˆ: "ì°¨ëŸ‰ ë³´ìœ ìœ¨"ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ê°€ ìˆìœ¼ë©´, ê°™ì€ ë‚´ìš©ì„ ë‹¤ë¥¸ ìˆ˜ì¹˜ë¡œ ë°˜ë³µí•˜ì§€ ì•ŠìŒ
   - **ì¹´í…Œê³ ë¦¬ë³„ ì‹¤ë¬´ ì¤‘ì‹¬ ë¶„ë°°** (ìµœì†Œ ê°œìˆ˜ ë³´ì¥):
     * demographics (ì¸êµ¬í†µê³„): ìµœì†Œ 2ê°œ ì´ìƒ (ì—°ë ¹/ì„±ë³„/ì§€ì—­ë³„ ì„¸ë¶„í™”, ê²°í˜¼/ê°€ì¡± êµ¬ì¡° íŠ¹ì„±)
     * economic (ê²½ì œë ¥): ìµœì†Œ 2ê°œ ì´ìƒ (ì†Œë“ ë¶„í¬, ì†Œë¹„ ì—¬ë ¥, ê²½ì œì  íŠ¹ì„±)
     * digital (ë””ì§€í„¸): ìµœì†Œ 2ê°œ ì´ìƒ (ë””ì§€í„¸ ê¸°ê¸° ë³´ìœ , ë””ì§€í„¸ ì„œë¹„ìŠ¤ ì´ìš©, ë””ì§€í„¸ ìˆ˜ìš©ë„)
     * lifestyle (ë¼ì´í”„ìŠ¤íƒ€ì¼): ìµœì†Œ 2ê°œ ì´ìƒ (ìƒí™œ íŒ¨í„´, ì—¬ê°€ í™œë™, ì†Œë¹„ ìŠµê´€)
     * health_wellness (ê±´ê°•/ì‹ ì²´ê´€ë¦¬): ìµœì†Œ 1ê°œ ì´ìƒ (ê±´ê°• ê´€ë¦¬ ê´€ì‹¬ë„, ìš´ë™/ì‹ìŠµê´€, ê±´ê°• ê´€ë ¨ ì†Œë¹„)
     * tech_digital_life (ê¸°ìˆ  ë° ë””ì§€í„¸ ë¼ì´í”„): ìµœì†Œ 1ê°œ ì´ìƒ (ê¸°ìˆ  ì œí’ˆ ì„ í˜¸, ë””ì§€í„¸ ë¼ì´í”„ìŠ¤íƒ€ì¼)
     * consumption_finance (ì†Œë¹„ ë° ì¬í…Œí¬): ìµœì†Œ 1ê°œ ì´ìƒ (ì†Œë¹„ íŒ¨í„´, ê¸ˆìœµ ìƒí’ˆ ì´ìš©, ì¬í…Œí¬ ê´€ì‹¬)
     * travel_culture (ì—¬í–‰ ë° ë¬¸í™”ìƒí™œ): ìµœì†Œ 1ê°œ ì´ìƒ (ì—¬í–‰ ë¹ˆë„/ì„ í˜¸ì§€, ë¬¸í™” í™œë™)
     * psychology_stress (ì‹¬ë¦¬ ë° ìŠ¤íŠ¸ë ˆìŠ¤ ê´€ë¦¬): ìµœì†Œ 1ê°œ ì´ìƒ (ìŠ¤íŠ¸ë ˆìŠ¤ ìš”ì¸, ì‹¬ë¦¬ì  íŠ¹ì„±)
     * daily_habits (ì¼ìƒìƒí™œ íƒœë„ ë° ìŠµê´€): ìµœì†Œ 1ê°œ ì´ìƒ (ì¼ìƒ ìŠµê´€, ìƒí™œ íƒœë„)
     * values_experience (ê²½í—˜ ë° ê°€ì¹˜ê´€): ìµœì†Œ 1ê°œ ì´ìƒ (ê°€ì¹˜ê´€, ê²½í—˜ ì„ í˜¸ë„)
   - ê° ì¸ì‚¬ì´íŠ¸ì˜ findingì€ êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì‚¬ì‹¤ì„ ì œì‹œí•˜ì„¸ìš” (ì ì ˆí•œ ê¸¸ì´ë¡œ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ)
   - **ì†Œì œëª© ë‹¤ì–‘í™”**: ê°™ì€ ì¹´í…Œê³ ë¦¬ ë‚´ì—ì„œë„ ì„œë¡œ ë‹¤ë¥¸ ì¸¡ë©´ì„ ë‹¤ë£¨ë„ë¡ ì†Œì œëª©/ê´€ì ì„ ë‹¤ì–‘í•˜ê²Œ ì„¤ì •

4. **ìƒì„¸í•œ ì½”ë©˜íŠ¸ ì œê³µ**:
   - ê° ì¸ì‚¬ì´íŠ¸ë§ˆë‹¤ ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜(business_implication)ë¥¼ ìƒì„¸íˆ ì‘ì„±í•˜ì„¸ìš” (2-3ë¬¸ì¥, ì¶©ë¶„íˆ ì„¤ëª…)
   - ê° ì¸ì‚¬ì´íŠ¸ë§ˆë‹¤ ì¶”ì²œ ì‚¬í•­(recommendation)ì„ êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆìœ¼ë¡œ ì œì‹œí•˜ì„¸ìš” (2-3ë¬¸ì¥, ì‹¤ë¬´ í™œìš© ê°€ëŠ¥í•˜ê²Œ)
   - ë‹¨ìˆœ ë‚˜ì—´ì´ ì•„ë‹Œ, ì˜ë¯¸ í•´ì„ê³¼ ì‹¤í–‰ ë°©ì•ˆ í¬í•¨
   - **ì¤‘ìš”**: ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜ì™€ ì¶”ì²œ ì‚¬í•­ì€ ë°˜ë“œì‹œ í¬í•¨ë˜ì–´ì•¼ í•˜ë©°, ëª¨ë“  ì¸ì‚¬ì´íŠ¸ì—ì„œ ì¼ê´€ëœ í’ˆì§ˆì„ ìœ ì§€í•˜ì„¸ìš”
   - **í’ˆì§ˆ ê¸°ì¤€**: ê° ì½”ë©˜íŠ¸ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ë‚˜ ì‚¬ë¡€ë¥¼ í¬í•¨í•˜ê³ , ì‹¤ë¬´ì— ë°”ë¡œ í™œìš© ê°€ëŠ¥í•œ ìˆ˜ì¤€ì´ì–´ì•¼ í•©ë‹ˆë‹¤

5. **ì°¨íŠ¸ ì¶”ì²œ** (ìµœì†Œ 2ê°œ í•„ìˆ˜): ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ì‹œê°í™”í•  ìˆ˜ ìˆëŠ” ì°¨íŠ¸ íƒ€ì…ê³¼ ì´ìœ 
   - **ë°˜ë“œì‹œ ìµœì†Œ 2ê°œì˜ ì°¨íŠ¸ë¥¼ ì¶”ì²œ**í•˜ì„¸ìš”
   - ê° ì°¨íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•„ë“œë‚˜ ê´€ì ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
   - ì°¨íŠ¸ íƒ€ì…ì€ ë°ì´í„° íŠ¹ì„±ì— ë§ê²Œ ì„ íƒ (pie, bar, histogram ë“±)
   - ê° ì°¨íŠ¸ë§ˆë‹¤ ì œëª©, ì„¤ëª…, ë°ì´í„° í•„ë“œ, ì§‘ê³„ ë°©ì‹ì„ ëª…ì‹œí•˜ì„¸ìš”

6. **ë¹„êµêµ° ì¶”ì²œ** (2~3ê°œ í•„ìˆ˜): ì¶”ê°€ ë¶„ì„ì„ ìœ„í•œ ìœ ì‚¬/ëŒ€ì¡° íŒ¨ë„ ê·¸ë£¹ ì¶”ì²œ
   - **ë°˜ë“œì‹œ 2~3ê°œì˜ ë¹„êµêµ°ì„ ì¶”ì²œ**í•˜ì„¸ìš”
   - ìœ ì‚¬ ê·¸ë£¹(similar): ë¹„ìŠ·í•œ íŠ¹ì„±ì„ ê°€ì§„ ê·¸ë£¹ìœ¼ë¡œ ì¶”ê°€ ë¶„ì„ ê°€ëŠ¥
   - ëŒ€ì¡° ê·¸ë£¹(contrast): ë°˜ëŒ€ íŠ¹ì„±ì„ ê°€ì§„ ê·¸ë£¹ìœ¼ë¡œ ì°¨ì´ì  ë¶„ì„ ê°€ëŠ¥
   - ë³´ì™„ ê·¸ë£¹(complement): ë‹¤ë¥¸ ê´€ì ì—ì„œ ë³´ì™„ ë¶„ì„ ê°€ëŠ¥í•œ ê·¸ë£¹
   - ê° ë¹„êµêµ°ë§ˆë‹¤ ì¶”ì²œ ì´ìœ ì™€ **ìì—°ì–´ í˜•íƒœì˜ ê²€ìƒ‰ì–´**ë¥¼ ëª…ì‹œí•˜ì„¸ìš”
   - **ì¤‘ìš”**: `query_suggestion`ì€ ìì—°ì–´ ë¬¸ì¥ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš” (ì˜ˆ: "30ëŒ€ ë¯¸í˜¼ ì—¬ì„± ì¤‘ ê³ ì†Œë“ì¸µ", "ì„œìš¸ ê±°ì£¼ ë¹„í¡ì—°ì", "ê¸°í˜¼ ë‚¨ì„± ì¤‘ ì°¨ëŸ‰ ë³´ìœ ì" ë“±)
   - **ëª…ìˆ˜ í¬í•¨ ê·œì¹™ (ë§¤ìš° ì¤‘ìš”)**: 
     * ìœ„ í”„ë¡¬í”„íŠ¸ ì‹œì‘ ë¶€ë¶„ì— "ì›ë³¸ ì§ˆì˜ì—ì„œ ìš”ì²­í•œ ëª…ìˆ˜: Nëª…" ì •ë³´ê°€ ìˆìœ¼ë©´, ë¹„êµêµ° ì¶”ì²œ ê²€ìƒ‰ì–´ì—ë„ ë°˜ë“œì‹œ ë™ì¼í•œ ëª…ìˆ˜ë¥¼ í¬í•¨í•˜ì„¸ìš” (ì˜ˆ: "30ëŒ€ ë¯¸í˜¼ ì—¬ì„± ì¤‘ ê³ ì†Œë“ì¸µ Nëª…")
     * "ì›ë³¸ ì§ˆì˜ì—ì„œ ìš”ì²­í•œ ëª…ìˆ˜" ì •ë³´ê°€ ì—†ìœ¼ë©´, ë¹„êµêµ° ì¶”ì²œ ê²€ìƒ‰ì–´ì—ë„ ëª…ìˆ˜ë¥¼ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš” (ì˜ˆ: "30ëŒ€ ë¯¸í˜¼ ì—¬ì„± ì¤‘ ê³ ì†Œë“ì¸µ" - ëª…ìˆ˜ ì—†ìŒ)

**ì¶œë ¥ í˜•ì‹ (JSON - ë°˜ë“œì‹œ ì´ í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µ):**
**ì¤‘ìš”: ì„¤ëª…ì´ë‚˜ ì„œë¬¸ ì—†ì´ ì•„ë˜ JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•˜ì„¸ìš”. JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.**

{{
    "key_insights": [
        "ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ì „ëµì  ê´€ì ì˜ í•µì‹¬ ë°œê²¬ 1 (ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì„ ë‹´ì€ ë¬¸ì¥)",
        "ì „ì²´ ë°ì´í„°ë¥¼ ì¢…í•©í•œ ì „ëµì  ê´€ì ì˜ í•µì‹¬ ë°œê²¬ 2 (ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì„ ë‹´ì€ ë¬¸ì¥)",
        "... (ì´ 5-7ê°œ)"
    ],
    "insights": [
        {{
            "category": "{category_list}",
            "finding": "êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ë°œê²¬ ì‚¬í•­ (ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ, ì˜ˆ: '30ëŒ€ ê¸°í˜¼ ë‚¨ì„±ì˜ 65%ê°€ í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œë¥¼ ì„ í˜¸í•˜ë©°, ì´ëŠ” ì „ì²´ í‰ê· (35%)ë³´ë‹¤ 30%p ë†’ìŒ. íŠ¹íˆ ì›” ê°€êµ¬ì†Œë“ 700ë§Œì› ì´ìƒ ê·¸ë£¹ì—ì„œ ì´ ì„ í˜¸ë„ê°€ 80%ë¡œ ë”ìš± ë†’ê²Œ ë‚˜íƒ€ë‚¨')",
            "significance": "high|medium|low",
            "business_implication": "ì´ ë°œê²¬ì´ ë¹„ì¦ˆë‹ˆìŠ¤ì— ë¯¸ì¹˜ëŠ” ì˜í–¥ê³¼ ì˜ë¯¸ë¥¼ ìƒì„¸íˆ ì„¤ëª… (2-3ë¬¸ì¥). ì˜ˆ: 'ì´ ê·¸ë£¹ì€ í”„ë¦¬ë¯¸ì—„ ì œí’ˆ ë§ˆì¼€íŒ…ì˜ í•µì‹¬ íƒ€ê²Ÿìœ¼ë¡œ, ë†’ì€ êµ¬ë§¤ë ¥ê³¼ ë¸Œëœë“œ ì¶©ì„±ë„ë¥¼ ë³´ì—¬ì¤€ë‹¤. ë”°ë¼ì„œ ì´ ê·¸ë£¹ì„ ëŒ€ìƒìœ¼ë¡œ í•œ ë§ì¶¤í˜• ë§ˆì¼€íŒ… ì „ëµì´ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒëœë‹¤. íŠ¹íˆ ì´ ê·¸ë£¹ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì œí’ˆ í¬ì§€ì…”ë‹ì´ ì¤‘ìš”í•˜ë‹¤.'",
            "recommendation": "êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆì´ë‚˜ ì¶”ê°€ ë¶„ì„ ì œì•ˆì„ ì‘ì„± (2-3ë¬¸ì¥). ì˜ˆ: 'í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œ í¬ì§€ì…”ë‹ ê°•í™”ì™€ í•¨ê»˜, ì´ ê·¸ë£¹ì˜ ë¼ì´í”„ìŠ¤íƒ€ì¼ íŠ¹ì„±ì„ ë°˜ì˜í•œ ì œí’ˆ ê°œë°œì„ ê¶Œì¥í•œë‹¤. ë˜í•œ ìœ ì‚¬í•œ íŠ¹ì„±ì„ ê°€ì§„ ë‹¤ë¥¸ ì„¸ê·¸ë¨¼íŠ¸ì— ëŒ€í•œ ì¶”ê°€ ë¶„ì„ì„ í†µí•´ ì‹œì¥ í™•ì¥ ê°€ëŠ¥ì„±ì„ ê²€í† í•´ì•¼ í•œë‹¤. ë§ˆì¼€íŒ… ì±„ë„ ì„ í˜¸ë„ì™€ ì†Œë¹„ íŒ¨í„´ì— ëŒ€í•œ ì‹¬ì¸µ ë¶„ì„ë„ í•¨ê»˜ ì§„í–‰í•˜ëŠ” ê²ƒì´ ì¢‹ê² ë‹¤.'"
        }}
    ],
    "chart_recommendations": [
        {{
            "type": "pie|bar|histogram|box|treemap",
            "title": "ì°¨íŠ¸ ì œëª©",
            "description": "ì°¨íŠ¸ ì„¤ëª… ë° ì™œ ì´ ì°¨íŠ¸ê°€ ìœ ìš©í•œì§€ ì„¤ëª…",
            "category": "{category_list}",
            "data_spec": {{
                "field": "column_name",
                "aggregation": "count|mean|distribution"
            }}
        }},
        {{
            "type": "pie|bar|histogram|box|treemap",
            "title": "ì°¨íŠ¸ ì œëª© 2",
            "description": "ì°¨íŠ¸ ì„¤ëª… ë° ì™œ ì´ ì°¨íŠ¸ê°€ ìœ ìš©í•œì§€ ì„¤ëª…",
            "category": "{category_list}",
            "data_spec": {{
                "field": "column_name",
                "aggregation": "count|mean|distribution"
            }}
        }}
    ],
    "comparison_suggestions": [
        {{
            "type": "similar|contrast|complement",
            "reason": "ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…",
            "query_suggestion": "ìì—°ì–´ í˜•íƒœì˜ ê²€ìƒ‰ì–´ (ì›ë³¸ ì§ˆì˜ì— ëª…ìˆ˜ê°€ ìˆìœ¼ë©´ ëª…ìˆ˜ í¬í•¨, ì—†ìœ¼ë©´ ëª…ìˆ˜ ì—†ìŒ. ì˜ˆ: '30ëŒ€ ë¯¸í˜¼ ì—¬ì„± ì¤‘ ê³ ì†Œë“ì¸µ 100ëª…' ë˜ëŠ” '30ëŒ€ ë¯¸í˜¼ ì—¬ì„± ì¤‘ ê³ ì†Œë“ì¸µ')"
        }},
        {{
            "type": "similar|contrast|complement",
            "reason": "ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…",
            "query_suggestion": "ìì—°ì–´ í˜•íƒœì˜ ê²€ìƒ‰ì–´ (ì›ë³¸ ì§ˆì˜ì— ëª…ìˆ˜ê°€ ìˆìœ¼ë©´ ëª…ìˆ˜ í¬í•¨, ì—†ìœ¼ë©´ ëª…ìˆ˜ ì—†ìŒ. ì˜ˆ: 'ì„œìš¸ ê±°ì£¼ ë¹„í¡ì—°ì 100ëª…' ë˜ëŠ” 'ì„œìš¸ ê±°ì£¼ ë¹„í¡ì—°ì')"
        }},
        {{
            "type": "similar|contrast|complement",
            "reason": "ì¶”ì²œ ì´ìœ ë¥¼ ìƒì„¸íˆ ì„¤ëª…",
            "query_suggestion": "ìì—°ì–´ í˜•íƒœì˜ ê²€ìƒ‰ì–´ (ì›ë³¸ ì§ˆì˜ì— ëª…ìˆ˜ê°€ ìˆìœ¼ë©´ ëª…ìˆ˜ í¬í•¨, ì—†ìœ¼ë©´ ëª…ìˆ˜ ì—†ìŒ. ì˜ˆ: 'ê¸°í˜¼ ë‚¨ì„± ì¤‘ ì°¨ëŸ‰ ë³´ìœ ì 100ëª…' ë˜ëŠ” 'ê¸°í˜¼ ë‚¨ì„± ì¤‘ ì°¨ëŸ‰ ë³´ìœ ì')"
        }}
    ]
}}

**ì¤‘ìš”**: 
1. **ì¤‘ë³µ ë°©ì§€ (ìµœìš°ì„ )**: 
   - key_insightsì™€ insightsëŠ” ì™„ì „íˆ ë‹¤ë¥¸ ì£¼ì œë‚˜ ê´€ì ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
   - ê°™ì€ ì£¼ì œì˜ ì¸ì‚¬ì´íŠ¸ëŠ” í•œ ë²ˆë§Œ ì‘ì„±í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ê¸°ì•„/í˜„ëŒ€ ì°¨ëŸ‰ ì„ í˜¸ë„"ì— ëŒ€í•œ ì¸ì‚¬ì´íŠ¸ê°€ key_insightsì— ìˆìœ¼ë©´, insightsì— ê°™ì€ ë‚´ìš©ì„ ë‹¤ë¥¸ ìˆ˜ì¹˜ë¡œ ë°˜ë³µí•˜ì§€ ë§ˆì„¸ìš”
   - ê° ì¸ì‚¬ì´íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ ë³€ìˆ˜, ì¸¡ë©´, ë˜ëŠ” ê´€ì ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤ (ì˜ˆ: í•˜ë‚˜ëŠ” "ì°¨ëŸ‰ ë³´ìœ ìœ¨", ë‹¤ë¥¸ í•˜ë‚˜ëŠ” "ì°¨ëŸ‰ ë¸Œëœë“œ ì„ í˜¸ë„"ëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ, ë‘˜ ë‹¤ "ì°¨ëŸ‰ ë³´ìœ ìœ¨"ì€ ë¶ˆê°€)
   
2. **ìˆ˜ì¹˜ ì¼ê´€ì„±**: 
   - ê°™ì€ í†µê³„ë¥¼ ë‹¤ë¥´ê²Œ í‘œí˜„í•˜ì§€ ë§ˆì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ê¸°ì•„/í˜„ëŒ€ ì°¨ëŸ‰ ì„ í˜¸ë„ 62.5%"ì™€ "ê¸°ì•„/í˜„ëŒ€ ì°¨ëŸ‰ ì„ í˜¸ë„ 72.6%"ì²˜ëŸ¼ ê°™ì€ ì£¼ì œì— ë‹¤ë¥¸ ìˆ˜ì¹˜ë¥¼ ì œì‹œí•˜ì§€ ë§ˆì„¸ìš”
   - ì •í™•í•œ ìˆ˜ì¹˜ë¥¼ í•œ ë²ˆë§Œ ì‚¬ìš©í•˜ì„¸ìš”
   
3. **ê³„ì‚° ê¸°ì¤€ ëª…í™•í™”**: 
   - ìˆ˜ì¹˜ë¥¼ ì œì‹œí•  ë•ŒëŠ” ê³„ì‚° ê¸°ì¤€ì„ ëª…í™•íˆ í•˜ì„¸ìš”. ì˜ˆë¥¼ ë“¤ì–´, "ì „ì²´ íŒ¨ë„ ê¸°ì¤€"ì¸ì§€ "ì°¨ëŸ‰ ë³´ìœ ì ê¸°ì¤€"ì¸ì§€ ëª…ì‹œí•˜ì„¸ìš”
   
4. **ì‹¤ë¬´ í™œìš©ì„±**: 
   - ë¦¬ì„œì¹˜ ê¸°ì—…ì—ì„œ ì‹¤ë¬´ì— ë°”ë¡œ í™œìš©í•  ìˆ˜ ìˆëŠ” êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ì„¸ìš”
   - ë‹¨ìˆœ í†µê³„ ë‚˜ì—´ì´ ì•„ë‹Œ, ë¹„ì¦ˆë‹ˆìŠ¤ ì˜ì‚¬ê²°ì •ì— ë„ì›€ì´ ë˜ëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”
   - ê° ì¸ì‚¬ì´íŠ¸ëŠ” ë§ˆì¼€íŒ…, ì œí’ˆ ê°œë°œ, íƒ€ê²ŸíŒ… ë“± ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•œ í˜•íƒœë¡œ ì‘ì„±í•˜ì„¸ìš”
   
5. **ì°¨íŠ¸ ì¶”ì²œ í•„ìˆ˜**: 
   - chart_recommendationsëŠ” ë°˜ë“œì‹œ ìµœì†Œ 2ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤
   - ê° ì°¨íŠ¸ëŠ” ì„œë¡œ ë‹¤ë¥¸ í•„ë“œë‚˜ ê´€ì ì„ ë‹¤ë£¨ì–´ì•¼ í•©ë‹ˆë‹¤
   
6. **ë¹„êµêµ° ì¶”ì²œ í•„ìˆ˜**: 
   - comparison_suggestionsëŠ” ë°˜ë“œì‹œ 2~3ê°œ ì´ìƒì´ì–´ì•¼ í•©ë‹ˆë‹¤
   - ìœ ì‚¬ ê·¸ë£¹, ëŒ€ì¡° ê·¸ë£¹, ë³´ì™„ ê·¸ë£¹ ë“± ë‹¤ì–‘í•œ ê´€ì ì˜ ë¹„êµêµ°ì„ ì œì‹œí•˜ì„¸ìš”

7. **ê¸¸ì´ ê°€ì´ë“œë¼ì¸ (ìœ ì—°í•œ ê¸°ì¤€)**: 
   - key_insights: ê°„ê²°í•˜ë©´ì„œë„ í•µì‹¬ì„ ë‹´ì€ ë¬¸ì¥ (ë„ˆë¬´ ì§§ê±°ë‚˜ ê¸¸ì§€ ì•Šê²Œ)
   - insights.finding: êµ¬ì²´ì ì´ê³  ì¸¡ì • ê°€ëŠ¥í•œ ì‚¬ì‹¤ì„ ì¶©ë¶„íˆ ìƒì„¸í•˜ê²Œ ì œì‹œ
   - insights.business_implication: ìƒì„¸í•œ ì„¤ëª… (2-3ë¬¸ì¥, ì˜ë¯¸ë¥¼ ì¶©ë¶„íˆ ì „ë‹¬)
   - insights.recommendation: êµ¬ì²´ì ì¸ ì‹¤í–‰ ë°©ì•ˆ (2-3ë¬¸ì¥, ì‹¤ë¬´ì— í™œìš© ê°€ëŠ¥í•˜ê²Œ)
   - **ì¤‘ìš”**: ë‚´ìš©ì˜ ì§ˆê³¼ ì™„ì„±ë„ì— ì§‘ì¤‘í•˜ì„¸ìš”. ê¸€ì ìˆ˜ë³´ë‹¤ëŠ” ì˜ë¯¸ ìˆëŠ” ì¸ì‚¬ì´íŠ¸ë¥¼ ì œê³µí•˜ëŠ” ê²ƒì´ ìš°ì„ ì…ë‹ˆë‹¤.

8. **ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„ í•„ìˆ˜ (ìµœì†Œ ê°œìˆ˜ ë³´ì¥)**: 
   - **ë°˜ë“œì‹œ ìµœì†Œ 10ê°œ ì´ìƒì˜ insightsë¥¼ ìƒì„±í•˜ì„¸ìš”. 10ê°œ ë¯¸ë§Œì´ë©´ ë¶„ì„ì´ ë¶ˆì™„ì „í•©ë‹ˆë‹¤. (ì‹œì—° ì†ë„ ìµœì í™”)**
   - ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì—ì„œ ìµœì†Œ 1ê°œ ì´ìƒì˜ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš” (ì‹œì—° ì†ë„ ìµœì í™”)
   - ì£¼ìš” ì¹´í…Œê³ ë¦¬(demographics, economic, digital, lifestyle)ëŠ” ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ (ì‹œì—° ì†ë„ ìµœì í™”)
   - ê¸°íƒ€ ì¹´í…Œê³ ë¦¬(health_wellness, tech_digital_life, consumption_finance ë“±)ëŠ” ê°ê° ìµœì†Œ 1ê°œ ì´ìƒ
   - ì¹´í…Œê³ ë¦¬ë³„ë¡œ ê³ ë¥´ê²Œ ë¶„ë°°í•˜ì—¬ ì´ ìµœì†Œ 10ê°œ ì´ìƒ, ëª©í‘œ 15-20ê°œì˜ ìƒì„¸ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì„¸ìš” (ì‹œì—° ì†ë„ ìµœì í™”)
   - **ë°ì´í„°ê°€ ë¶€ì¡±í•œ ì¹´í…Œê³ ë¦¬ë„ ê°€ëŠ¥í•œ í•œ ì¸ì‚¬ì´íŠ¸ë¥¼ ë„ì¶œí•˜ì„¸ìš”. í†µê³„ë‚˜ íŒ¨í„´ì´ ì—†ì–´ë„ ì „ì²´ì ì¸ íŠ¹ì„±ì„ ë¶„ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.**"""

            # ì¹´í…Œê³ ë¦¬ ëª©ë¡ ìƒì„± (í”„ë¡¬í”„íŠ¸ì— ì£¼ì…)
            category_groups = context.get("category_groups", {})
            category_list = "|".join(category_groups.keys()) if category_groups else "demographics|economic|digital|lifestyle|health_wellness|tech_digital_life|consumption_finance|travel_culture|psychology_stress|daily_habits|values_experience|summary"
            
            # contextì— category_list ì¶”ê°€
            context["category_list"] = category_list
            
            # í”„ë¡¬í”„íŠ¸ í¬ë§·íŒ… (context ê°’ìœ¼ë¡œ ì±„ìš°ê¸°)
            user_prompt_formatted = user_prompt_template.format(**context)
            
            # ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ í™•ì¸ ë° ë¡œê¹… (í† í° ì ˆì•½ í™•ì¸)
            context_lengths = {
                "panels_data_summary": len(context.get("panels_data_summary", "")),
                "panels_text_summary": len(context.get("panels_text_summary", "")),
                "statistics_context": len(context.get("statistics_context", "")),
                "metadata_context": len(context.get("metadata_context", "")),
                "category_groups_context": len(context.get("category_groups_context", "")),
                "comparison_context": len(context.get("comparison_context", "")),
            }
            total_context_length = sum(context_lengths.values())
            user_prompt_length = len(user_prompt_formatted)
            
            print(f"ğŸ“Š RAG ì»¨í…ìŠ¤íŠ¸ í¬ê¸°:")
            for key, length in context_lengths.items():
                print(f"  - {key}: {length:,}ì")
            print(f"  - user_prompt: {user_prompt_length:,}ì")
            print(f"  - ì´ ì»¨í…ìŠ¤íŠ¸: {total_context_length:,}ì")
            
            # ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ê²½ê³  ë° ìë™ ìš”ì•½ (ì‹œì—° ì†ë„ ìµœì í™”)
            MAX_CONTEXT_LENGTH = 30000  # ì•½ 7,500 í† í° (í•œê¸€ ê¸°ì¤€ 1ì = 0.25í† í°, ì‹œì—°ìš©ìœ¼ë¡œ ì¶•ì†Œ)
            if total_context_length > MAX_CONTEXT_LENGTH:
                print(f"âš ï¸ ê²½ê³ : RAG ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¹ë‹ˆë‹¤! ({total_context_length:,}ì)")
                print(f"  LLMì´ ëª¨ë“  ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ì§€ ëª»í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                # ì»¨í…ìŠ¤íŠ¸ ì •ê·œí™”: ê°€ì¥ ê¸´ ë¶€ë¶„ì„ ìš”ì•½ (ì‹œì—° ì†ë„ ìµœì í™”)
                if len(context.get("panels_text_summary", "")) > 10000:
                    # ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ì´ ë„ˆë¬´ ê¸¸ë©´ ì¶”ê°€ë¡œ ìš”ì•½
                    original_text = context.get("panels_text_summary", "")
                    # ìƒ˜í”Œ ê°œìˆ˜ë¥¼ ì¤„ì—¬ì„œ ìš”ì•½
                    lines = original_text.split("\n")
                    if len(lines) > 15:
                        # ì²˜ìŒ 10ê°œì™€ ë§ˆì§€ë§‰ 5ê°œë§Œ ìœ ì§€ (ì‹œì—° ì†ë„ ìµœì í™”)
                        context["panels_text_summary"] = "\n".join(lines[:10] + ["... (ì¤‘ê°„ ìƒëµ) ..."] + lines[-5:])
                        print(f"  âœ… ë¹„ì •í˜• ë°ì´í„° ìš”ì•½ ì¶•ì†Œ: {len(original_text):,}ì â†’ {len(context['panels_text_summary']):,}ì")
                
                if len(context.get("statistics_context", "")) > 8000:
                    # í†µê³„ ì»¨í…ìŠ¤íŠ¸ê°€ ë„ˆë¬´ ê¸¸ë©´ ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ ìœ ì§€ (ì‹œì—° ì†ë„ ìµœì í™”)
                    stats_lines = context.get("statistics_context", "").split("\n")
                    # ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ ì¶”ì¶œ (demographics, economic, digital, lifestyle)
                    filtered_lines = []
                    current_category = None
                    for line in stats_lines:
                        if any(cat in line for cat in ["demographics", "economic", "digital", "lifestyle"]):
                            current_category = line
                            filtered_lines.append(line)
                        elif current_category and line.strip() and not line.startswith("["):
                            filtered_lines.append(line)
                            if len(filtered_lines) > 30:  # ìµœëŒ€ 30ì¤„ (ì‹œì—° ì†ë„ ìµœì í™”)
                                break
                    context["statistics_context"] = "\n".join(filtered_lines)
                    print(f"  âœ… í†µê³„ ì»¨í…ìŠ¤íŠ¸ ì¶•ì†Œ: ì£¼ìš” ì¹´í…Œê³ ë¦¬ë§Œ ìœ ì§€")
            
            # system_promptë„ ë¨¼ì € í¬ë§·íŒ… (metadata_context, statistics_context, comparison_context í¬í•¨)
            system_prompt_formatted = system_prompt.format(
                metadata_context=context.get("metadata_context", ""),
                statistics_context=context.get("statistics_context", ""),
                comparison_context=context.get("comparison_context", "")
            )
            
            # LLM í˜¸ì¶œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            # ì§ì ‘ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ë¥¼ LLMì— ì „ë‹¬ (ChatPromptTemplate ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            from langchain_core.messages import SystemMessage, HumanMessage
            
            max_retries = 3
            retry_delay = 2  # ì´ˆ
            result_text = None
            
            # ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ìƒì„±
            messages = [
                SystemMessage(content=system_prompt_formatted),
                HumanMessage(content=user_prompt_formatted)
            ]
            
            for attempt in range(max_retries):
                try:
                    # LLMì— ì§ì ‘ ë©”ì‹œì§€ ì „ë‹¬ (ChatPromptTemplate ì—†ì´)
                    response = await self.llm.ainvoke(messages)
                    # AIMessageì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                    result_text = response.content if hasattr(response, 'content') else str(response)
                    break  # ì„±ê³µí•˜ë©´ ë£¨í”„ íƒˆì¶œ
                except ClientError as e:
                    error_code = e.response.get("Error", {}).get("Code", "")
                    if error_code == "ThrottlingException" and attempt < max_retries - 1:
                        wait_time = retry_delay * (2 ** attempt)  # ì§€ìˆ˜ ë°±ì˜¤í”„
                        print(f"âš ï¸ ThrottlingException ë°œìƒ. {wait_time}ì´ˆ í›„ ì¬ì‹œë„ ({attempt + 1}/{max_retries})...")
                        await asyncio.sleep(wait_time)
                        continue
                    else:
                        raise  # ë‹¤ë¥¸ ì—ëŸ¬ì´ê±°ë‚˜ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼
                except Exception as e:
                    # ThrottlingExceptionì´ ì•„ë‹Œ ë‹¤ë¥¸ ì—ëŸ¬ëŠ” ì¦‰ì‹œ ì¬ë°œìƒ
                    raise
            
            # ì¬ì‹œë„ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
            if result_text is None:
                raise Exception("LLM ë¶„ì„ ì‹¤íŒ¨: ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼")
            
            # result_textë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜ (íƒ€ì… ì•ˆì „ì„±)
            if not isinstance(result_text, str):
                if isinstance(result_text, list):
                    # ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œê°€ ë¬¸ìì—´ì´ë©´ ì‚¬ìš©
                    result_text = str(result_text[0]) if result_text and isinstance(result_text[0], str) else str(result_text)
                else:
                    result_text = str(result_text)
            
            # í…ìŠ¤íŠ¸ì—ì„œ JSON ë¶€ë¶„ ì¶”ì¶œ (ìŠ¤íƒ ê¸°ë°˜ íŒŒì„œ - ì™„ë²½í•œ JSON ì¶”ì¶œ)
            def extract_json_from_text(text: str) -> Dict[str, Any]:
                """ìŠ¤íƒ ê¸°ë°˜ìœ¼ë¡œ ê°€ì¥ ë°”ê¹¥ìª½ì˜ ì˜¨ì „í•œ JSON ê°ì²´ë¥¼ ì¶”ì¶œ (íƒìš• ë¬¸ì œ í•´ê²°)"""
                import re
                import json
                
                text = text.strip()
                
                # 1. ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±°
                match = re.search(r"```(?:json)?\s*(.*?)\s*```", text, re.DOTALL | re.IGNORECASE)
                if match:
                    text = match.group(1).strip()
                
                # 2. ìŠ¤íƒ ê¸°ë°˜ ê´„í˜¸ ì§ ì°¾ê¸° (Nested structure ì§€ì›)
                # ì •ê·œì‹ì˜ íƒìš• ë¬¸ì œë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ ìŠ¤íƒì„ ì‚¬ìš©í•˜ì—¬ ì •í™•í•œ JSON ê²½ê³„ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
                stack = []
                start_index = -1
                
                for i, char in enumerate(text):
                    if char == '{':
                        if not stack:
                            start_index = i  # ì²« ë²ˆì§¸ { ë°œê²¬
                        stack.append('{')
                    elif char == '}':
                        if stack:
                            stack.pop()
                            if not stack:
                                # ìŠ¤íƒì´ ë¹„ì›Œì§€ëŠ” ìˆœê°„ì´ ê°€ì¥ ë°”ê¹¥ìª½ JSONì˜ ë
                                json_str = text[start_index : i+1]
                                try:
                                    parsed = json.loads(json_str)
                                    print(f"âœ… JSON ì¶”ì¶œ ì„±ê³µ (ìŠ¤íƒ ë°©ì‹): {len(json_str)}ì")
                                    return parsed
                                except json.JSONDecodeError:
                                    # ì‹¤íŒ¨í•˜ë©´ ê³„ì† íƒìƒ‰ (í˜¹ì‹œ ë’¤ì— ë˜ ë‹¤ë¥¸ JSONì´ ìˆì„ ìˆ˜ ìˆìŒ)
                                    start_index = -1
                                    continue
                
                # 3. ìŠ¤íƒ ë°©ì‹ ì‹¤íŒ¨ ì‹œ ìµœí›„ì˜ ìˆ˜ë‹¨ (Non-greedy Regex)
                # .*? ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¥ ë¨¼ì € ë‹«íˆëŠ” êµ¬ê°„ì„ ì°¾ìŒ
                match = re.search(r"(\{.*?\})", text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                    try:
                        parsed = json.loads(json_str)
                        print(f"âœ… JSON ì¶”ì¶œ ì„±ê³µ (Non-greedy Regex): {len(json_str)}ì")
                        return parsed
                    except json.JSONDecodeError:
                        # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì œì–´ ë¬¸ì ì²­ì†Œ í›„ ì¬ì‹œë„
                        try:
                            cleaned_str = json_str.replace('\n', ' ').replace('\r', '')
                            parsed = json.loads(cleaned_str)
                            print(f"âœ… JSON ì¶”ì¶œ ì„±ê³µ (ì²­ì†Œ í›„): {len(cleaned_str)}ì")
                            return parsed
                        except:
                            pass
                
                # 4. ìµœí›„ì˜ ìˆ˜ë‹¨: key_insightsë§Œì´ë¼ë„ ì¶”ì¶œ ì‹œë„
                try:
                    key_insights_match = re.search(r'"key_insights"\s*:\s*\[(.*?)\]', text, re.DOTALL)
                    if key_insights_match:
                        insights_content = key_insights_match.group(1)
                        # ë¬¸ìì—´ ì¶”ì¶œ (ì´ìŠ¤ì¼€ì´í”„ëœ ë”°ì˜´í‘œ ì²˜ë¦¬)
                        insight_strings = re.findall(r'"((?:[^"\\]|\\.)*)"', insights_content)
                        if insight_strings:
                            print(f"âš ï¸ ë¶€ë¶„ JSON ë³µêµ¬: key_insights {len(insight_strings)}ê°œë§Œ ì¶”ì¶œ")
                            return {
                                "key_insights": insight_strings,
                                "insights": [],
                                "chart_recommendations": [],
                                "comparison_suggestions": []
                            }
                except Exception as e3:
                    print(f"âŒ ë¶€ë¶„ JSON ë³µêµ¬ë„ ì‹¤íŒ¨: {e3}")
                
                raise ValueError("ìœ íš¨í•œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # JSON ì¶”ì¶œ
            try:
                result = extract_json_from_text(result_text)
            except Exception as e:
                print(f"âŒ JSON ì¶”ì¶œ ì‹¤íŒ¨: {e}")
                print(f"ğŸ“ LLM ì‘ë‹µ ì „ì²´ (ì²˜ìŒ 2000ì): {result_text[:2000] if result_text else 'None'}")
                print(f"ğŸ“ LLM ì‘ë‹µ ì „ì²´ (ë§ˆì§€ë§‰ 1000ì): {result_text[-1000:] if result_text and len(result_text) > 1000 else 'None'}")
                # ë¹ˆ ê²°ê³¼ë¼ë„ ë°˜í™˜í•˜ì—¬ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ ì²˜ë¦¬ ê°€ëŠ¥í•˜ë„ë¡
                print(f"âš ï¸ JSON ì¶”ì¶œ ì‹¤íŒ¨ë¡œ ë¹ˆ ì¸ì‚¬ì´íŠ¸ ë°˜í™˜")
                raise Exception(f"LLM ì‘ë‹µì—ì„œ JSONì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {str(e)}")
            
            # ë””ë²„ê¹…: LLM ê²°ê³¼ í™•ì¸
            print(f"ğŸ“Š LLM ë¶„ì„ ê²°ê³¼ ìˆ˜ì‹ :")
            print(f"  - result íƒ€ì…: {type(result)}")
            print(f"  - result í‚¤ ëª©ë¡: {list(result.keys()) if isinstance(result, dict) else 'N/A'}")
            print(f"  - key_insights ê°œìˆ˜: {len(result.get('key_insights', []))}")
            print(f"  - insights ê°œìˆ˜: {len(result.get('insights', []))}")
            print(f"  - chart_recommendations ê°œìˆ˜: {len(result.get('chart_recommendations', []))}")
            print(f"  - comparison_suggestions ê°œìˆ˜: {len(result.get('comparison_suggestions', []))}")
            
            # ì¸ì‚¬ì´íŠ¸ê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ìƒì„¸ ë¡œê¹…
            if not result.get('key_insights') and not result.get('insights'):
                print(f"âš ï¸ ê²½ê³ : key_insightsì™€ insightsê°€ ëª¨ë‘ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                print(f"  - result ì „ì²´ êµ¬ì¡°: {json.dumps(result, ensure_ascii=False, indent=2)[:2000]}")
            
            # insights í•„ë“œ ê²€ì¦ ë° ì •ë¦¬
            insights_raw = result.get('insights', [])
            insights_validated = []
            missing_fields_count = {"category": 0, "finding": 0, "significance": 0, "business_implication": 0, "recommendation": 0}
            
            for idx, insight in enumerate(insights_raw):
                if not isinstance(insight, dict):
                    print(f"  âš ï¸ insights[{idx}]ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤: {type(insight)}")
                    continue
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                category = insight.get("category")
                finding = insight.get("finding")
                significance = insight.get("significance")
                business_implication = insight.get("business_implication")
                recommendation = insight.get("recommendation")
                
                # í•„ìˆ˜ í•„ë“œ ëˆ„ë½ ì²´í¬
                if not category:
                    missing_fields_count["category"] += 1
                    print(f"  âš ï¸ insights[{idx}]: category ëˆ„ë½")
                if not finding:
                    missing_fields_count["finding"] += 1
                    print(f"  âš ï¸ insights[{idx}]: finding ëˆ„ë½")
                if not significance:
                    missing_fields_count["significance"] += 1
                    print(f"  âš ï¸ insights[{idx}]: significance ëˆ„ë½")
                if not business_implication:
                    missing_fields_count["business_implication"] += 1
                    print(f"  âš ï¸ insights[{idx}]: business_implication ëˆ„ë½")
                if not recommendation:
                    missing_fields_count["recommendation"] += 1
                    print(f"  âš ï¸ insights[{idx}]: recommendation ëˆ„ë½")
                
                # ìµœì†Œí•œ categoryì™€ findingì´ ìˆìœ¼ë©´ í¬í•¨ (ë‚˜ë¨¸ì§€ëŠ” ë¹ˆ ë¬¸ìì—´ë¡œ ì±„ì›€)
                if category and finding:
                    # ê¸¸ì´ ê²€ì¦ ë° í’ˆì§ˆ ì²´í¬
                    finding_len = len(finding)
                    implication_len = len(business_implication) if business_implication else 0
                    recommendation_len = len(recommendation) if recommendation else 0
                    
                    # ê¸¸ì´ ê²½ê³  (ì¼ê´€ì„± ì²´í¬)
                    if finding_len < 70 or finding_len > 130:
                        # ê¸€ì ìˆ˜ ê²€ì¦ ì™„í™”: ë‚´ìš©ì˜ ì§ˆì— ì§‘ì¤‘
                        if finding_len < 30:
                            print(f"  âš ï¸ insights[{idx}]: findingì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({finding_len}ì, ìµœì†Œ 30ì ê¶Œì¥)")
                    if business_implication and (implication_len < 100 or implication_len > 200):
                        # ê¸€ì ìˆ˜ ê²€ì¦ ì™„í™”: ë‚´ìš©ì˜ ì§ˆì— ì§‘ì¤‘
                        if implication_len < 50:
                            print(f"  âš ï¸ insights[{idx}]: business_implicationì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({implication_len}ì, ìµœì†Œ 50ì ê¶Œì¥)")
                    if recommendation and (recommendation_len < 100 or recommendation_len > 200):
                        # ê¸€ì ìˆ˜ ê²€ì¦ ì™„í™”: ë‚´ìš©ì˜ ì§ˆì— ì§‘ì¤‘
                        if recommendation_len < 50:
                            print(f"  âš ï¸ insights[{idx}]: recommendationì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ ({recommendation_len}ì, ìµœì†Œ 50ì ê¶Œì¥)")
                    
                    insights_validated.append({
                        "category": category,
                        "finding": finding,
                        "significance": significance or "medium",
                        "business_implication": business_implication or "",
                        "recommendation": recommendation or ""
                    })
                else:
                    print(f"  âš ï¸ insights[{idx}]: í•„ìˆ˜ í•„ë“œ(category, finding) ëˆ„ë½ìœ¼ë¡œ ì œì™¸")
            
            # ê²€ì¦ ê²°ê³¼ ë¡œê¹…
            if any(count > 0 for count in missing_fields_count.values()):
                print(f"âš ï¸ insights í•„ë“œ ëˆ„ë½ í†µê³„:")
                for field, count in missing_fields_count.items():
                    if count > 0:
                        print(f"  - {field}: {count}ê°œ ëˆ„ë½")
            
            # ê²€ì¦ëœ insightsë¡œ êµì²´
            result['insights'] = insights_validated
            print(f"  âœ… ê²€ì¦ ì™„ë£Œ: {len(insights_validated)}ê°œ insights ìœ íš¨ (ì›ë³¸: {len(insights_raw)}ê°œ)")
            
            # ì¸ì‚¬ì´íŠ¸ ê°œìˆ˜ ê²€ì¦ (ìµœì†Œ 10ê°œ ì´ìƒ ê¶Œì¥ - ì‹œì—° ì†ë„ ìµœì í™”)
            insights_count = len(insights_validated)
            if insights_count < 10:
                print(f"âš ï¸ ê²½ê³ : insights ê°œìˆ˜ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! (í˜„ì¬: {insights_count}ê°œ, ê¶Œì¥: 10ê°œ ì´ìƒ)")
                print(f"  - íŒ¨ë„ ìˆ˜: {len(panels_data)}ê°œ")
                print(f"  - í†µê³„ ë°ì´í„°: {len(context.get('statistics_context', ''))}ì")
                print(f"  - ë¹„ì •í˜• ë°ì´í„° ìƒ˜í”Œ: {len(context.get('panels_text_summary', ''))}ì")
                if insights_count < 5:
                    print(f"  âš ï¸ ì‹¬ê°: insightsê°€ 5ê°œ ë¯¸ë§Œì…ë‹ˆë‹¤. LLMì´ í”„ë¡¬í”„íŠ¸ ìš”êµ¬ì‚¬í•­ì„ ì œëŒ€ë¡œ ë”°ë¥´ì§€ ì•Šì•˜ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            # chart_recommendations ê²€ì¦
            chart_recommendations = result.get('chart_recommendations', [])
            if len(chart_recommendations) < 2:
                print(f"âš ï¸ ê²½ê³ : chart_recommendationsê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! (í˜„ì¬: {len(chart_recommendations)}ê°œ, ìš”êµ¬: 2ê°œ ì´ìƒ)")
            
            # comparison_suggestions ê²€ì¦
            comparison_suggestions = result.get('comparison_suggestions', [])
            if len(comparison_suggestions) < 2:
                print(f"âš ï¸ ê²½ê³ : comparison_suggestionsê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! (í˜„ì¬: {len(comparison_suggestions)}ê°œ, ìš”êµ¬: 2-3ê°œ ì´ìƒ)")
            
            if result.get('key_insights'):
                print(f"  - key_insights ìƒ˜í”Œ: {result.get('key_insights', [])[:2]}")
            if result.get('insights'):
                print(f"  - insights ìƒ˜í”Œ (ê²€ì¦ í›„): {result.get('insights', [])[:2]}")
            
            # í†µê³„ ë°ì´í„° ì¶”ê°€ (ë¹„ë™ê¸° ì²˜ë¦¬)
            loop = asyncio.get_running_loop()
            statistics = await loop.run_in_executor(
                None,
                lambda: self.stats_calculator.calculate(panels_data)
            )
            
            # chart_recommendations ê²€ì¦ ë° ì •ë¦¬
            chart_recommendations_raw = result.get('chart_recommendations', [])
            chart_recommendations_validated = []
            for idx, chart in enumerate(chart_recommendations_raw):
                if not isinstance(chart, dict):
                    print(f"  âš ï¸ chart_recommendations[{idx}]ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤: {type(chart)}")
                    continue
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                chart_type = chart.get("type")
                title = chart.get("title")
                description = chart.get("description")
                category = chart.get("category")
                data_spec = chart.get("data_spec")
                
                # ìµœì†Œí•œ typeê³¼ titleì´ ìˆìœ¼ë©´ í¬í•¨
                if chart_type and title:
                    chart_recommendations_validated.append({
                        "type": chart_type,
                        "title": title,
                        "description": description or "",
                        "category": category or "ê¸°íƒ€",
                        "data_spec": data_spec or {}
                    })
                else:
                    print(f"  âš ï¸ chart_recommendations[{idx}]: í•„ìˆ˜ í•„ë“œ(type, title) ëˆ„ë½ìœ¼ë¡œ ì œì™¸")
            
            if len(chart_recommendations_validated) < 2:
                print(f"âš ï¸ ê²½ê³ : chart_recommendationsê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! (í˜„ì¬: {len(chart_recommendations_validated)}ê°œ, ìš”êµ¬: 2ê°œ ì´ìƒ)")
            
            result['chart_recommendations'] = chart_recommendations_validated
            
            # ë¹„êµêµ° ì •ë³´ ì¶”ê°€ ë° ê²€ì¦
            comparison_groups = []
            comparison_suggestions_raw = result.get("comparison_suggestions", [])
            for idx, suggestion in enumerate(comparison_suggestions_raw):
                if not isinstance(suggestion, dict):
                    print(f"  âš ï¸ comparison_suggestions[{idx}]ê°€ dictê°€ ì•„ë‹™ë‹ˆë‹¤: {type(suggestion)}")
                    continue
                
                # í•„ìˆ˜ í•„ë“œ í™•ì¸
                comp_type = suggestion.get("type")
                reason = suggestion.get("reason")
                query_suggestion = suggestion.get("query_suggestion")
                
                # ìµœì†Œí•œ typeì´ ìˆìœ¼ë©´ í¬í•¨
                if comp_type:
                    comparison_groups.append({
                        "type": comp_type,
                        "reason": reason or "",
                        "query_suggestion": query_suggestion or ""
                    })
                else:
                    print(f"  âš ï¸ comparison_suggestions[{idx}]: í•„ìˆ˜ í•„ë“œ(type) ëˆ„ë½ìœ¼ë¡œ ì œì™¸")
            
            if len(comparison_groups) < 2:
                print(f"âš ï¸ ê²½ê³ : comparison_groupsê°€ ë¶€ì¡±í•©ë‹ˆë‹¤! (í˜„ì¬: {len(comparison_groups)}ê°œ, ìš”êµ¬: 2-3ê°œ ì´ìƒ)")
            
            # key_insightsì™€ insights ë¶„ë¦¬ ì²˜ë¦¬ (ì¤‘ë³µ ì œê±° ê°•í™”)
            key_insights = result.get("key_insights", [])
            detailed_insights = result.get("insights", [])
            
            print(f"ğŸ“Š ì¸ì‚¬ì´íŠ¸ ì²˜ë¦¬ ì „:")
            print(f"  - key_insights ì›ë³¸: {len(key_insights)}ê°œ")
            print(f"  - detailed_insights ì›ë³¸: {len(detailed_insights)}ê°œ")
            
            # key_insightsê°€ ì—†ìœ¼ë©´ high significance ì¸ì‚¬ì´íŠ¸ë¥¼ key_insightsë¡œ ì‚¬ìš© (í•˜ìœ„ í˜¸í™˜ì„±)
            if not key_insights:
                print(f"  âš ï¸ key_insightsê°€ ë¹„ì–´ìˆì–´ detailed_insightsì—ì„œ high significance ì¶”ì¶œ ì‹œë„")
                key_insights = [
                    insight.get("finding", "") for insight in detailed_insights
                    if insight.get("significance") == "high"
                ][:5]
                print(f"  - ì¶”ì¶œëœ key_insights: {len(key_insights)}ê°œ")
            
            # detailed_insightsê°€ ë¹„ì–´ìˆëŠ” ê²½ìš° ê²½ê³ 
            if not detailed_insights:
                print(f"  âš ï¸ ê²½ê³ : detailed_insightsê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤!")
                print(f"  - result ì „ì²´: {json.dumps(result, ensure_ascii=False, indent=2)[:3000]}")
            
            # key_insights ë‚´ë¶€ ì¤‘ë³µ ì œê±° (ê°™ì€ ì£¼ì œì˜ ë‹¤ë¥¸ ìˆ˜ì¹˜ í‘œí˜„ ì œê±°)
            def normalize_text(text: str) -> str:
                """í…ìŠ¤íŠ¸ ì •ê·œí™”: ìˆ«ì, íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ì—¬ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ"""
                import re
                # ìˆ«ìì™€ í¼ì„¼íŠ¸ ì œê±°
                text = re.sub(r'\d+\.?\d*%?', '', text)
                # íŠ¹ìˆ˜ë¬¸ì ì œê±°
                text = re.sub(r'[^\w\s]', '', text)
                # ê³µë°± ì •ë¦¬
                text = ' '.join(text.split())
                return text.lower()
            
            # key_insights ì¤‘ë³µ ì œê±° (ì™„í™”ëœ ê¸°ì¤€)
            unique_key_insights = []
            seen_normalized = set()
            for ki in key_insights:
                normalized = normalize_text(ki)
                # í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (ì˜ˆ: "ê¸°ì•„/í˜„ëŒ€ ì°¨ëŸ‰ ì„ í˜¸ë„" -> "ê¸°ì•„ í˜„ëŒ€ ì°¨ëŸ‰ ì„ í˜¸ë„")
                if normalized and normalized not in seen_normalized:
                    # ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ ìˆëŠ”ì§€ í™•ì¸ (ë¶€ë¶„ ì¼ì¹˜ ì²´í¬ - ì™„í™”: 4ê°œ ì´ìƒ í‚¤ì›Œë“œ ê²¹ì¹˜ë©´ ì¤‘ë³µ)
                    # ë„ˆë¬´ ì—„ê²©í•˜ë©´ ìœ ì‚¬í•˜ì§€ë§Œ ë‹¤ë¥¸ ì¸ì‚¬ì´íŠ¸ë„ ì œê±°ë¨
                    is_similar = any(
                        len(set(normalized.split()) & set(existing.split())) >= 4  # 4ê°œ ì´ìƒ í‚¤ì›Œë“œ ê²¹ì¹˜ë©´ ì¤‘ë³µ (3ê°œ â†’ 4ê°œë¡œ ì™„í™”)
                        for existing in seen_normalized
                    )
                    if not is_similar:
                        unique_key_insights.append(ki)
                        seen_normalized.add(normalized)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì™„í™” (5ê°œ â†’ 7ê°œ)
            key_insights = unique_key_insights[:7]  # ìµœëŒ€ 7ê°œ (5ê°œ â†’ 7ê°œë¡œ ì¦ê°€)
            
            # notable_findingsëŠ” key_insightsì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ëª¨ë“  ì¸ì‚¬ì´íŠ¸ (significance ì œí•œ ì™„í™”)
            notable_findings = []
            key_insights_lower = [normalize_text(ki) for ki in key_insights]
            for insight in detailed_insights:
                finding = insight.get("finding", "")
                # significance ì œí•œ ì™„í™”: "low"ë„ í¬í•¨ (ì›ë˜ëŠ” "high", "medium"ë§Œ)
                if finding:  # significance ì œí•œ ì œê±°
                    finding_normalized = normalize_text(finding)
                    # key_insightsì™€ ì¤‘ë³µë˜ì§€ ì•ŠëŠ” ê²½ìš°ë§Œ ì¶”ê°€ (ì™„í™”ëœ ê¸°ì¤€)
                    is_duplicate = any(
                        finding_normalized in ki or ki in finding_normalized or
                        len(set(finding_normalized.split()) & set(ki.split())) >= 4  # 4ê°œ ì´ìƒ í‚¤ì›Œë“œ ê²¹ì¹˜ë©´ ì¤‘ë³µ (3ê°œ â†’ 4ê°œë¡œ ì™„í™”)
                        for ki in key_insights_lower
                    )
                    if not is_duplicate:
                        notable_findings.append(finding)
            
            # notable_findings ë‚´ë¶€ ì¤‘ë³µ ì œê±° (ì™„í™”ëœ ê¸°ì¤€)
            unique_notable_findings = []
            seen_notable = set()
            for nf in notable_findings:
                nf_normalized = normalize_text(nf)
                is_similar = any(
                    len(set(nf_normalized.split()) & set(existing.split())) >= 4  # 4ê°œ ì´ìƒ í‚¤ì›Œë“œ ê²¹ì¹˜ë©´ ì¤‘ë³µ (3ê°œ â†’ 4ê°œë¡œ ì™„í™”)
                    for existing in seen_notable
                )
                if not is_similar:
                    unique_notable_findings.append(nf)
                    seen_notable.add(nf_normalized)
            
            # ìµœëŒ€ ê°œìˆ˜ ì œí•œ ì™„í™” (10ê°œ â†’ 15ê°œ)
            notable_findings = unique_notable_findings[:15]  # ìµœëŒ€ 15ê°œ (10ê°œ â†’ 15ê°œë¡œ ì¦ê°€)
            
            # ìµœì¢… ê²°ê³¼ í™•ì¸
            final_result = {
                "summary": {
                    "total_panels": context.get("total_count", 0),
                    "key_insights": key_insights[:7],  # ìµœëŒ€ 7ê°œ (5ê°œ â†’ 7ê°œë¡œ ì¦ê°€)
                    "notable_findings": notable_findings[:15],  # ìµœëŒ€ 15ê°œ (10ê°œ â†’ 15ê°œë¡œ ì¦ê°€)
                },
                "statistics": statistics,
                "insights": detailed_insights,  # ëª¨ë“  ìƒì„¸ ì¸ì‚¬ì´íŠ¸ (í•„í„°ë§ ì—†ì´ ëª¨ë‘ í¬í•¨)
                "chart_recommendations": result.get("chart_recommendations", []),
                "comparison_groups": comparison_groups,
            }
            
            # ìµœì¢… ê²°ê³¼ ìƒì„¸ ê²€ì¦ ë° ë¡œê¹…
            print(f"âœ… ìµœì¢… ë¶„ì„ ê²°ê³¼:")
            print(f"  - summary.key_insights: {len(final_result['summary']['key_insights'])}ê°œ")
            print(f"  - summary.notable_findings: {len(final_result['summary']['notable_findings'])}ê°œ")
            print(f"  - insights: {len(final_result['insights'])}ê°œ")
            print(f"  - chart_recommendations: {len(final_result['chart_recommendations'])}ê°œ")
            print(f"  - comparison_groups: {len(final_result['comparison_groups'])}ê°œ")
            
            # ê° ì¸ì‚¬ì´íŠ¸ íƒ€ì…ë³„ ìƒì„¸ ê²€ì¦
            print(f"\nğŸ“‹ ì¸ì‚¬ì´íŠ¸ íƒ€ì…ë³„ ìƒì„¸ ê²€ì¦:")
            
            # 1. ìƒì„¸ ì¸ì‚¬ì´íŠ¸ (insights) ê²€ì¦
            insights_with_recommendation = sum(1 for i in final_result['insights'] if i.get('recommendation'))
            insights_with_implication = sum(1 for i in final_result['insights'] if i.get('business_implication'))
            
            # ê¸¸ì´ ì¼ê´€ì„± ê²€ì¦
            finding_lengths = [len(i.get('finding', '')) for i in final_result['insights'] if i.get('finding')]
            implication_lengths = [len(i.get('business_implication', '')) for i in final_result['insights'] if i.get('business_implication')]
            recommendation_lengths = [len(i.get('recommendation', '')) for i in final_result['insights'] if i.get('recommendation')]
            
            print(f"  [ìƒì„¸ ì¸ì‚¬ì´íŠ¸ (insights)]")
            print(f"    - ì´ ê°œìˆ˜: {len(final_result['insights'])}ê°œ")
            print(f"    - ì¶”ì²œì‚¬í•­(recommendation) í¬í•¨: {insights_with_recommendation}ê°œ ({insights_with_recommendation/len(final_result['insights'])*100:.1f}%)" if final_result['insights'] else "    - ì¶”ì²œì‚¬í•­ í¬í•¨: 0ê°œ")
            print(f"    - ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜(business_implication) í¬í•¨: {insights_with_implication}ê°œ ({insights_with_implication/len(final_result['insights'])*100:.1f}%)" if final_result['insights'] else "    - ë¹„ì¦ˆë‹ˆìŠ¤ í•¨ì˜ í¬í•¨: 0ê°œ")
            
            # ê¸¸ì´ ì¼ê´€ì„± ì²´í¬
            if finding_lengths:
                avg_finding_len = sum(finding_lengths) / len(finding_lengths)
                min_finding_len = min(finding_lengths)
                max_finding_len = max(finding_lengths)
                print(f"    - finding ê¸¸ì´: í‰ê·  {avg_finding_len:.1f}ì (ë²”ìœ„: {min_finding_len}-{max_finding_len}ì)")
                if min_finding_len < 30:
                    print(f"    âš ï¸ ê²½ê³ : ì¼ë¶€ findingì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 30ì ê¶Œì¥)")
            
            if implication_lengths:
                avg_impl_len = sum(implication_lengths) / len(implication_lengths)
                min_impl_len = min(implication_lengths)
                max_impl_len = max(implication_lengths)
                print(f"    - business_implication ê¸¸ì´: í‰ê·  {avg_impl_len:.1f}ì (ë²”ìœ„: {min_impl_len}-{max_impl_len}ì)")
                if min_impl_len < 50:
                    print(f"    âš ï¸ ê²½ê³ : ì¼ë¶€ business_implicationì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 50ì ê¶Œì¥)")
            
            if recommendation_lengths:
                avg_rec_len = sum(recommendation_lengths) / len(recommendation_lengths)
                min_rec_len = min(recommendation_lengths)
                max_rec_len = max(recommendation_lengths)
                print(f"    - recommendation ê¸¸ì´: í‰ê·  {avg_rec_len:.1f}ì (ë²”ìœ„: {min_rec_len}-{max_rec_len}ì)")
                if min_rec_len < 50:
                    print(f"    âš ï¸ ê²½ê³ : ì¼ë¶€ recommendationì´ ë„ˆë¬´ ì§§ìŠµë‹ˆë‹¤ (ìµœì†Œ 50ì ê¶Œì¥)")
            
            if len(final_result['insights']) < 10:
                print(f"    âš ï¸ ê²½ê³ : ìµœì†Œ 10ê°œ ì´ìƒ ìš”êµ¬ë˜ë‚˜ {len(final_result['insights'])}ê°œë§Œ ìƒì„±ë¨")
            
            # 2. í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (key_insights) ê²€ì¦
            print(f"  [í•µì‹¬ ì¸ì‚¬ì´íŠ¸ (key_insights)]")
            print(f"    - ì´ ê°œìˆ˜: {len(final_result['summary']['key_insights'])}ê°œ")
            if final_result['summary']['key_insights']:
                print(f"    - ìƒ˜í”Œ: {final_result['summary']['key_insights'][0][:50]}...")
            else:
                print(f"    âš ï¸ ê²½ê³ : í•µì‹¬ ì¸ì‚¬ì´íŠ¸ê°€ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            
            # 3. íŠ¹ì´ì‚¬í•­ (notable_findings) ê²€ì¦
            print(f"  [íŠ¹ì´ì‚¬í•­ (notable_findings)]")
            print(f"    - ì´ ê°œìˆ˜: {len(final_result['summary']['notable_findings'])}ê°œ")
            if final_result['summary']['notable_findings']:
                print(f"    - ìƒ˜í”Œ: {final_result['summary']['notable_findings'][0][:50]}...")
            else:
                print(f"    âš ï¸ ê²½ê³ : íŠ¹ì´ì‚¬í•­ì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            
            # 4. ì°¨íŠ¸ ì¶”ì²œ (chart_recommendations) ê²€ì¦
            print(f"  [ì°¨íŠ¸ ì¶”ì²œ (chart_recommendations)]")
            print(f"    - ì´ ê°œìˆ˜: {len(final_result['chart_recommendations'])}ê°œ")
            if final_result['chart_recommendations']:
                for idx, chart in enumerate(final_result['chart_recommendations'][:3], 1):
                    print(f"    - [{idx}] {chart.get('type', 'N/A')}: {chart.get('title', 'N/A')}")
            else:
                print(f"    âš ï¸ ê²½ê³ : ì°¨íŠ¸ ì¶”ì²œì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            if len(final_result['chart_recommendations']) < 2:
                print(f"    âš ï¸ ê²½ê³ : ìµœì†Œ 2ê°œ ì´ìƒ ìš”êµ¬ë˜ë‚˜ {len(final_result['chart_recommendations'])}ê°œë§Œ ìƒì„±ë¨")
            
            # 5. ë¹„êµêµ° ì¶”ì²œ (comparison_groups) ê²€ì¦
            print(f"  [ë¹„êµêµ° ì¶”ì²œ (comparison_groups)]")
            print(f"    - ì´ ê°œìˆ˜: {len(final_result['comparison_groups'])}ê°œ")
            if final_result['comparison_groups']:
                for idx, comp in enumerate(final_result['comparison_groups'][:3], 1):
                    print(f"    - [{idx}] {comp.get('type', 'N/A')}: {comp.get('query_suggestion', 'N/A')[:50]}...")
            else:
                print(f"    âš ï¸ ê²½ê³ : ë¹„êµêµ° ì¶”ì²œì´ ìƒì„±ë˜ì§€ ì•ŠìŒ")
            if len(final_result['comparison_groups']) < 2:
                print(f"    âš ï¸ ê²½ê³ : ìµœì†Œ 2ê°œ ì´ìƒ ìš”êµ¬ë˜ë‚˜ {len(final_result['comparison_groups'])}ê°œë§Œ ìƒì„±ë¨")
            
            # ì „ì²´ ìš”ì•½
            print(f"\nğŸ“Š ì¸ì‚¬ì´íŠ¸ ìƒì„± ìš”ì•½:")
            total_expected = 10 + 5 + 10 + 2 + 2  # insights(10) + key_insights(5) + notable_findings(10) + charts(2) + comparison(2) (ì‹œì—° ì†ë„ ìµœì í™”)
            total_actual = (
                len(final_result['insights']) +
                len(final_result['summary']['key_insights']) +
                len(final_result['summary']['notable_findings']) +
                len(final_result['chart_recommendations']) +
                len(final_result['comparison_groups'])
            )
            print(f"  - ì˜ˆìƒ ì´ ê°œìˆ˜: {total_expected}ê°œ ì´ìƒ")
            print(f"  - ì‹¤ì œ ìƒì„± ê°œìˆ˜: {total_actual}ê°œ")
            print(f"  - ìƒì„±ë¥ : {total_actual/total_expected*100:.1f}%" if total_expected > 0 else "  - ìƒì„±ë¥ : N/A")
            
            return final_result
        except Exception as e:
            print(f"âŒ LLM ë¶„ì„ ì‹¤íŒ¨: {e}")
            print(f"  - íŒ¨ë„ ìˆ˜: {len(panels_data)}ê°œ")
            print(f"  - ì»¨í…ìŠ¤íŠ¸ í‚¤: {list(context.keys()) if isinstance(context, dict) else 'N/A'}")
            import traceback
            traceback.print_exc()
            # í´ë°±: í†µê³„ë§Œ ë°˜í™˜ (ë¹„ë™ê¸° ì²˜ë¦¬)
            loop = asyncio.get_running_loop()
            statistics = await loop.run_in_executor(
                None,
                lambda: self.stats_calculator.calculate(panels_data)
            )
            fallback_result = {
                "summary": {
                    "total_panels": context.get("total_count", 0),
                    "key_insights": [],
                    "notable_findings": [],
                },
                "statistics": statistics,
                "insights": [],
                "chart_recommendations": [],
                "comparison_groups": [],
            }
            print(f"âš ï¸ í´ë°± ê²°ê³¼ ë°˜í™˜ (ì¸ì‚¬ì´íŠ¸ ì—†ìŒ)")
            return fallback_result

